True cuda
args: None URLord URLord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/178/exp2d_DF_3_2d True urlord2d_178 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d_remote /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d_remote
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
here seg decay
here recon_loss
/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                          | 0/45 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                         | 0/126 [00:00<?, ?it/s]
0.5 recon_decay
------------ 0.17718791961669922 backwards -------------------------------
0.5 recon_decay
------------ 0.1585373878479004 backwards -------------------------------
0.5 recon_decay
------------ 0.16028952598571777 backwards -------------------------------
0.5 recon_decay
------------ 0.15971732139587402 backwards -------------------------------
0.5 recon_decay
------------ 0.1607506275177002 backwards -------------------------------
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   1%|███▎                                                                                                                                                                                                                                                        | 5/378 [00:02<01:54,  3.26it/s, loss=1.11]
  1%|███▌                                                                                                                                                                                                                                                                             | 5/378 [00:02<01:54,  3.26it/s]
------------ 0.16182303428649902 backwards -------------------------------
0.5 recon_decay
------------ 0.16038942337036133 backwards -------------------------------
0.5 recon_decay
------------ 0.15990042686462402 backwards -------------------------------
0.5 recon_decay
------------ 0.16149044036865234 backwards -------------------------------
0.5 recon_decay
------------ 0.16063141822814941 backwards -------------------------------
0.5 recon_decay
------------ 0.15998125076293945 backwards -------------------------------
0.5 recon_decay
------------ 0.16166234016418457 backwards -------------------------------
0.5 recon_decay
------------ 0.15964198112487793 backwards -------------------------------

  3%|█████████▎                                                                                                                                                                                                                                                                      | 13/378 [00:03<01:25,  4.27it/s]
------------ 0.16163182258605957 backwards -------------------------------
0.5 recon_decay
------------ 0.16052651405334473 backwards -------------------------------
0.5 recon_decay
------------ 0.15980052947998047 backwards -------------------------------
0.5 recon_decay
------------ 0.1590898036956787 backwards -------------------------------
0.5 recon_decay
------------ 0.16067910194396973 backwards -------------------------------
0.5 recon_decay
------------ 0.1594860553741455 backwards -------------------------------
0.5 recon_decay
------------ 0.16052842140197754 backwards -------------------------------
0.5 recon_decay
------------ 0.16016054153442383 backwards -------------------------------
0.5 recon_decay
------------ 0.1598062515258789 backwards -------------------------------

  6%|███████████████▊                                                                                                                                                                                                                                                                | 22/378 [00:05<01:21,  4.36it/s]
------------ 0.16121339797973633 backwards -------------------------------
0.5 recon_decay
------------ 0.15993142127990723 backwards -------------------------------
0.5 recon_decay
------------ 0.16026067733764648 backwards -------------------------------
0.5 recon_decay
------------ 0.1610100269317627 backwards -------------------------------
0.5 recon_decay
------------ 0.15963196754455566 backwards -------------------------------
0.5 recon_decay
------------ 0.1597275733947754 backwards -------------------------------
0.5 recon_decay
------------ 0.16061663627624512 backwards -------------------------------
0.5 recon_decay
------------ 0.1599409580230713 backwards -------------------------------
0.5 recon_decay
------------ 0.1623520851135254 backwards -------------------------------

  8%|██████████████████████▎                                                                                                                                                                                                                                                         | 31/378 [00:08<01:20,  4.33it/s]
------------ 0.15918898582458496 backwards -------------------------------
0.5 recon_decay
------------ 0.16187667846679688 backwards -------------------------------
0.5 recon_decay
------------ 0.16196966171264648 backwards -------------------------------
0.5 recon_decay
------------ 0.16058611869812012 backwards -------------------------------
0.5 recon_decay
------------ 0.16005539894104004 backwards -------------------------------
0.5 recon_decay
------------ 0.16121196746826172 backwards -------------------------------
0.5 recon_decay
------------ 0.15953516960144043 backwards -------------------------------
  File "lord_exp.py", line 140, in <module>                                                                                                                                                                                                                                          | 38/378 [00:09<01:18,  4.34it/s]
    main()
  File "lord_exp.py", line 133, in main
    args.func(args)
  File "lord_exp.py", line 114, in run_exp
    train_model(model_dict, path_new_exp, model_name, exp_dict['data_l_name'], exp_dict['data_u_name'], exp_dict['data_v_name'], exp_dict['data_t_name'], exp_dict['base_dir'])
  File "lord_exp.py", line 31, in train_model
    take_from_arg = False)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/lord_unet.py", line 367, in train_ulord
    loaded_model=load_model
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 423, in train
    segs_t, classes_t, model_dir, tensorboard_dir, loaded_model, dim)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 518, in train_URLordModel
    self.training_model(model_dir, tensorboard_dir)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 695, in training_model
    reco_loss_epoch, reco_loss_epoch_witha, i)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 378, in do_step_ulord
    loss.backward()
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt