True cuda
args: None URLord URLord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/168/exp2d_DF_3_2d True urlord2d_168 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                          | 0/45 [00:00<?, ?it/s]
  0%|                                                                                                                                                                                                                                                                                         | 0/126 [00:00<?, ?it/s]
here seg decay
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   3%|████████▋                                                                                                                                                                                                                                                  | 13/378 [00:02<00:49,  7.32it/s, loss=2.27]
  3%|█████████▎                                                                                                                                                                                                                                                                      | 13/378 [00:02<00:49,  7.32it/s]
10 recon_decay
------------ 0.10216927528381348 backwards -------------------------------
10 recon_decay
------------ 0.08533596992492676 backwards -------------------------------
10 recon_decay
------------ 0.08491921424865723 backwards -------------------------------
10 recon_decay
------------ 0.08537960052490234 backwards -------------------------------
10 recon_decay
------------ 0.08546733856201172 backwards -------------------------------
10 recon_decay
------------ 0.08539700508117676 backwards -------------------------------
10 recon_decay
------------ 0.08545041084289551 backwards -------------------------------
10 recon_decay
------------ 0.08443236351013184 backwards -------------------------------
10 recon_decay
------------ 0.08509945869445801 backwards -------------------------------
10 recon_decay
------------ 0.08557534217834473 backwards -------------------------------
10 recon_decay
------------ 0.08500885963439941 backwards -------------------------------
10 recon_decay
------------ 0.08552670478820801 backwards -------------------------------
10 recon_decay
------------ 0.08450984954833984 backwards -------------------------------
10 recon_decay
------------ 0.08501338958740234 backwards -------------------------------
10 recon_decay
------------ 0.08545088768005371 backwards -------------------------------
10 recon_decay
------------ 0.08556771278381348 backwards -------------------------------
10 recon_decay
------------ 0.08534049987792969 backwards -------------------------------
10 recon_decay
------------ 0.08506464958190918 backwards -------------------------------
10 recon_decay
------------ 0.08503317832946777 backwards -------------------------------
10 recon_decay
------------ 0.08619284629821777 backwards -------------------------------
10 recon_decay
------------ 0.08496475219726562 backwards -------------------------------
10 recon_decay
------------ 0.0855555534362793 backwards -------------------------------
10 recon_decay
------------ 0.08464384078979492 backwards -------------------------------
10 recon_decay
------------ 0.08482480049133301 backwards -------------------------------
10 recon_decay
------------ 0.08493328094482422 backwards -------------------------------
10 recon_decay
------------ 0.08533644676208496 backwards -------------------------------
10 recon_decay
------------ 0.08560848236083984 backwards -------------------------------


 11%|██████████████████████████████▉                                                                                                                                                                                                                                                 | 43/378 [00:06<00:45,  7.44it/s]
------------ 0.08501982688903809 backwards -------------------------------
10 recon_decay
------------ 0.08476090431213379 backwards -------------------------------
10 recon_decay
------------ 0.08561015129089355 backwards -------------------------------
10 recon_decay
------------ 0.08536958694458008 backwards -------------------------------
10 recon_decay
------------ 0.08989930152893066 backwards -------------------------------
10 recon_decay
------------ 0.08480262756347656 backwards -------------------------------
10 recon_decay
------------ 0.08541560173034668 backwards -------------------------------
10 recon_decay
------------ 0.0855264663696289 backwards -------------------------------
10 recon_decay
------------ 0.08463025093078613 backwards -------------------------------
10 recon_decay
------------ 0.08492159843444824 backwards -------------------------------
10 recon_decay
------------ 0.08571052551269531 backwards -------------------------------
10 recon_decay
------------ 0.0851583480834961 backwards -------------------------------
10 recon_decay
------------ 0.08478403091430664 backwards -------------------------------
10 recon_decay
------------ 0.08460831642150879 backwards -------------------------------
10 recon_decay
------------ 0.08615374565124512 backwards -------------------------------
10 recon_decay
------------ 0.08504867553710938 backwards -------------------------------
10 recon_decay
------------ 0.08461618423461914 backwards -------------------------------
10 recon_decay
------------ 0.08453965187072754 backwards -------------------------------
10 recon_decay
------------ 0.08571910858154297 backwards -------------------------------
10 recon_decay
------------ 0.09025382995605469 backwards -------------------------------
10 recon_decay
------------ 0.08481407165527344 backwards -------------------------------
10 recon_decay
------------ 0.08528017997741699 backwards -------------------------------
10 recon_decay
------------ 0.08527612686157227 backwards -------------------------------
10 recon_decay
------------ 0.08519649505615234 backwards -------------------------------
10 recon_decay
------------ 0.08514213562011719 backwards -------------------------------
10 recon_decay
------------ 0.08616781234741211 backwards -------------------------------
10 recon_decay
------------ 0.08535647392272949 backwards -------------------------------
10 recon_decay
------------ 0.08557415008544922 backwards -------------------------------
10 recon_decay
------------ 0.0860142707824707 backwards -------------------------------
10 recon_decay
------------ 0.08574891090393066 backwards -------------------------------

 15%|█████████████████████████████████████████▋                                                                                                                                                                                                                                      | 58/378 [00:08<00:43,  7.39it/s]
------------ 0.08523821830749512 backwards -------------------------------
10 recon_decay
------------ 0.08489823341369629 backwards -------------------------------
10 recon_decay
------------ 0.0852811336517334 backwards -------------------------------
10 recon_decay
------------ 0.08577299118041992 backwards -------------------------------
10 recon_decay
------------ 0.08711838722229004 backwards -------------------------------
10 recon_decay
------------ 0.08510804176330566 backwards -------------------------------
10 recon_decay
------------ 0.08661866188049316 backwards -------------------------------
10 recon_decay
------------ 0.08537530899047852 backwards -------------------------------
10 recon_decay
------------ 0.08555102348327637 backwards -------------------------------
10 recon_decay
------------ 0.08542752265930176 backwards -------------------------------
10 recon_decay
------------ 0.08599400520324707 backwards -------------------------------
10 recon_decay
------------ 0.08549785614013672 backwards -------------------------------
10 recon_decay
------------ 0.08555078506469727 backwards -------------------------------
10 recon_decay
------------ 0.08572959899902344 backwards -------------------------------
10 recon_decay
------------ 0.08536291122436523 backwards -------------------------------

 19%|████████████████████████████████████████████████████▌                                                                                                                                                                                                                           | 73/378 [00:10<00:41,  7.36it/s]
------------ 0.08551311492919922 backwards -------------------------------
10 recon_decay
------------ 0.08536100387573242 backwards -------------------------------
10 recon_decay
------------ 0.08647513389587402 backwards -------------------------------
10 recon_decay
------------ 0.08576560020446777 backwards -------------------------------
10 recon_decay
------------ 0.08522510528564453 backwards -------------------------------
10 recon_decay
------------ 0.08539128303527832 backwards -------------------------------
10 recon_decay
------------ 0.08640551567077637 backwards -------------------------------
10 recon_decay
------------ 0.08564257621765137 backwards -------------------------------
10 recon_decay
------------ 0.08518695831298828 backwards -------------------------------
10 recon_decay
------------ 0.0852823257446289 backwards -------------------------------
10 recon_decay
------------ 0.08570313453674316 backwards -------------------------------
10 recon_decay
------------ 0.08559894561767578 backwards -------------------------------
10 recon_decay
------------ 0.08557987213134766 backwards -------------------------------
10 recon_decay
------------ 0.08678936958312988 backwards -------------------------------

 23%|██████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                 | 87/378 [00:12<00:39,  7.31it/s]
------------ 0.08477377891540527 backwards -------------------------------
10 recon_decay
------------ 0.08500123023986816 backwards -------------------------------
10 recon_decay
------------ 0.08499979972839355 backwards -------------------------------
10 recon_decay
------------ 0.08602619171142578 backwards -------------------------------
10 recon_decay
------------ 0.08472084999084473 backwards -------------------------------
10 recon_decay
------------ 0.08517336845397949 backwards -------------------------------
10 recon_decay
------------ 0.08503890037536621 backwards -------------------------------
10 recon_decay
------------ 0.08543658256530762 backwards -------------------------------
10 recon_decay
------------ 0.08561038970947266 backwards -------------------------------
10 recon_decay
------------ 0.08492708206176758 backwards -------------------------------
10 recon_decay
------------ 0.08637094497680664 backwards -------------------------------
10 recon_decay
------------ 0.08467340469360352 backwards -------------------------------
10 recon_decay
------------ 0.0850982666015625 backwards -------------------------------
10 recon_decay
------------ 0.08598041534423828 backwards -------------------------------
10 recon_decay
------------ 0.08536601066589355 backwards -------------------------------

 27%|█████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                     | 102/378 [00:14<00:37,  7.28it/s]
------------ 0.0858008861541748 backwards -------------------------------
10 recon_decay
------------ 0.0854487419128418 backwards -------------------------------
10 recon_decay
------------ 0.08550047874450684 backwards -------------------------------
10 recon_decay
------------ 0.08521318435668945 backwards -------------------------------
10 recon_decay
------------ 0.08715605735778809 backwards -------------------------------
10 recon_decay
------------ 0.08536410331726074 backwards -------------------------------
10 recon_decay
------------ 0.08605408668518066 backwards -------------------------------
10 recon_decay
------------ 0.08516573905944824 backwards -------------------------------
10 recon_decay
------------ 0.08555030822753906 backwards -------------------------------
10 recon_decay
------------ 0.0849757194519043 backwards -------------------------------
10 recon_decay
------------ 0.08575153350830078 backwards -------------------------------
10 recon_decay
------------ 0.08602428436279297 backwards -------------------------------
10 recon_decay
------------ 0.08551287651062012 backwards -------------------------------
10 recon_decay
------------ 0.08533358573913574 backwards -------------------------------
10 recon_decay
------------ 0.08563852310180664 backwards -------------------------------


 35%|██████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                | 132/378 [00:18<00:33,  7.29it/s]
------------ 0.08577585220336914 backwards -------------------------------
10 recon_decay
------------ 0.08516120910644531 backwards -------------------------------
10 recon_decay
------------ 0.085906982421875 backwards -------------------------------
10 recon_decay
------------ 0.08582472801208496 backwards -------------------------------
10 recon_decay
------------ 0.08554267883300781 backwards -------------------------------
10 recon_decay
------------ 0.08505439758300781 backwards -------------------------------
10 recon_decay
------------ 0.08554768562316895 backwards -------------------------------
10 recon_decay
------------ 0.08532857894897461 backwards -------------------------------
10 recon_decay
------------ 0.0857229232788086 backwards -------------------------------
10 recon_decay
------------ 0.08625626564025879 backwards -------------------------------
10 recon_decay
------------ 0.08520317077636719 backwards -------------------------------
10 recon_decay
------------ 0.08510637283325195 backwards -------------------------------
10 recon_decay
------------ 0.08555054664611816 backwards -------------------------------
10 recon_decay
------------ 0.08578205108642578 backwards -------------------------------

 39%|████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                      | 146/378 [00:20<00:32,  7.23it/s]
------------ 0.08559322357177734 backwards -------------------------------
10 recon_decay
------------ 0.08521842956542969 backwards -------------------------------
10 recon_decay
------------ 0.08502388000488281 backwards -------------------------------
10 recon_decay
------------ 0.08564877510070801 backwards -------------------------------
10 recon_decay
------------ 0.0858304500579834 backwards -------------------------------
10 recon_decay
------------ 0.08592081069946289 backwards -------------------------------
10 recon_decay
------------ 0.08664655685424805 backwards -------------------------------
10 recon_decay
------------ 0.08539271354675293 backwards -------------------------------
10 recon_decay
------------ 0.08545184135437012 backwards -------------------------------
10 recon_decay
------------ 0.0856328010559082 backwards -------------------------------
10 recon_decay
------------ 0.0860433578491211 backwards -------------------------------
10 recon_decay
------------ 0.08596491813659668 backwards -------------------------------
10 recon_decay
------------ 0.08549189567565918 backwards -------------------------------
10 recon_decay
------------ 0.08545136451721191 backwards -------------------------------
10 recon_decay
------------ 0.0857229232788086 backwards -------------------------------

 43%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                           | 161/378 [00:22<00:29,  7.26it/s]
------------ 0.0855855941772461 backwards -------------------------------
10 recon_decay
------------ 0.08558130264282227 backwards -------------------------------
10 recon_decay
------------ 0.08632540702819824 backwards -------------------------------
10 recon_decay
------------ 0.08543205261230469 backwards -------------------------------
10 recon_decay
------------ 0.08562827110290527 backwards -------------------------------
10 recon_decay
------------ 0.0858004093170166 backwards -------------------------------
10 recon_decay
------------ 0.08556818962097168 backwards -------------------------------
10 recon_decay
------------ 0.08626008033752441 backwards -------------------------------
10 recon_decay
------------ 0.08527135848999023 backwards -------------------------------
10 recon_decay
------------ 0.08582067489624023 backwards -------------------------------
10 recon_decay
------------ 0.0861968994140625 backwards -------------------------------
10 recon_decay
------------ 0.0852041244506836 backwards -------------------------------
10 recon_decay
------------ 0.08563518524169922 backwards -------------------------------
10 recon_decay
------------ 0.08611822128295898 backwards -------------------------------

 47%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                | 176/378 [00:24<00:27,  7.24it/s]
------------ 0.08507347106933594 backwards -------------------------------
10 recon_decay
------------ 0.08586812019348145 backwards -------------------------------
10 recon_decay
------------ 0.08527731895446777 backwards -------------------------------
10 recon_decay
------------ 0.08633279800415039 backwards -------------------------------
10 recon_decay
------------ 0.08539795875549316 backwards -------------------------------
10 recon_decay
------------ 0.08531403541564941 backwards -------------------------------
10 recon_decay
------------ 0.08617401123046875 backwards -------------------------------
10 recon_decay
------------ 0.08527326583862305 backwards -------------------------------
10 recon_decay
------------ 0.08544206619262695 backwards -------------------------------
10 recon_decay
------------ 0.08599591255187988 backwards -------------------------------
10 recon_decay
------------ 0.08567166328430176 backwards -------------------------------
10 recon_decay
------------ 0.0856165885925293 backwards -------------------------------
10 recon_decay
------------ 0.08562922477722168 backwards -------------------------------
10 recon_decay
------------ 0.08601021766662598 backwards -------------------------------
10 recon_decay
------------ 0.08567118644714355 backwards -------------------------------

 50%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                      | 190/378 [00:26<00:26,  7.18it/s]
------------ 0.0861208438873291 backwards -------------------------------
10 recon_decay
------------ 0.08562612533569336 backwards -------------------------------
10 recon_decay
------------ 0.08667469024658203 backwards -------------------------------
10 recon_decay
------------ 0.08557391166687012 backwards -------------------------------
10 recon_decay
------------ 0.0862572193145752 backwards -------------------------------
10 recon_decay
------------ 0.08573770523071289 backwards -------------------------------
10 recon_decay
------------ 0.08637166023254395 backwards -------------------------------
10 recon_decay
------------ 0.08644366264343262 backwards -------------------------------
10 recon_decay
------------ 0.08578801155090332 backwards -------------------------------
10 recon_decay
------------ 0.08738994598388672 backwards -------------------------------
10 recon_decay
------------ 0.08577799797058105 backwards -------------------------------
10 recon_decay
------------ 0.08592700958251953 backwards -------------------------------
10 recon_decay
------------ 0.08693242073059082 backwards -------------------------------
10 recon_decay
------------ 0.08686494827270508 backwards -------------------------------

 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                            | 205/378 [00:28<00:24,  7.19it/s]
------------ 0.08562517166137695 backwards -------------------------------
10 recon_decay
------------ 0.08561968803405762 backwards -------------------------------
10 recon_decay
------------ 0.08604669570922852 backwards -------------------------------
10 recon_decay
------------ 0.08587384223937988 backwards -------------------------------
10 recon_decay
------------ 0.08600926399230957 backwards -------------------------------
10 recon_decay
------------ 0.08697628974914551 backwards -------------------------------
10 recon_decay
------------ 0.08660316467285156 backwards -------------------------------
10 recon_decay
------------ 0.08593869209289551 backwards -------------------------------
10 recon_decay
------------ 0.08661150932312012 backwards -------------------------------
10 recon_decay
------------ 0.08554267883300781 backwards -------------------------------
10 recon_decay
------------ 0.08651971817016602 backwards -------------------------------
10 recon_decay
------------ 0.08573412895202637 backwards -------------------------------
10 recon_decay
------------ 0.08626317977905273 backwards -------------------------------
10 recon_decay
------------ 0.08656835556030273 backwards -------------------------------
10 recon_decay
------------ 0.08599328994750977 backwards -------------------------------

 58%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                  | 219/378 [00:30<00:22,  7.13it/s]
------------ 0.08703875541687012 backwards -------------------------------
10 recon_decay
------------ 0.08573484420776367 backwards -------------------------------
10 recon_decay
------------ 0.08668684959411621 backwards -------------------------------
10 recon_decay
------------ 0.08582353591918945 backwards -------------------------------
10 recon_decay
------------ 0.08657360076904297 backwards -------------------------------
10 recon_decay
------------ 0.08772420883178711 backwards -------------------------------
10 recon_decay
------------ 0.08591413497924805 backwards -------------------------------
10 recon_decay
------------ 0.0856020450592041 backwards -------------------------------
10 recon_decay
------------ 0.08618640899658203 backwards -------------------------------
10 recon_decay
------------ 0.0865163803100586 backwards -------------------------------
10 recon_decay
------------ 0.08680248260498047 backwards -------------------------------
10 recon_decay
------------ 0.08719944953918457 backwards -------------------------------
10 recon_decay
------------ 0.08593869209289551 backwards -------------------------------
10 recon_decay
------------ 0.0860743522644043 backwards -------------------------------

 62%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                       | 234/378 [00:32<00:20,  7.18it/s]
------------ 0.08626198768615723 backwards -------------------------------
10 recon_decay
------------ 0.08628559112548828 backwards -------------------------------
10 recon_decay
------------ 0.08643341064453125 backwards -------------------------------
10 recon_decay
------------ 0.08646130561828613 backwards -------------------------------
10 recon_decay
------------ 0.08582067489624023 backwards -------------------------------
10 recon_decay
------------ 0.08615946769714355 backwards -------------------------------
10 recon_decay
------------ 0.08668303489685059 backwards -------------------------------
10 recon_decay
------------ 0.08626651763916016 backwards -------------------------------
10 recon_decay
------------ 0.0859076976776123 backwards -------------------------------
10 recon_decay
------------ 0.0861349105834961 backwards -------------------------------
10 recon_decay
------------ 0.08575701713562012 backwards -------------------------------
10 recon_decay
------------ 0.08657598495483398 backwards -------------------------------
10 recon_decay
------------ 0.08566927909851074 backwards -------------------------------
10 recon_decay
------------ 0.08675503730773926 backwards -------------------------------

 66%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                             | 248/378 [00:34<00:18,  7.14it/s]
------------ 0.08561086654663086 backwards -------------------------------
10 recon_decay
------------ 0.08594679832458496 backwards -------------------------------
10 recon_decay
------------ 0.08584189414978027 backwards -------------------------------
10 recon_decay
------------ 0.08554506301879883 backwards -------------------------------
10 recon_decay
------------ 0.08643889427185059 backwards -------------------------------
10 recon_decay
------------ 0.08594679832458496 backwards -------------------------------
10 recon_decay
------------ 0.08637380599975586 backwards -------------------------------
10 recon_decay
------------ 0.08581113815307617 backwards -------------------------------
10 recon_decay
------------ 0.08675646781921387 backwards -------------------------------
10 recon_decay
------------ 0.0856924057006836 backwards -------------------------------
10 recon_decay
------------ 0.08645343780517578 backwards -------------------------------
10 recon_decay
------------ 0.08551263809204102 backwards -------------------------------
10 recon_decay
------------ 0.08565378189086914 backwards -------------------------------
10 recon_decay
------------ 0.08604598045349121 backwards -------------------------------
10 recon_decay
------------ 0.08576226234436035 backwards -------------------------------
10 recon_decay
------------ 0.08606219291687012 backwards -------------------------------
10 recon_decay
------------ 0.0859079360961914 backwards -------------------------------
10 recon_decay
------------ 0.08553171157836914 backwards -------------------------------
10 recon_decay
------------ 0.08678627014160156 backwards -------------------------------
10 recon_decay
------------ 0.0857381820678711 backwards -------------------------------
10 recon_decay
------------ 0.08622217178344727 backwards -------------------------------
10 recon_decay
------------ 0.08508849143981934 backwards -------------------------------
10 recon_decay
------------ 0.0854654312133789 backwards -------------------------------
10 recon_decay
------------ 0.08581089973449707 backwards -------------------------------
10 recon_decay
------------ 0.09114694595336914 backwards -------------------------------
10 recon_decay
------------ 0.08535981178283691 backwards -------------------------------
10 recon_decay
------------ 0.08574461936950684 backwards -------------------------------
10 recon_decay
------------ 0.0863950252532959 backwards -------------------------------
10 recon_decay
------------ 0.08570122718811035 backwards -------------------------------


 73%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                        | 277/378 [00:38<00:14,  7.09it/s]
------------ 0.0858912467956543 backwards -------------------------------
10 recon_decay
------------ 0.0853126049041748 backwards -------------------------------
10 recon_decay
------------ 0.08664774894714355 backwards -------------------------------
10 recon_decay
------------ 0.08584427833557129 backwards -------------------------------
10 recon_decay
------------ 0.08579635620117188 backwards -------------------------------
10 recon_decay
------------ 0.08607125282287598 backwards -------------------------------
10 recon_decay
------------ 0.08584380149841309 backwards -------------------------------
10 recon_decay
------------ 0.08536505699157715 backwards -------------------------------
10 recon_decay
------------ 0.08604574203491211 backwards -------------------------------
10 recon_decay
------------ 0.08643889427185059 backwards -------------------------------
10 recon_decay
------------ 0.08569097518920898 backwards -------------------------------
10 recon_decay
------------ 0.08543062210083008 backwards -------------------------------
10 recon_decay
------------ 0.0868995189666748 backwards -------------------------------
10 recon_decay
------------ 0.08915591239929199 backwards -------------------------------
10 recon_decay
------------ 0.08629512786865234 backwards -------------------------------
10 recon_decay
------------ 0.08564352989196777 backwards -------------------------------
10 recon_decay
------------ 0.08698368072509766 backwards -------------------------------
10 recon_decay
------------ 0.08633756637573242 backwards -------------------------------
10 recon_decay
------------ 0.08621740341186523 backwards -------------------------------
10 recon_decay
------------ 0.08653473854064941 backwards -------------------------------
10 recon_decay
------------ 0.08624863624572754 backwards -------------------------------
10 recon_decay
------------ 0.08631181716918945 backwards -------------------------------
10 recon_decay
------------ 0.08589577674865723 backwards -------------------------------
10 recon_decay
------------ 0.08776521682739258 backwards -------------------------------
10 recon_decay
------------ 0.08593177795410156 backwards -------------------------------
10 recon_decay
------------ 0.08915376663208008 backwards -------------------------------
10 recon_decay
------------ 0.09650087356567383 backwards -------------------------------
10 recon_decay
------------ 0.08725643157958984 backwards -------------------------------

 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                 | 287/378 [00:40<00:14,  6.44it/s]
------------ 0.1302204132080078 backwards -------------------------------
10 recon_decay
------------ 0.09230852127075195 backwards -------------------------------
10 recon_decay
------------ 0.09442448616027832 backwards -------------------------------
10 recon_decay
------------ 0.11035704612731934 backwards -------------------------------
10 recon_decay
------------ 0.08918309211730957 backwards -------------------------------
10 recon_decay
------------ 0.09149718284606934 backwards -------------------------------
10 recon_decay
------------ 0.09260344505310059 backwards -------------------------------
10 recon_decay
------------ 0.08747506141662598 backwards -------------------------------
10 recon_decay
------------ 0.1556408405303955 backwards -------------------------------
10 recon_decay
------------ 0.09364128112792969 backwards -------------------------------
10 recon_decay
------------ 0.09217596054077148 backwards -------------------------------
10 recon_decay
------------ 0.13465428352355957 backwards -------------------------------

 79%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                        | 300/378 [00:42<00:13,  5.88it/s]
------------ 0.09563350677490234 backwards -------------------------------
10 recon_decay
------------ 0.08804988861083984 backwards -------------------------------
10 recon_decay
------------ 0.09207701683044434 backwards -------------------------------
10 recon_decay
------------ 0.08625197410583496 backwards -------------------------------
10 recon_decay
------------ 0.13574934005737305 backwards -------------------------------
10 recon_decay
------------ 0.08772730827331543 backwards -------------------------------
10 recon_decay
------------ 0.0895836353302002 backwards -------------------------------
10 recon_decay
------------ 0.08825397491455078 backwards -------------------------------
10 recon_decay
------------ 0.09647107124328613 backwards -------------------------------
10 recon_decay
------------ 0.08609938621520996 backwards -------------------------------
10 recon_decay
------------ 0.08648228645324707 backwards -------------------------------
10 recon_decay
------------ 0.09185314178466797 backwards -------------------------------
10 recon_decay
------------ 0.08749032020568848 backwards -------------------------------


 86%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                     | 326/378 [00:46<00:08,  6.33it/s]
------------ 0.09311437606811523 backwards -------------------------------
10 recon_decay
------------ 0.08640027046203613 backwards -------------------------------
10 recon_decay
------------ 0.08658313751220703 backwards -------------------------------
10 recon_decay
------------ 0.08638668060302734 backwards -------------------------------
10 recon_decay
------------ 0.08705019950866699 backwards -------------------------------
10 recon_decay
------------ 0.1045234203338623 backwards -------------------------------
10 recon_decay
------------ 0.12670683860778809 backwards -------------------------------
10 recon_decay
------------ 0.08712172508239746 backwards -------------------------------
10 recon_decay
------------ 0.09148502349853516 backwards -------------------------------
10 recon_decay
------------ 0.08877444267272949 backwards -------------------------------
10 recon_decay
------------ 0.08939266204833984 backwards -------------------------------
10 recon_decay
------------ 0.09087729454040527 backwards -------------------------------
10 recon_decay
------------ 0.08959341049194336 backwards -------------------------------

 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                           | 340/378 [00:48<00:05,  6.92it/s]
------------ 0.08653426170349121 backwards -------------------------------
10 recon_decay
------------ 0.0862581729888916 backwards -------------------------------
10 recon_decay
------------ 0.08618950843811035 backwards -------------------------------
10 recon_decay
------------ 0.08694028854370117 backwards -------------------------------
10 recon_decay
------------ 0.08630180358886719 backwards -------------------------------
10 recon_decay
------------ 0.0862736701965332 backwards -------------------------------
10 recon_decay
------------ 0.08616304397583008 backwards -------------------------------
10 recon_decay
------------ 0.08715629577636719 backwards -------------------------------
10 recon_decay
------------ 0.08663249015808105 backwards -------------------------------
10 recon_decay
------------ 0.08675384521484375 backwards -------------------------------
10 recon_decay
------------ 0.08725976943969727 backwards -------------------------------
10 recon_decay
------------ 0.08619809150695801 backwards -------------------------------
10 recon_decay
------------ 0.08715438842773438 backwards -------------------------------
10 recon_decay
------------ 0.08769679069519043 backwards -------------------------------
10 recon_decay
------------ 0.0867159366607666 backwards -------------------------------
10 recon_decay
------------ 0.08582806587219238 backwards -------------------------------
10 recon_decay
------------ 0.08695721626281738 backwards -------------------------------
10 recon_decay
------------ 0.08664083480834961 backwards -------------------------------
10 recon_decay
------------ 0.08553791046142578 backwards -------------------------------
10 recon_decay
------------ 0.08626985549926758 backwards -------------------------------
10 recon_decay
------------ 0.08692622184753418 backwards -------------------------------
10 recon_decay
------------ 0.08675384521484375 backwards -------------------------------
10 recon_decay
------------ 0.08868694305419922 backwards -------------------------------
10 recon_decay
------------ 0.08987736701965332 backwards -------------------------------
10 recon_decay
------------ 0.08926153182983398 backwards -------------------------------
10 recon_decay
------------ 0.08860158920288086 backwards -------------------------------
10 recon_decay
------------ 0.08907341957092285 backwards -------------------------------
10 recon_decay
------------ 0.08748650550842285 backwards -------------------------------

 94%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                 | 354/378 [00:50<00:03,  6.84it/s]
------------ 0.08760261535644531 backwards -------------------------------
10 recon_decay
------------ 0.0872962474822998 backwards -------------------------------
10 recon_decay
------------ 0.08769702911376953 backwards -------------------------------
10 recon_decay
------------ 0.0883946418762207 backwards -------------------------------
10 recon_decay
------------ 0.08742094039916992 backwards -------------------------------
10 recon_decay
------------ 0.08753013610839844 backwards -------------------------------
10 recon_decay
------------ 0.0892794132232666 backwards -------------------------------
10 recon_decay
------------ 0.08890199661254883 backwards -------------------------------
10 recon_decay
------------ 0.0891563892364502 backwards -------------------------------
10 recon_decay
------------ 0.08805227279663086 backwards -------------------------------
10 recon_decay
------------ 0.08746170997619629 backwards -------------------------------
10 recon_decay
------------ 0.08790922164916992 backwards -------------------------------
10 recon_decay
------------ 0.08772993087768555 backwards -------------------------------

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 377/378 [00:53<00:00,  6.98it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 22%|████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                    | 10/45 [00:54<01:35,  2.73s/it]
------------ 0.08639407157897949 backwards -------------------------------
10 recon_decay
------------ 0.08595681190490723 backwards -------------------------------
10 recon_decay
------------ 0.08632087707519531 backwards -------------------------------
10 recon_decay
------------ 0.08800864219665527 backwards -------------------------------
10 recon_decay
------------ 0.08661746978759766 backwards -------------------------------
10 recon_decay
------------ 0.08527851104736328 backwards -------------------------------
10 recon_decay
------------ 0.08730459213256836 backwards -------------------------------
10 recon_decay
------------ 0.08643794059753418 backwards -------------------------------
10 recon_decay
------------ 0.0870981216430664 backwards -------------------------------
10 recon_decay
------------ 0.08629846572875977 backwards -------------------------------
10 recon_decay
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 43/45 [00:55<00:00, 12.80it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)


 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                          | 99/126 [01:00<00:01, 14.20it/s]
saving model......................................................
done
------------ 62.20615911483765 seg time alll epoch -------------------------------
saving model......................................................

/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py:913: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.16007946 0.1613622  0.16192864 ... 0.99814117 0.9981457  0.9981646 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.16005814 0.16341375 0.16362956 ... 0.998018   0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.16362956 0.16418909 0.1646207  ... 0.9980293  0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.15953502 0.16199172 0.16241464 ... 0.9981457  0.9981476  0.9981646 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.1624585  0.16304837 0.16362958 ... 0.997997   0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.16007946 0.1613622  0.16192864 ... 0.99814117 0.9981457  0.9981646 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.16005814 0.16341375 0.16362956 ... 0.998018   0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.16362956 0.16418909 0.1646207  ... 0.9980293  0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.15953502 0.16199172 0.16241464 ... 0.9981457  0.9981476  0.9981646 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.1624585  0.16304837 0.16362958 ... 0.997997   0.99803513 0.9981457 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None ULord ULord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/168/exp2d_DF_3_2d True ulord2d_168 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
here seg decay
here recon_loss
 ... (more hidden) ...[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 ... (more hidden) ...
10 recon_decay
------------ 0.09031343460083008 backwards -------------------------------
10 recon_decay
------------ 0.08851385116577148 backwards -------------------------------
10 recon_decay
------------ 0.08570265769958496 backwards -------------------------------
10 recon_decay
------------ 0.08580994606018066 backwards -------------------------------
10 recon_decay
------------ 0.08461380004882812 backwards -------------------------------
10 recon_decay
------------ 0.08509254455566406 backwards -------------------------------
10 recon_decay
------------ 0.0865936279296875 backwards -------------------------------
10 recon_decay
------------ 0.08519911766052246 backwards -------------------------------
10 recon_decay
------------ 0.08557343482971191 backwards -------------------------------
10 recon_decay
------------ 0.0847780704498291 backwards -------------------------------
10 recon_decay
------------ 0.08573508262634277 backwards -------------------------------
10 recon_decay
------------ 0.08511519432067871 backwards -------------------------------
10 recon_decay
------------ 0.0850992202758789 backwards -------------------------------
10 recon_decay
------------ 0.08443427085876465 backwards -------------------------------
10 recon_decay
------------ 0.08503365516662598 backwards -------------------------------
10 recon_decay
------------ 0.08489155769348145 backwards -------------------------------
10 recon_decay
------------ 0.09221959114074707 backwards -------------------------------
10 recon_decay
------------ 0.09437870979309082 backwards -------------------------------
10 recon_decay
------------ 0.08636355400085449 backwards -------------------------------
10 recon_decay
------------ 0.09071564674377441 backwards -------------------------------
10 recon_decay
------------ 0.08518052101135254 backwards -------------------------------
10 recon_decay
------------ 0.08841443061828613 backwards -------------------------------
10 recon_decay
------------ 0.09510374069213867 backwards -------------------------------
10 recon_decay
------------ 0.08685183525085449 backwards -------------------------------
10 recon_decay
------------ 0.08575320243835449 backwards -------------------------------
10 recon_decay
------------ 0.08622097969055176 backwards -------------------------------
10 recon_decay
------------ 0.08448028564453125 backwards -------------------------------
10 recon_decay
------------ 0.0847928524017334 backwards -------------------------------
10 recon_decay
------------ 0.0891413688659668 backwards -------------------------------
10 recon_decay
------------ 0.08835101127624512 backwards -------------------------------
10 recon_decay
------------ 0.08917570114135742 backwards -------------------------------
10 recon_decay
------------ 0.08775019645690918 backwards -------------------------------
10 recon_decay
------------ 0.08480358123779297 backwards -------------------------------
10 recon_decay
------------ 0.08601188659667969 backwards -------------------------------
10 recon_decay
------------ 0.08440184593200684 backwards -------------------------------
10 recon_decay
------------ 0.08561897277832031 backwards -------------------------------
10 recon_decay
------------ 0.08713412284851074 backwards -------------------------------
10 recon_decay
------------ 0.0853724479675293 backwards -------------------------------
10 recon_decay
------------ 0.08845734596252441 backwards -------------------------------
10 recon_decay
------------ 0.08643794059753418 backwards -------------------------------
10 recon_decay
------------ 0.0854482650756836 backwards -------------------------------
10 recon_decay
------------ 0.0925743579864502 backwards -------------------------------
10 recon_decay
------------ 0.09636425971984863 backwards -------------------------------
10 recon_decay
------------ 0.09672975540161133 backwards -------------------------------
10 recon_decay
------------ 0.09759402275085449 backwards -------------------------------
10 recon_decay
------------ 0.09276747703552246 backwards -------------------------------
10 recon_decay
------------ 0.09619736671447754 backwards -------------------------------
10 recon_decay
------------ 0.08742642402648926 backwards -------------------------------
10 recon_decay
------------ 0.08578801155090332 backwards -------------------------------
10 recon_decay
------------ 0.08525586128234863 backwards -------------------------------
10 recon_decay
------------ 0.08583760261535645 backwards -------------------------------
10 recon_decay
------------ 0.32966113090515137 backwards -------------------------------
10 recon_decay
------------ 0.08482694625854492 backwards -------------------------------
10 recon_decay
------------ 0.10960936546325684 backwards -------------------------------
10 recon_decay
------------ 0.08498001098632812 backwards -------------------------------
10 recon_decay
------------ 0.17056035995483398 backwards -------------------------------
10 recon_decay
------------ 0.22166919708251953 backwards -------------------------------
10 recon_decay
------------ 0.08613252639770508 backwards -------------------------------
10 recon_decay
------------ 0.08520627021789551 backwards -------------------------------
10 recon_decay
------------ 0.18141722679138184 backwards -------------------------------
10 recon_decay
------------ 0.08576107025146484 backwards -------------------------------
10 recon_decay
------------ 0.2878403663635254 backwards -------------------------------
10 recon_decay
------------ 0.30023860931396484 backwards -------------------------------
10 recon_decay
------------ 0.16605424880981445 backwards -------------------------------
10 recon_decay
------------ 0.08530592918395996 backwards -------------------------------
10 recon_decay
------------ 0.19872713088989258 backwards -------------------------------
10 recon_decay
------------ 0.08500266075134277 backwards -------------------------------
10 recon_decay
------------ 0.16721463203430176 backwards -------------------------------
10 recon_decay
------------ 0.19121050834655762 backwards -------------------------------
10 recon_decay
------------ 0.0858457088470459 backwards -------------------------------
10 recon_decay
------------ 0.12668538093566895 backwards -------------------------------
10 recon_decay
------------ 0.30831146240234375 backwards -------------------------------
10 recon_decay
------------ 0.08547687530517578 backwards -------------------------------
10 recon_decay
------------ 0.29924702644348145 backwards -------------------------------
10 recon_decay
------------ 0.2798304557800293 backwards -------------------------------
10 recon_decay
------------ 0.08581352233886719 backwards -------------------------------
10 recon_decay
------------ 0.3058922290802002 backwards -------------------------------
10 recon_decay
------------ 0.0871434211730957 backwards -------------------------------
10 recon_decay
------------ 0.08678388595581055 backwards -------------------------------
10 recon_decay
------------ 0.1349797248840332 backwards -------------------------------
10 recon_decay
------------ 0.19308686256408691 backwards -------------------------------
10 recon_decay
------------ 0.2840874195098877 backwards -------------------------------
10 recon_decay
------------ 0.24499225616455078 backwards -------------------------------
10 recon_decay
------------ 0.0862879753112793 backwards -------------------------------
10 recon_decay
------------ 0.2771575450897217 backwards -------------------------------
10 recon_decay
------------ 0.08650565147399902 backwards -------------------------------
10 recon_decay
------------ 0.08690762519836426 backwards -------------------------------
10 recon_decay
------------ 0.26226162910461426 backwards -------------------------------
10 recon_decay
------------ 0.08669424057006836 backwards -------------------------------
10 recon_decay
------------ 0.08633208274841309 backwards -------------------------------
10 recon_decay
------------ 0.08621001243591309 backwards -------------------------------
10 recon_decay
------------ 0.32176971435546875 backwards -------------------------------
10 recon_decay
------------ 0.0865473747253418 backwards -------------------------------
10 recon_decay
------------ 0.08588433265686035 backwards -------------------------------
10 recon_decay
------------ 0.08562302589416504 backwards -------------------------------
10 recon_decay
------------ 0.08631396293640137 backwards -------------------------------
10 recon_decay
------------ 0.08670806884765625 backwards -------------------------------
10 recon_decay
------------ 0.08530569076538086 backwards -------------------------------
10 recon_decay
------------ 0.08515334129333496 backwards -------------------------------
10 recon_decay
------------ 0.08573150634765625 backwards -------------------------------
10 recon_decay
------------ 0.08580374717712402 backwards -------------------------------
10 recon_decay
------------ 0.08540654182434082 backwards -------------------------------
10 recon_decay
------------ 0.08533668518066406 backwards -------------------------------
10 recon_decay
------------ 0.08577728271484375 backwards -------------------------------
10 recon_decay
------------ 0.08532977104187012 backwards -------------------------------
10 recon_decay
------------ 0.28438687324523926 backwards -------------------------------
10 recon_decay
------------ 0.08483409881591797 backwards -------------------------------
10 recon_decay
------------ 0.08485221862792969 backwards -------------------------------
10 recon_decay
------------ 0.08545684814453125 backwards -------------------------------
10 recon_decay
------------ 0.08527517318725586 backwards -------------------------------
10 recon_decay
------------ 0.0849153995513916 backwards -------------------------------
10 recon_decay
------------ 0.08519887924194336 backwards -------------------------------
10 recon_decay
------------ 0.08574318885803223 backwards -------------------------------
10 recon_decay
------------ 0.08488965034484863 backwards -------------------------------
10 recon_decay
------------ 0.08480310440063477 backwards -------------------------------
10 recon_decay
------------ 0.0849311351776123 backwards -------------------------------
10 recon_decay
------------ 0.08565235137939453 backwards -------------------------------
10 recon_decay
------------ 0.085052490234375 backwards -------------------------------
10 recon_decay
------------ 0.0850977897644043 backwards -------------------------------
10 recon_decay
------------ 0.0852658748626709 backwards -------------------------------
10 recon_decay
------------ 0.2308967113494873 backwards -------------------------------
10 recon_decay
------------ 0.1713097095489502 backwards -------------------------------
10 recon_decay
------------ 0.08713126182556152 backwards -------------------------------
10 recon_decay
------------ 0.08546876907348633 backwards -------------------------------
10 recon_decay
------------ 0.08731269836425781 backwards -------------------------------
10 recon_decay
------------ 0.26717138290405273 backwards -------------------------------
10 recon_decay
------------ 0.08603215217590332 backwards -------------------------------
10 recon_decay
------------ 0.08619880676269531 backwards -------------------------------
10 recon_decay
------------ 0.08487582206726074 backwards -------------------------------
10 recon_decay
------------ 0.08576846122741699 backwards -------------------------------
10 recon_decay
------------ 0.08614945411682129 backwards -------------------------------
10 recon_decay
------------ 0.08603286743164062 backwards -------------------------------
10 recon_decay
------------ 0.08572912216186523 backwards -------------------------------
10 recon_decay
------------ 0.08499789237976074 backwards -------------------------------
10 recon_decay
------------ 0.08585786819458008 backwards -------------------------------
10 recon_decay
------------ 0.08586788177490234 backwards -------------------------------
10 recon_decay
------------ 0.08687496185302734 backwards -------------------------------
10 recon_decay
 ... (more hidden) ...
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 ... (more hidden) ...
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 ... (more hidden) ...
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
10 recon_decay
------------ 0.10391068458557129 backwards -------------------------------
10 recon_decay
------------ 0.08575797080993652 backwards -------------------------------
10 recon_decay
------------ 0.33800554275512695 backwards -------------------------------
10 recon_decay
------------ 0.09658646583557129 backwards -------------------------------
10 recon_decay
------------ 0.26543378829956055 backwards -------------------------------
10 recon_decay
------------ 0.09520077705383301 backwards -------------------------------
10 recon_decay
------------ 0.09566974639892578 backwards -------------------------------
10 recon_decay
------------ 0.4548065662384033 backwards -------------------------------
10 recon_decay
------------ 0.08602643013000488 backwards -------------------------------
10 recon_decay
------------ 0.0860908031463623 backwards -------------------------------
10 recon_decay
------------ 0.0926523208618164 backwards -------------------------------
10 recon_decay
------------ 0.08564496040344238 backwards -------------------------------
10 recon_decay
------------ 0.08653450012207031 backwards -------------------------------
10 recon_decay
------------ 0.08579277992248535 backwards -------------------------------
10 recon_decay
------------ 0.16685175895690918 backwards -------------------------------
10 recon_decay
------------ 0.08592057228088379 backwards -------------------------------
10 recon_decay
------------ 0.101959228515625 backwards -------------------------------
10 recon_decay
------------ 0.08619356155395508 backwards -------------------------------
10 recon_decay
------------ 0.08581066131591797 backwards -------------------------------
10 recon_decay
------------ 0.0860753059387207 backwards -------------------------------
10 recon_decay
------------ 0.08559274673461914 backwards -------------------------------
10 recon_decay
------------ 0.08539938926696777 backwards -------------------------------
10 recon_decay
------------ 0.08537554740905762 backwards -------------------------------
10 recon_decay
------------ 0.08556222915649414 backwards -------------------------------
10 recon_decay
------------ 0.0855870246887207 backwards -------------------------------
10 recon_decay
------------ 0.08573794364929199 backwards -------------------------------
10 recon_decay
------------ 0.08666753768920898 backwards -------------------------------
10 recon_decay
------------ 0.08592462539672852 backwards -------------------------------
10 recon_decay
------------ 0.08608293533325195 backwards -------------------------------
10 recon_decay
------------ 0.0860905647277832 backwards -------------------------------
10 recon_decay
------------ 0.08585429191589355 backwards -------------------------------
10 recon_decay
------------ 0.0855567455291748 backwards -------------------------------
10 recon_decay
------------ 0.08537435531616211 backwards -------------------------------
10 recon_decay
------------ 0.08534502983093262 backwards -------------------------------
10 recon_decay
------------ 0.08677935600280762 backwards -------------------------------
10 recon_decay
------------ 0.08533906936645508 backwards -------------------------------
10 recon_decay
------------ 0.08610200881958008 backwards -------------------------------
10 recon_decay
------------ 0.08568930625915527 backwards -------------------------------
10 recon_decay
------------ 0.08568072319030762 backwards -------------------------------
10 recon_decay
------------ 0.08561325073242188 backwards -------------------------------
10 recon_decay
------------ 0.08673930168151855 backwards -------------------------------
10 recon_decay
------------ 0.08602595329284668 backwards -------------------------------
10 recon_decay
------------ 0.08583545684814453 backwards -------------------------------
10 recon_decay
------------ 0.08535432815551758 backwards -------------------------------
10 recon_decay
------------ 0.08631753921508789 backwards -------------------------------
10 recon_decay
------------ 0.08608508110046387 backwards -------------------------------
10 recon_decay
------------ 0.08547329902648926 backwards -------------------------------
10 recon_decay
------------ 0.08549213409423828 backwards -------------------------------
10 recon_decay
------------ 0.08570981025695801 backwards -------------------------------
10 recon_decay
------------ 0.0864567756652832 backwards -------------------------------
10 recon_decay
------------ 0.0870208740234375 backwards -------------------------------
10 recon_decay
------------ 0.08563947677612305 backwards -------------------------------
10 recon_decay
------------ 0.08565473556518555 backwards -------------------------------
10 recon_decay
------------ 0.08529067039489746 backwards -------------------------------
10 recon_decay
------------ 0.08552837371826172 backwards -------------------------------
10 recon_decay
------------ 0.08555483818054199 backwards -------------------------------
10 recon_decay
------------ 0.08560633659362793 backwards -------------------------------
10 recon_decay
------------ 0.08556365966796875 backwards -------------------------------
10 recon_decay
------------ 0.08662533760070801 backwards -------------------------------
10 recon_decay
------------ 0.08574891090393066 backwards -------------------------------
10 recon_decay
------------ 0.08666419982910156 backwards -------------------------------
10 recon_decay
------------ 0.08573293685913086 backwards -------------------------------
10 recon_decay
------------ 0.0862424373626709 backwards -------------------------------
10 recon_decay
------------ 0.08569145202636719 backwards -------------------------------
10 recon_decay
------------ 0.0862877368927002 backwards -------------------------------
10 recon_decay
------------ 0.08527898788452148 backwards -------------------------------
10 recon_decay
------------ 0.086181640625 backwards -------------------------------
10 recon_decay
------------ 0.08553671836853027 backwards -------------------------------
10 recon_decay
------------ 0.0860893726348877 backwards -------------------------------
10 recon_decay
------------ 0.08562564849853516 backwards -------------------------------
10 recon_decay
------------ 0.08569788932800293 backwards -------------------------------
10 recon_decay
------------ 0.08638453483581543 backwards -------------------------------
10 recon_decay
------------ 0.08605647087097168 backwards -------------------------------
10 recon_decay
------------ 0.08574438095092773 backwards -------------------------------
10 recon_decay
------------ 0.08584260940551758 backwards -------------------------------
10 recon_decay
------------ 0.08573675155639648 backwards -------------------------------
10 recon_decay
------------ 0.08551883697509766 backwards -------------------------------
10 recon_decay
------------ 0.08541631698608398 backwards -------------------------------
10 recon_decay
------------ 0.0921316146850586 backwards -------------------------------
10 recon_decay
------------ 0.08620047569274902 backwards -------------------------------
10 recon_decay
------------ 0.0880577564239502 backwards -------------------------------
10 recon_decay
------------ 0.08742642402648926 backwards -------------------------------
10 recon_decay
------------ 0.08614563941955566 backwards -------------------------------
10 recon_decay
------------ 0.08705425262451172 backwards -------------------------------
10 recon_decay
------------ 0.08608174324035645 backwards -------------------------------
10 recon_decay
------------ 0.0861358642578125 backwards -------------------------------
10 recon_decay
------------ 0.08770990371704102 backwards -------------------------------
10 recon_decay
------------ 0.09080147743225098 backwards -------------------------------
10 recon_decay
------------ 0.0885009765625 backwards -------------------------------
10 recon_decay
------------ 0.08688569068908691 backwards -------------------------------
10 recon_decay
------------ 0.08784675598144531 backwards -------------------------------
10 recon_decay
------------ 0.08561420440673828 backwards -------------------------------
10 recon_decay
------------ 0.08545088768005371 backwards -------------------------------
10 recon_decay
------------ 0.08563232421875 backwards -------------------------------
10 recon_decay
------------ 0.0901346206665039 backwards -------------------------------
10 recon_decay
------------ 0.11942243576049805 backwards -------------------------------
10 recon_decay
------------ 0.08581185340881348 backwards -------------------------------
10 recon_decay
------------ 0.08665966987609863 backwards -------------------------------
10 recon_decay
------------ 0.08602190017700195 backwards -------------------------------
10 recon_decay
------------ 0.08630609512329102 backwards -------------------------------
10 recon_decay
------------ 0.0854942798614502 backwards -------------------------------
10 recon_decay
------------ 0.08624887466430664 backwards -------------------------------
10 recon_decay
------------ 0.10431385040283203 backwards -------------------------------
10 recon_decay
------------ 0.10893726348876953 backwards -------------------------------
10 recon_decay
------------ 0.08628392219543457 backwards -------------------------------
10 recon_decay
------------ 0.0858919620513916 backwards -------------------------------
10 recon_decay
------------ 0.08634495735168457 backwards -------------------------------
10 recon_decay
------------ 0.0859677791595459 backwards -------------------------------
10 recon_decay
------------ 0.08687019348144531 backwards -------------------------------
10 recon_decay
------------ 0.08604860305786133 backwards -------------------------------
10 recon_decay
------------ 0.08549189567565918 backwards -------------------------------
10 recon_decay
------------ 0.08644676208496094 backwards -------------------------------
10 recon_decay
------------ 0.0868380069732666 backwards -------------------------------
10 recon_decay
------------ 0.08622193336486816 backwards -------------------------------
10 recon_decay
------------ 0.08620047569274902 backwards -------------------------------
10 recon_decay
------------ 0.11598753929138184 backwards -------------------------------
10 recon_decay
------------ 0.10109567642211914 backwards -------------------------------
10 recon_decay
------------ 0.08571553230285645 backwards -------------------------------
10 recon_decay
------------ 0.08699536323547363 backwards -------------------------------
10 recon_decay
------------ 0.08751988410949707 backwards -------------------------------
10 recon_decay
------------ 0.0867464542388916 backwards -------------------------------
10 recon_decay
------------ 0.08599996566772461 backwards -------------------------------
10 recon_decay
------------ 0.08745884895324707 backwards -------------------------------
10 recon_decay
------------ 0.08591198921203613 backwards -------------------------------
10 recon_decay
------------ 0.08718323707580566 backwards -------------------------------
10 recon_decay
------------ 0.08588027954101562 backwards -------------------------------
10 recon_decay
------------ 0.08631277084350586 backwards -------------------------------
10 recon_decay
------------ 0.08612179756164551 backwards -------------------------------
10 recon_decay
------------ 0.08607196807861328 backwards -------------------------------
10 recon_decay
------------ 0.12386870384216309 backwards -------------------------------
10 recon_decay
------------ 0.09595465660095215 backwards -------------------------------
10 recon_decay
------------ 0.0863032341003418 backwards -------------------------------
10 recon_decay
------------ 0.0858309268951416 backwards -------------------------------
10 recon_decay
------------ 0.08585858345031738 backwards -------------------------------
10 recon_decay
------------ 0.08608746528625488 backwards -------------------------------
10 recon_decay
------------ 0.08612823486328125 backwards -------------------------------
10 recon_decay
------------ 0.0875539779663086 backwards -------------------------------
10 recon_decay
------------ 0.08676624298095703 backwards -------------------------------
10 recon_decay
------------ 0.08580636978149414 backwards -------------------------------
10 recon_decay
------------ 0.0858144760131836 backwards -------------------------------
10 recon_decay
------------ 0.08604788780212402 backwards -------------------------------
10 recon_decay
------------ 0.0867314338684082 backwards -------------------------------
10 recon_decay
------------ 0.08654165267944336 backwards -------------------------------
10 recon_decay
------------ 0.08742427825927734 backwards -------------------------------
10 recon_decay
------------ 0.08592891693115234 backwards -------------------------------
10 recon_decay
------------ 0.08632111549377441 backwards -------------------------------
10 recon_decay
------------ 0.0858919620513916 backwards -------------------------------
10 recon_decay
------------ 0.08687448501586914 backwards -------------------------------
10 recon_decay
------------ 0.08623313903808594 backwards -------------------------------
10 recon_decay
------------ 0.08629179000854492 backwards -------------------------------
10 recon_decay
------------ 0.08623790740966797 backwards -------------------------------
10 recon_decay
------------ 0.10788679122924805 backwards -------------------------------
10 recon_decay
------------ 0.11428952217102051 backwards -------------------------------
10 recon_decay
------------ 0.08728718757629395 backwards -------------------------------
10 recon_decay
------------ 0.08779382705688477 backwards -------------------------------
10 recon_decay
------------ 0.08668899536132812 backwards -------------------------------
10 recon_decay
------------ 0.08597302436828613 backwards -------------------------------
10 recon_decay
------------ 0.08592987060546875 backwards -------------------------------
10 recon_decay
------------ 0.09667682647705078 backwards -------------------------------
10 recon_decay
------------ 0.11972355842590332 backwards -------------------------------
10 recon_decay
------------ 0.09649777412414551 backwards -------------------------------
10 recon_decay
------------ 0.08642792701721191 backwards -------------------------------
10 recon_decay
------------ 0.08718109130859375 backwards -------------------------------
10 recon_decay
------------ 0.10988140106201172 backwards -------------------------------
10 recon_decay
------------ 0.11515426635742188 backwards -------------------------------
10 recon_decay
------------ 0.08723139762878418 backwards -------------------------------
10 recon_decay
------------ 0.08580470085144043 backwards -------------------------------
10 recon_decay
------------ 0.0859827995300293 backwards -------------------------------
10 recon_decay
------------ 0.08595728874206543 backwards -------------------------------
10 recon_decay
------------ 0.0867764949798584 backwards -------------------------------
10 recon_decay
------------ 0.08687591552734375 backwards -------------------------------
10 recon_decay
------------ 0.08613824844360352 backwards -------------------------------
10 recon_decay
------------ 0.08698892593383789 backwards -------------------------------
10 recon_decay
------------ 0.0869901180267334 backwards -------------------------------
10 recon_decay
------------ 0.08614277839660645 backwards -------------------------------
10 recon_decay
------------ 0.08649754524230957 backwards -------------------------------
10 recon_decay
------------ 0.08580970764160156 backwards -------------------------------
10 recon_decay
------------ 0.08654069900512695 backwards -------------------------------
10 recon_decay
------------ 0.08663821220397949 backwards -------------------------------
10 recon_decay
------------ 0.08625435829162598 backwards -------------------------------
10 recon_decay
------------ 0.08605647087097168 backwards -------------------------------
10 recon_decay
------------ 0.08673381805419922 backwards -------------------------------
10 recon_decay
------------ 0.08625388145446777 backwards -------------------------------
10 recon_decay
------------ 0.08631253242492676 backwards -------------------------------
10 recon_decay
------------ 0.08670330047607422 backwards -------------------------------
10 recon_decay
------------ 0.08679628372192383 backwards -------------------------------
10 recon_decay
------------ 0.08649325370788574 backwards -------------------------------
10 recon_decay
------------ 0.08630943298339844 backwards -------------------------------
10 recon_decay
------------ 0.0868539810180664 backwards -------------------------------
10 recon_decay
------------ 0.08774948120117188 backwards -------------------------------
10 recon_decay
------------ 0.08741140365600586 backwards -------------------------------
10 recon_decay
------------ 0.08623790740966797 backwards -------------------------------
10 recon_decay
------------ 0.08629488945007324 backwards -------------------------------
10 recon_decay
------------ 0.08678269386291504 backwards -------------------------------
10 recon_decay
------------ 0.0863802433013916 backwards -------------------------------
10 recon_decay
------------ 0.0861515998840332 backwards -------------------------------
10 recon_decay
------------ 0.08717775344848633 backwards -------------------------------
10 recon_decay
------------ 0.0868382453918457 backwards -------------------------------
10 recon_decay
------------ 0.08655238151550293 backwards -------------------------------
10 recon_decay
------------ 0.08701920509338379 backwards -------------------------------
10 recon_decay
------------ 0.08695006370544434 backwards -------------------------------
10 recon_decay
------------ 0.0871882438659668 backwards -------------------------------
10 recon_decay
------------ 0.08665180206298828 backwards -------------------------------
10 recon_decay
------------ 0.08686518669128418 backwards -------------------------------
10 recon_decay
------------ 0.12219953536987305 backwards -------------------------------
10 recon_decay
------------ 0.09406590461730957 backwards -------------------------------
10 recon_decay
------------ 0.08735489845275879 backwards -------------------------------
10 recon_decay
------------ 0.08677148818969727 backwards -------------------------------
10 recon_decay
------------ 0.08637857437133789 backwards -------------------------------
10 recon_decay
------------ 0.08660650253295898 backwards -------------------------------
10 recon_decay
------------ 0.0877370834350586 backwards -------------------------------
10 recon_decay
------------ 0.0869450569152832 backwards -------------------------------
10 recon_decay
------------ 0.08668661117553711 backwards -------------------------------
10 recon_decay
------------ 0.08614540100097656 backwards -------------------------------
10 recon_decay
------------ 0.08678817749023438 backwards -------------------------------
10 recon_decay
------------ 0.08642983436584473 backwards -------------------------------
10 recon_decay
------------ 0.08630752563476562 backwards -------------------------------
10 recon_decay
------------ 0.08729147911071777 backwards -------------------------------
10 recon_decay
------------ 0.08678650856018066 backwards -------------------------------
10 recon_decay
------------ 0.08614444732666016 backwards -------------------------------
10 recon_decay
------------ 0.08632373809814453 backwards -------------------------------
10 recon_decay
------------ 0.08565568923950195 backwards -------------------------------
10 recon_decay
------------ 0.08592915534973145 backwards -------------------------------
10 recon_decay
------------ 0.0861515998840332 backwards -------------------------------
10 recon_decay
------------ 0.08612346649169922 backwards -------------------------------
10 recon_decay
------------ 0.0875096321105957 backwards -------------------------------
10 recon_decay
------------ 0.08564019203186035 backwards -------------------------------
10 recon_decay
------------ 0.08555459976196289 backwards -------------------------------
10 recon_decay
------------ 0.08593463897705078 backwards -------------------------------
10 recon_decay
------------ 0.08653593063354492 backwards -------------------------------
10 recon_decay
------------ 0.08576035499572754 backwards -------------------------------
10 recon_decay
------------ 0.08611798286437988 backwards -------------------------------
10 recon_decay
------------ 0.08596229553222656 backwards -------------------------------
10 recon_decay
------------ 0.08660078048706055 backwards -------------------------------
10 recon_decay
------------ 0.08661746978759766 backwards -------------------------------
10 recon_decay
------------ 0.08605122566223145 backwards -------------------------------
10 recon_decay
------------ 0.08724355697631836 backwards -------------------------------
10 recon_decay
------------ 0.08697915077209473 backwards -------------------------------
10 recon_decay
------------ 0.08590412139892578 backwards -------------------------------
10 recon_decay
------------ 0.08655762672424316 backwards -------------------------------
saving model......................................................
done
------------ 65.92535829544067 seg time alll epoch -------------------------------
saving model......................................................
here1
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.06274109 0.07005331 0.07385751 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.04385587 0.04616507 0.04844983 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.0732856  0.0744105  0.07445687 ... 0.9963272  0.9963295  0.99637187] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.04940162 0.05318497 0.06094382 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.07341745 0.07377107 0.07544497 ... 0.9962496  0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.06274109 0.07005331 0.07385751 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.04385587 0.04616507 0.04844983 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.0732856  0.0744105  0.07445687 ... 0.9963272  0.9963295  0.99637187] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.04940162 0.05318497 0.06094382 ... 0.99629647 0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.07341745 0.07377107 0.07544497 ... 0.9962496  0.9963272  0.9963295 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNet UNet_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/168/exp2d_DF_3_2d True unet2d_168 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
arrive location
here opt
here seg decay
here recon_loss
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:  22%|█████████████████████████████████████████████████████████                                                                                                                                                                                                     | 85/378 [00:03<00:10, 26.70it/s, loss=0]
epoch #0:  37%|█████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                | 139/378 [00:05<00:08, 26.68it/s, loss=0]
epoch #0:  52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                         | 196/378 [00:07<00:06, 26.80it/s, loss=0]
epoch #0:  66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                     | 250/378 [00:09<00:04, 26.79it/s, loss=0]
epoch #0:  80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                 | 304/378 [00:11<00:02, 26.46it/s, loss=0]
epoch #0:  95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌             | 358/378 [00:13<00:00, 26.40it/s, loss=0]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:14<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:14<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:15<00:00,  2.96it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)███████████████████████████████████████████████████████████████████████████████████████████                                                                                           | 30/45 [00:15<00:04,  3.40it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 121/126 [00:16<00:00, 39.44it/s]
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 121/126 [00:16<00:00, 39.44it/s]
saving model......................................................
done
------------ 16.617642402648926 seg time alll epoch -------------------------------
saving model......................................................
here1
saving model......................................................
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.24928322 0.24944162 0.24948311 ... 0.99779415 0.9977943  0.9977944 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.24574219 0.24662109 0.24712501 ... 0.99779475 0.9977957  0.99779713] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.24664214 0.2471075  0.24728747 ... 0.9977932  0.9977933  0.99779344] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.24649672 0.24783687 0.24809031 ... 0.99779284 0.99779296 0.9977931 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.24826552 0.24876422 0.24888147 ... 0.9977912  0.9977913  0.9977914 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.24928322 0.24944162 0.24948311 ... 0.99779415 0.9977943  0.9977944 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.24574219 0.24662109 0.24712501 ... 0.99779475 0.9977957  0.99779713] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.24664214 0.2471075  0.24728747 ... 0.9977932  0.9977933  0.99779344] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.24649672 0.24783687 0.24809031 ... 0.99779284 0.99779296 0.9977931 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.24826552 0.24876422 0.24888147 ... 0.9977912  0.9977913  0.9977914 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNetC UNetC_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/168/exp2d_DF_3_2d True unet2dC_168 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
here opt
here seg decay
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
    angle1 = random.choice(self.angles)ing Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1131, in __getattr__
    type(self).__name__, name))
AttributeError: 'UNetSC' object has no attribute 'angles'
    angle1 = random.choice(self.angles)ing Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
    angle1 = random.choice(self.angles)ing Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]