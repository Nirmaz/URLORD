True cuda
args: None URLord URLord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/212/exp2d_DF_3_2d True urlord2d_212exp2d_DF_3_2d DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
here seg decay
here recon_loss
/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
1 recon_decay
------------ 0.10277795791625977 loss -------------------------------
------------ 0.2907581329345703 backwards -------------------------------
1 recon_decay
------------ 0.08717775344848633 loss -------------------------------
------------ 0.2716236114501953 backwards -------------------------------
1 recon_decay
------------ 0.08407235145568848 loss -------------------------------
------------ 0.27306699752807617 backwards -------------------------------
1 recon_decay
  0%|                                                                                                                                                                                                                                                                                        | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                             | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                             | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   1%|██                                                                                                                                                                                                                                                          | 3/378 [00:01<03:26,  1.81it/s, loss=127]
  1%|██▏                                                                                                                                                                                                                                                                             | 3/378 [00:01<03:26,  1.81it/s]
------------ 0.08440709114074707 loss -------------------------------
------------ 0.2721686363220215 backwards -------------------------------
1 recon_decay
------------ 0.08797836303710938 loss -------------------------------
------------ 0.2716970443725586 backwards -------------------------------
1 recon_decay
------------ 0.08561372756958008 loss -------------------------------
------------ 0.27171754837036133 backwards -------------------------------
1 recon_decay
------------ 0.08503150939941406 loss -------------------------------
------------ 0.2720925807952881 backwards -------------------------------
1 recon_decay
------------ 0.08495879173278809 loss -------------------------------
------------ 0.273190975189209 backwards -------------------------------

  File "lord_exp.py", line 140, in <module>                                                                                                                                                                                                                                         | 10/378 [00:04<02:33,  2.41it/s]
    main()
  File "lord_exp.py", line 133, in main
    args.func(args)
  File "lord_exp.py", line 114, in run_exp
    train_model(model_dict, path_new_exp, model_name, exp_dict['data_l_name'], exp_dict['data_u_name'], exp_dict['data_v_name'], exp_dict['data_t_name'], exp_dict['base_dir'])
  File "lord_exp.py", line 31, in train_model
    take_from_arg = False)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/lord_unet.py", line 367, in train_ulord
    loaded_model=load_model
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 434, in train
    segs_t, classes_t, model_dir, tensorboard_dir, loaded_model, dim)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 529, in train_URLordModel
    self.training_model(model_dir, tensorboard_dir)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 706, in training_model
    reco_loss_epoch, reco_loss_epoch_witha, i)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 389, in do_step_ulord
    loss.backward()
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/_tensor.py", line 255, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph, inputs=inputs)
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/autograd/__init__.py", line 149, in backward
    allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag
KeyboardInterrupt
------------ 0.08508801460266113 loss -------------------------------
------------ 0.2721083164215088 backwards -------------------------------
1 recon_decay
------------ 0.08889460563659668 loss -------------------------------
------------ 0.27280426025390625 backwards -------------------------------
1 recon_decay
------------ 0.08574676513671875 loss -------------------------------