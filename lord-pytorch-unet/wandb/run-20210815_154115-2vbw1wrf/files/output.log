True cuda
args: None URLord URLord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/167/exp2d_DF_3_2d True urlord2d_167 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
here seg decay
here recon_loss
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   2%|█████▎                                                                                                                                                                                                                                                      | 8/378 [00:01<01:08,  5.38it/s, loss=3.38]
  2%|█████▊                                                                                                                                                                                                                                                                           | 8/378 [00:01<01:08,  5.38it/s]
10 recon_decay
------------ 0.09696459770202637 backwards -------------------------------
10 recon_decay
------------ 0.08503413200378418 backwards -------------------------------
10 recon_decay
------------ 0.21210694313049316 backwards -------------------------------
10 recon_decay
------------ 0.08611488342285156 backwards -------------------------------
10 recon_decay
------------ 0.08506655693054199 backwards -------------------------------
10 recon_decay
------------ 0.08617067337036133 backwards -------------------------------
10 recon_decay
------------ 0.1595759391784668 backwards -------------------------------
10 recon_decay
------------ 0.08524942398071289 backwards -------------------------------
10 recon_decay
------------ 0.15896177291870117 backwards -------------------------------
10 recon_decay
------------ 0.08612227439880371 backwards -------------------------------

  5%|█████████████▋                                                                                                                                                                                                                                                                  | 19/378 [00:03<01:08,  5.26it/s]
------------ 0.16573429107666016 backwards -------------------------------
10 recon_decay
------------ 0.08627653121948242 backwards -------------------------------
10 recon_decay
------------ 0.08537697792053223 backwards -------------------------------
10 recon_decay
------------ 0.08548855781555176 backwards -------------------------------
10 recon_decay
------------ 0.12357783317565918 backwards -------------------------------
10 recon_decay
------------ 0.11774468421936035 backwards -------------------------------
10 recon_decay
------------ 0.08547830581665039 backwards -------------------------------
10 recon_decay
------------ 0.18439006805419922 backwards -------------------------------
10 recon_decay
------------ 0.08624982833862305 backwards -------------------------------
10 recon_decay
------------ 0.23089241981506348 backwards -------------------------------
10 recon_decay
------------ 0.08567118644714355 backwards -------------------------------

  8%|████████████████████▊                                                                                                                                                                                                                                                           | 29/378 [00:05<01:08,  5.07it/s]
------------ 0.16892743110656738 backwards -------------------------------
10 recon_decay
------------ 0.08605670928955078 backwards -------------------------------
10 recon_decay
------------ 0.08537721633911133 backwards -------------------------------
10 recon_decay
------------ 0.22818875312805176 backwards -------------------------------
10 recon_decay
------------ 0.08635640144348145 backwards -------------------------------
10 recon_decay
------------ 0.2548844814300537 backwards -------------------------------
10 recon_decay
------------ 0.08569860458374023 backwards -------------------------------
10 recon_decay
------------ 0.14479541778564453 backwards -------------------------------
10 recon_decay
------------ 0.11471414566040039 backwards -------------------------------
10 recon_decay
------------ 0.08621358871459961 backwards -------------------------------
10 recon_decay
------------ 0.08597970008850098 backwards -------------------------------

 10%|████████████████████████████                                                                                                                                                                                                                                                    | 39/378 [00:07<01:06,  5.09it/s]
------------ 0.08537793159484863 backwards -------------------------------
10 recon_decay
------------ 0.08567142486572266 backwards -------------------------------
10 recon_decay
------------ 0.0855855941772461 backwards -------------------------------
10 recon_decay
------------ 0.4000687599182129 backwards -------------------------------
10 recon_decay
------------ 0.2617459297180176 backwards -------------------------------
10 recon_decay
------------ 0.08491277694702148 backwards -------------------------------
10 recon_decay
------------ 0.08483171463012695 backwards -------------------------------
10 recon_decay
------------ 0.08627033233642578 backwards -------------------------------
10 recon_decay
------------ 0.08546662330627441 backwards -------------------------------
10 recon_decay
------------ 0.08549928665161133 backwards -------------------------------

 13%|███████████████████████████████████▎                                                                                                                                                                                                                                            | 49/378 [00:09<01:03,  5.15it/s]
------------ 0.25493383407592773 backwards -------------------------------
10 recon_decay
------------ 0.08579087257385254 backwards -------------------------------
10 recon_decay
------------ 0.16347074508666992 backwards -------------------------------
10 recon_decay
------------ 0.0866689682006836 backwards -------------------------------
10 recon_decay
------------ 0.1304306983947754 backwards -------------------------------
10 recon_decay
------------ 0.10068988800048828 backwards -------------------------------
10 recon_decay
------------ 0.08672142028808594 backwards -------------------------------
10 recon_decay
------------ 0.28133440017700195 backwards -------------------------------
10 recon_decay
------------ 0.08630728721618652 backwards -------------------------------
10 recon_decay
------------ 0.08528280258178711 backwards -------------------------------

 17%|█████████████████████████████████████████████▎                                                                                                                                                                                                                                  | 63/378 [00:11<00:42,  7.33it/s]
------------ 0.08597588539123535 backwards -------------------------------
10 recon_decay
------------ 0.08462977409362793 backwards -------------------------------
10 recon_decay
------------ 0.0860750675201416 backwards -------------------------------
10 recon_decay
------------ 0.08504414558410645 backwards -------------------------------
10 recon_decay
------------ 0.08587145805358887 backwards -------------------------------
10 recon_decay
------------ 0.0851602554321289 backwards -------------------------------
10 recon_decay
------------ 0.08501362800598145 backwards -------------------------------
10 recon_decay
------------ 0.08549737930297852 backwards -------------------------------
10 recon_decay
------------ 0.08617353439331055 backwards -------------------------------
10 recon_decay
------------ 0.0852808952331543 backwards -------------------------------
10 recon_decay
------------ 0.08514618873596191 backwards -------------------------------

 20%|██████████████████████████████████████████████████████▋                                                                                                                                                                                                                         | 76/378 [00:14<00:42,  7.11it/s]
------------ 0.27940845489501953 backwards -------------------------------
10 recon_decay
------------ 0.08599400520324707 backwards -------------------------------
10 recon_decay
------------ 0.08655142784118652 backwards -------------------------------
10 recon_decay
------------ 0.11840081214904785 backwards -------------------------------
10 recon_decay
------------ 0.14379167556762695 backwards -------------------------------
10 recon_decay
------------ 0.08542442321777344 backwards -------------------------------
10 recon_decay
------------ 0.08552336692810059 backwards -------------------------------
10 recon_decay
------------ 0.08565258979797363 backwards -------------------------------
10 recon_decay
------------ 0.0868844985961914 backwards -------------------------------
10 recon_decay
------------ 0.08591723442077637 backwards -------------------------------
10 recon_decay
------------ 0.08506035804748535 backwards -------------------------------
10 recon_decay
------------ 0.08532571792602539 backwards -------------------------------

 23%|███████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                | 88/378 [00:16<00:40,  7.10it/s]
------------ 0.08605456352233887 backwards -------------------------------
10 recon_decay
------------ 0.0858461856842041 backwards -------------------------------
10 recon_decay
------------ 0.15336132049560547 backwards -------------------------------
10 recon_decay
------------ 0.08620142936706543 backwards -------------------------------
10 recon_decay
------------ 0.21287775039672852 backwards -------------------------------
10 recon_decay
------------ 0.08663225173950195 backwards -------------------------------
10 recon_decay
------------ 0.08559298515319824 backwards -------------------------------
10 recon_decay
------------ 0.08546972274780273 backwards -------------------------------
10 recon_decay
------------ 0.08592486381530762 backwards -------------------------------
10 recon_decay
------------ 0.08620023727416992 backwards -------------------------------
10 recon_decay
------------ 0.08559703826904297 backwards -------------------------------
10 recon_decay
------------ 0.08587312698364258 backwards -------------------------------

 27%|████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                                      | 101/378 [00:18<00:41,  6.61it/s]
------------ 0.08580183982849121 backwards -------------------------------
10 recon_decay
------------ 0.0847938060760498 backwards -------------------------------
10 recon_decay
------------ 0.08570384979248047 backwards -------------------------------
10 recon_decay
------------ 0.08504486083984375 backwards -------------------------------
10 recon_decay
------------ 0.08658146858215332 backwards -------------------------------
10 recon_decay
------------ 0.08540940284729004 backwards -------------------------------
10 recon_decay
------------ 0.10202145576477051 backwards -------------------------------
10 recon_decay
------------ 0.10435771942138672 backwards -------------------------------
10 recon_decay
------------ 0.0862886905670166 backwards -------------------------------
10 recon_decay
------------ 0.21639418601989746 backwards -------------------------------
10 recon_decay
------------ 0.08625602722167969 backwards -------------------------------
10 recon_decay
------------ 0.08623337745666504 backwards -------------------------------
10 recon_decay
------------ 0.08548355102539062 backwards -------------------------------

 30%|████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                              | 112/378 [00:20<00:46,  5.70it/s]
------------ 0.08679866790771484 backwards -------------------------------
10 recon_decay
------------ 0.0864570140838623 backwards -------------------------------
10 recon_decay
------------ 0.13052845001220703 backwards -------------------------------
10 recon_decay
------------ 0.0909731388092041 backwards -------------------------------
10 recon_decay
------------ 0.08613324165344238 backwards -------------------------------
10 recon_decay
------------ 0.08650827407836914 backwards -------------------------------
10 recon_decay
------------ 0.10358381271362305 backwards -------------------------------
10 recon_decay
------------ 0.13835406303405762 backwards -------------------------------
10 recon_decay
------------ 0.08642029762268066 backwards -------------------------------
10 recon_decay
------------ 0.11377596855163574 backwards -------------------------------
10 recon_decay
------------ 0.08597636222839355 backwards -------------------------------

 33%|████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                      | 123/378 [00:22<00:42,  6.03it/s]
------------ 0.08738136291503906 backwards -------------------------------
10 recon_decay
------------ 0.2337033748626709 backwards -------------------------------
10 recon_decay
------------ 0.0873727798461914 backwards -------------------------------
10 recon_decay
------------ 0.08543181419372559 backwards -------------------------------
10 recon_decay
------------ 0.0862281322479248 backwards -------------------------------
10 recon_decay
------------ 0.08525466918945312 backwards -------------------------------
10 recon_decay
------------ 0.24561071395874023 backwards -------------------------------
10 recon_decay
------------ 0.08595705032348633 backwards -------------------------------
10 recon_decay
------------ 0.2467801570892334 backwards -------------------------------
10 recon_decay
------------ 0.08616137504577637 backwards -------------------------------
10 recon_decay
------------ 0.0854501724243164 backwards -------------------------------

 35%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                               | 134/378 [00:24<00:36,  6.67it/s]
------------ 0.08523797988891602 backwards -------------------------------
10 recon_decay
------------ 0.08650493621826172 backwards -------------------------------
10 recon_decay
------------ 0.08621358871459961 backwards -------------------------------
10 recon_decay
------------ 0.27657556533813477 backwards -------------------------------
10 recon_decay
------------ 0.08702826499938965 backwards -------------------------------
10 recon_decay
------------ 0.0865328311920166 backwards -------------------------------
10 recon_decay
------------ 0.26048898696899414 backwards -------------------------------
10 recon_decay
------------ 0.08584403991699219 backwards -------------------------------
10 recon_decay
------------ 0.0862572193145752 backwards -------------------------------
10 recon_decay
------------ 0.08538079261779785 backwards -------------------------------
10 recon_decay
------------ 0.08600044250488281 backwards -------------------------------
10 recon_decay
------------ 0.08509111404418945 backwards -------------------------------

 39%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                     | 147/378 [00:26<00:32,  7.05it/s]
------------ 0.2712111473083496 backwards -------------------------------
10 recon_decay
------------ 0.08609843254089355 backwards -------------------------------
10 recon_decay
------------ 0.08623266220092773 backwards -------------------------------
10 recon_decay
------------ 0.0904088020324707 backwards -------------------------------
10 recon_decay
------------ 0.19202017784118652 backwards -------------------------------
10 recon_decay
------------ 0.08650684356689453 backwards -------------------------------
10 recon_decay
------------ 0.08611249923706055 backwards -------------------------------
10 recon_decay
------------ 0.08692097663879395 backwards -------------------------------
10 recon_decay
------------ 0.08614635467529297 backwards -------------------------------
10 recon_decay
------------ 0.08627843856811523 backwards -------------------------------
10 recon_decay
------------ 0.08527350425720215 backwards -------------------------------

 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                              | 157/378 [00:28<00:45,  4.90it/s]
------------ 0.08660888671875 backwards -------------------------------
10 recon_decay
------------ 0.08608698844909668 backwards -------------------------------
10 recon_decay
------------ 0.09837484359741211 backwards -------------------------------
10 recon_decay
------------ 0.08510112762451172 backwards -------------------------------
10 recon_decay
------------ 0.08669471740722656 backwards -------------------------------
10 recon_decay
------------ 0.08571481704711914 backwards -------------------------------
10 recon_decay
------------ 0.08610129356384277 backwards -------------------------------
10 recon_decay
------------ 0.08545422554016113 backwards -------------------------------
10 recon_decay
------------ 0.2899923324584961 backwards -------------------------------
10 recon_decay
------------ 0.08653616905212402 backwards -------------------------------

 44%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                       | 167/378 [00:30<00:37,  5.69it/s]
------------ 0.2911808490753174 backwards -------------------------------
10 recon_decay
------------ 0.08645439147949219 backwards -------------------------------
10 recon_decay
------------ 0.3182191848754883 backwards -------------------------------
10 recon_decay
------------ 0.08675193786621094 backwards -------------------------------
10 recon_decay
------------ 0.2528364658355713 backwards -------------------------------
10 recon_decay
------------ 0.08533549308776855 backwards -------------------------------
10 recon_decay
------------ 0.08472299575805664 backwards -------------------------------
10 recon_decay
------------ 0.0849919319152832 backwards -------------------------------
10 recon_decay
------------ 0.08557295799255371 backwards -------------------------------
10 recon_decay
------------ 0.30239391326904297 backwards -------------------------------

 47%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                              | 179/378 [00:32<00:29,  6.82it/s]
------------ 0.08650469779968262 backwards -------------------------------
10 recon_decay
------------ 0.08573126792907715 backwards -------------------------------
10 recon_decay
------------ 0.08597254753112793 backwards -------------------------------
10 recon_decay
------------ 0.08532452583312988 backwards -------------------------------
10 recon_decay
------------ 0.08515763282775879 backwards -------------------------------
10 recon_decay
------------ 0.08527994155883789 backwards -------------------------------
10 recon_decay
------------ 0.08541631698608398 backwards -------------------------------
10 recon_decay
------------ 0.08491277694702148 backwards -------------------------------
10 recon_decay
------------ 0.13971352577209473 backwards -------------------------------
10 recon_decay
------------ 0.08633804321289062 backwards -------------------------------
10 recon_decay
------------ 0.08693218231201172 backwards -------------------------------
10 recon_decay
------------ 0.08565402030944824 backwards -------------------------------

 51%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                    | 193/378 [00:34<00:30,  5.98it/s]
------------ 0.08650708198547363 backwards -------------------------------
10 recon_decay
------------ 0.08562827110290527 backwards -------------------------------
10 recon_decay
------------ 0.08502864837646484 backwards -------------------------------
10 recon_decay
------------ 0.08521389961242676 backwards -------------------------------
10 recon_decay
------------ 0.08522295951843262 backwards -------------------------------
10 recon_decay
------------ 0.08544683456420898 backwards -------------------------------
10 recon_decay
------------ 0.08475208282470703 backwards -------------------------------
10 recon_decay
------------ 0.08570551872253418 backwards -------------------------------
10 recon_decay
------------ 0.08533120155334473 backwards -------------------------------
10 recon_decay
------------ 0.08541488647460938 backwards -------------------------------
10 recon_decay
------------ 0.08522534370422363 backwards -------------------------------
10 recon_decay
------------ 0.0853884220123291 backwards -------------------------------
10 recon_decay
------------ 0.08460259437561035 backwards -------------------------------

 54%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                            | 204/378 [00:36<00:38,  4.53it/s]
------------ 0.08581209182739258 backwards -------------------------------
10 recon_decay
------------ 0.09830808639526367 backwards -------------------------------
10 recon_decay
------------ 0.08479022979736328 backwards -------------------------------
10 recon_decay
------------ 0.08496761322021484 backwards -------------------------------
10 recon_decay
------------ 0.0858762264251709 backwards -------------------------------
10 recon_decay
------------ 0.318997859954834 backwards -------------------------------
10 recon_decay
------------ 0.0856931209564209 backwards -------------------------------
10 recon_decay
------------ 0.08546090126037598 backwards -------------------------------
10 recon_decay
------------ 0.08550906181335449 backwards -------------------------------
10 recon_decay
------------ 0.08624148368835449 backwards -------------------------------
10 recon_decay
------------ 0.0859379768371582 backwards -------------------------------
10 recon_decay
------------ 0.085174560546875 backwards -------------------------------
10 recon_decay
------------ 0.08573150634765625 backwards -------------------------------

 57%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                     | 214/378 [00:38<00:26,  6.17it/s]
------------ 0.3584780693054199 backwards -------------------------------
10 recon_decay
------------ 0.2594120502471924 backwards -------------------------------
10 recon_decay
------------ 0.08593201637268066 backwards -------------------------------
10 recon_decay
------------ 0.27973055839538574 backwards -------------------------------
10 recon_decay
------------ 0.08545756340026855 backwards -------------------------------
10 recon_decay
------------ 0.08526062965393066 backwards -------------------------------
10 recon_decay
------------ 0.31090521812438965 backwards -------------------------------
10 recon_decay
------------ 0.08576178550720215 backwards -------------------------------

 59%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 224/378 [00:40<00:31,  4.88it/s]
------------ 0.08653616905212402 backwards -------------------------------
10 recon_decay
------------ 0.08556604385375977 backwards -------------------------------
10 recon_decay
------------ 0.08635187149047852 backwards -------------------------------
10 recon_decay
------------ 0.08559131622314453 backwards -------------------------------
10 recon_decay
------------ 0.08569121360778809 backwards -------------------------------
10 recon_decay
------------ 0.08565378189086914 backwards -------------------------------
10 recon_decay
------------ 0.08539366722106934 backwards -------------------------------
10 recon_decay
------------ 0.36450934410095215 backwards -------------------------------
10 recon_decay
------------ 0.08633899688720703 backwards -------------------------------
10 recon_decay
------------ 0.08582186698913574 backwards -------------------------------
10 recon_decay
------------ 0.08521103858947754 backwards -------------------------------

 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                     | 236/378 [00:42<00:24,  5.82it/s]
------------ 0.09134697914123535 backwards -------------------------------
10 recon_decay
------------ 0.08542370796203613 backwards -------------------------------
10 recon_decay
------------ 0.09042143821716309 backwards -------------------------------
10 recon_decay
------------ 0.08654069900512695 backwards -------------------------------
10 recon_decay
------------ 0.08576416969299316 backwards -------------------------------
10 recon_decay
------------ 0.08582043647766113 backwards -------------------------------
10 recon_decay
------------ 0.08547687530517578 backwards -------------------------------
10 recon_decay
------------ 0.25394177436828613 backwards -------------------------------
10 recon_decay
------------ 0.08671283721923828 backwards -------------------------------
10 recon_decay
------------ 0.08659863471984863 backwards -------------------------------
10 recon_decay
------------ 0.08540892601013184 backwards -------------------------------
10 recon_decay
------------ 0.261002779006958 backwards -------------------------------

 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 250/378 [00:44<00:17,  7.36it/s]
------------ 0.08741593360900879 backwards -------------------------------
10 recon_decay
------------ 0.085845947265625 backwards -------------------------------
10 recon_decay
------------ 0.09040045738220215 backwards -------------------------------
10 recon_decay
------------ 0.0868682861328125 backwards -------------------------------
10 recon_decay
------------ 0.08555936813354492 backwards -------------------------------
10 recon_decay
------------ 0.28087854385375977 backwards -------------------------------
10 recon_decay
------------ 0.08614087104797363 backwards -------------------------------
10 recon_decay
------------ 0.08549094200134277 backwards -------------------------------
10 recon_decay
------------ 0.09105753898620605 backwards -------------------------------
10 recon_decay
------------ 0.08646321296691895 backwards -------------------------------
10 recon_decay
------------ 0.08565616607666016 backwards -------------------------------
10 recon_decay
------------ 0.08600378036499023 backwards -------------------------------
10 recon_decay
------------ 0.08632612228393555 backwards -------------------------------

 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                 | 265/378 [00:46<00:15,  7.47it/s]
------------ 0.08614468574523926 backwards -------------------------------
10 recon_decay
------------ 0.08526086807250977 backwards -------------------------------
10 recon_decay
------------ 0.08562922477722168 backwards -------------------------------
10 recon_decay
------------ 0.08623003959655762 backwards -------------------------------
10 recon_decay
------------ 0.08600091934204102 backwards -------------------------------
10 recon_decay
------------ 0.08607339859008789 backwards -------------------------------
10 recon_decay
------------ 0.08557558059692383 backwards -------------------------------
10 recon_decay
------------ 0.0866551399230957 backwards -------------------------------
10 recon_decay
------------ 0.08564615249633789 backwards -------------------------------
10 recon_decay
------------ 0.08617901802062988 backwards -------------------------------
10 recon_decay
------------ 0.0910637378692627 backwards -------------------------------
10 recon_decay
------------ 0.08582019805908203 backwards -------------------------------
10 recon_decay
------------ 0.08556485176086426 backwards -------------------------------
10 recon_decay
------------ 0.08671045303344727 backwards -------------------------------
10 recon_decay
------------ 0.08571219444274902 backwards -------------------------------

 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                      | 280/378 [00:48<00:13,  7.44it/s]
------------ 0.0853278636932373 backwards -------------------------------
10 recon_decay
------------ 0.08615708351135254 backwards -------------------------------
10 recon_decay
------------ 0.08549737930297852 backwards -------------------------------
10 recon_decay
------------ 0.08607125282287598 backwards -------------------------------
10 recon_decay
------------ 0.08616185188293457 backwards -------------------------------
10 recon_decay
------------ 0.08560347557067871 backwards -------------------------------
10 recon_decay
------------ 0.08549880981445312 backwards -------------------------------
10 recon_decay
------------ 0.08570599555969238 backwards -------------------------------
10 recon_decay
------------ 0.08646368980407715 backwards -------------------------------
10 recon_decay
------------ 0.08553695678710938 backwards -------------------------------
10 recon_decay
------------ 0.08527731895446777 backwards -------------------------------
10 recon_decay
------------ 0.08616518974304199 backwards -------------------------------
10 recon_decay
------------ 0.08549094200134277 backwards -------------------------------
10 recon_decay
------------ 0.08593106269836426 backwards -------------------------------
10 recon_decay
------------ 0.08550095558166504 backwards -------------------------------

 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 293/378 [00:50<00:13,  6.14it/s]
------------ 0.08619284629821777 backwards -------------------------------
10 recon_decay
------------ 0.08597731590270996 backwards -------------------------------
10 recon_decay
------------ 0.08640575408935547 backwards -------------------------------
10 recon_decay
------------ 0.08707165718078613 backwards -------------------------------
10 recon_decay
------------ 0.08590364456176758 backwards -------------------------------
10 recon_decay
------------ 0.08551836013793945 backwards -------------------------------
10 recon_decay
------------ 0.08541727066040039 backwards -------------------------------
10 recon_decay
------------ 0.08553528785705566 backwards -------------------------------
10 recon_decay
------------ 0.08587789535522461 backwards -------------------------------
10 recon_decay
------------ 0.08564138412475586 backwards -------------------------------
10 recon_decay
------------ 0.08579587936401367 backwards -------------------------------
10 recon_decay
------------ 0.08633589744567871 backwards -------------------------------
10 recon_decay
------------ 0.20053362846374512 backwards -------------------------------

 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                     | 303/378 [00:51<00:10,  7.40it/s]
------------ 0.08750653266906738 backwards -------------------------------
10 recon_decay
------------ 0.0869290828704834 backwards -------------------------------
10 recon_decay
------------ 0.08677434921264648 backwards -------------------------------
10 recon_decay
------------ 0.08743977546691895 backwards -------------------------------
10 recon_decay
------------ 0.08679509162902832 backwards -------------------------------
10 recon_decay
------------ 0.08621525764465332 backwards -------------------------------
10 recon_decay
------------ 0.08587813377380371 backwards -------------------------------
10 recon_decay
------------ 0.08645367622375488 backwards -------------------------------
10 recon_decay
------------ 0.08595776557922363 backwards -------------------------------
10 recon_decay
------------ 0.08601546287536621 backwards -------------------------------
10 recon_decay
------------ 0.0858154296875 backwards -------------------------------
10 recon_decay
------------ 0.08667349815368652 backwards -------------------------------
10 recon_decay
------------ 0.0856313705444336 backwards -------------------------------

 83%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                              | 314/378 [00:54<00:09,  6.50it/s]
------------ 0.32396936416625977 backwards -------------------------------
10 recon_decay
------------ 0.08628392219543457 backwards -------------------------------
10 recon_decay
------------ 0.35111069679260254 backwards -------------------------------
10 recon_decay
------------ 0.08688211441040039 backwards -------------------------------
10 recon_decay
------------ 0.08648562431335449 backwards -------------------------------
10 recon_decay
------------ 0.31090497970581055 backwards -------------------------------
10 recon_decay
------------ 0.08684992790222168 backwards -------------------------------
10 recon_decay
------------ 0.08936119079589844 backwards -------------------------------
10 recon_decay
------------ 0.08755803108215332 backwards -------------------------------
10 recon_decay
------------ 0.08678197860717773 backwards -------------------------------
10 recon_decay
------------ 0.08673572540283203 backwards -------------------------------

 87%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                   | 329/378 [00:56<00:06,  7.11it/s]
------------ 0.08726954460144043 backwards -------------------------------
10 recon_decay
------------ 0.08642888069152832 backwards -------------------------------
10 recon_decay
------------ 0.08553147315979004 backwards -------------------------------
10 recon_decay
------------ 0.0856785774230957 backwards -------------------------------
10 recon_decay
------------ 0.08621096611022949 backwards -------------------------------
10 recon_decay
------------ 0.08611536026000977 backwards -------------------------------
10 recon_decay
------------ 0.08633637428283691 backwards -------------------------------
10 recon_decay
------------ 0.09114956855773926 backwards -------------------------------
10 recon_decay
------------ 0.0866851806640625 backwards -------------------------------
10 recon_decay
------------ 0.0894474983215332 backwards -------------------------------
10 recon_decay
------------ 0.09711408615112305 backwards -------------------------------
10 recon_decay
------------ 0.0888981819152832 backwards -------------------------------
10 recon_decay
------------ 0.08824014663696289 backwards -------------------------------
10 recon_decay
------------ 0.08705735206604004 backwards -------------------------------
10 recon_decay
------------ 0.08783435821533203 backwards -------------------------------

 91%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 343/378 [00:58<00:04,  7.33it/s]
------------ 0.08563232421875 backwards -------------------------------
10 recon_decay
------------ 0.08646965026855469 backwards -------------------------------
10 recon_decay
------------ 0.09416818618774414 backwards -------------------------------
10 recon_decay
------------ 0.08821868896484375 backwards -------------------------------
10 recon_decay
------------ 0.0856163501739502 backwards -------------------------------
10 recon_decay
------------ 0.0859827995300293 backwards -------------------------------
10 recon_decay
------------ 0.09040188789367676 backwards -------------------------------
10 recon_decay
------------ 0.08632159233093262 backwards -------------------------------
10 recon_decay
------------ 0.08684563636779785 backwards -------------------------------
10 recon_decay
------------ 0.08735513687133789 backwards -------------------------------
10 recon_decay
------------ 0.09013748168945312 backwards -------------------------------
10 recon_decay
------------ 0.08604216575622559 backwards -------------------------------
10 recon_decay
------------ 0.08563566207885742 backwards -------------------------------
10 recon_decay
------------ 0.08549022674560547 backwards -------------------------------

 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋              | 358/378 [01:00<00:02,  7.34it/s]
------------ 0.09158706665039062 backwards -------------------------------
10 recon_decay
------------ 0.09221673011779785 backwards -------------------------------
10 recon_decay
------------ 0.08744239807128906 backwards -------------------------------
10 recon_decay
------------ 0.08792352676391602 backwards -------------------------------
10 recon_decay
------------ 0.0867764949798584 backwards -------------------------------
10 recon_decay
------------ 0.08612251281738281 backwards -------------------------------
10 recon_decay
------------ 0.08646941184997559 backwards -------------------------------
10 recon_decay
------------ 0.08591890335083008 backwards -------------------------------
10 recon_decay
------------ 0.08582854270935059 backwards -------------------------------
10 recon_decay
------------ 0.08532118797302246 backwards -------------------------------
10 recon_decay
------------ 0.08543562889099121 backwards -------------------------------
10 recon_decay
------------ 0.08713006973266602 backwards -------------------------------
10 recon_decay
------------ 0.08572697639465332 backwards -------------------------------
10 recon_decay
------------ 0.08557605743408203 backwards -------------------------------
10 recon_decay
------------ 0.08792400360107422 backwards -------------------------------

 99%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍   | 373/378 [01:02<00:00,  7.45it/s]
------------ 0.08653712272644043 backwards -------------------------------
10 recon_decay
------------ 0.09316301345825195 backwards -------------------------------
10 recon_decay
------------ 0.08586955070495605 backwards -------------------------------
10 recon_decay
------------ 0.08683013916015625 backwards -------------------------------
10 recon_decay
------------ 0.08659076690673828 backwards -------------------------------
10 recon_decay
------------ 0.08537507057189941 backwards -------------------------------
10 recon_decay
------------ 0.08589506149291992 backwards -------------------------------
10 recon_decay
------------ 0.08543729782104492 backwards -------------------------------
10 recon_decay
------------ 0.08569979667663574 backwards -------------------------------
10 recon_decay
------------ 0.0935664176940918 backwards -------------------------------
10 recon_decay
------------ 0.08668112754821777 backwards -------------------------------
10 recon_decay
------------ 0.09206366539001465 backwards -------------------------------
10 recon_decay
------------ 0.08519697189331055 backwards -------------------------------
10 recon_decay
------------ 0.0852653980255127 backwards -------------------------------
10 recon_decay
------------ 0.08626794815063477 backwards -------------------------------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 377/378 [01:02<00:00,  6.00it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                       | 28/45 [01:04<00:05,  3.29it/s]
------------ 0.08681154251098633 backwards -------------------------------
10 recon_decay
------------ 0.10184073448181152 backwards -------------------------------
10 recon_decay
------------ 0.08753681182861328 backwards -------------------------------
10 recon_decay
------------ 0.08639740943908691 backwards -------------------------------
10 recon_decay
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 43/45 [01:04<00:00, 12.14it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)


 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 124/126 [01:10<00:00, 25.59it/s]
saving model......................................................
done
------------ 70.34124398231506 seg time alll epoch -------------------------------
saving model......................................................
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py:913: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.02725521 0.02757564 0.02769409 ... 0.9986797  0.9986928  0.9988355 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.02799824 0.028427   0.02842701 ... 0.9986634  0.99866915 0.9988355 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.02679413 0.02800053 0.02830386 ... 0.9986797  0.9986928  0.9988355 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.02760944 0.02799824 0.02842701 ... 0.99866915 0.9986797  0.9988355 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.02799824 0.03018982 0.03040854 ... 0.9986634  0.99866915 0.9988355 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.02725521 0.02757564 0.02769409 ... 0.9986797  0.9986928  0.9988355 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.02799824 0.028427   0.02842701 ... 0.9986634  0.99866915 0.9988355 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.02679413 0.02800053 0.02830386 ... 0.9986797  0.9986928  0.9988355 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.02760944 0.02799824 0.02842701 ... 0.99866915 0.9986797  0.9988355 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.02799824 0.03018982 0.03040854 ... 0.9986634  0.99866915 0.9988355 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None ULord ULord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/167/exp2d_DF_3_2d True ulord2d_167 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
here seg decay
here recon_loss
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
10 recon_decay
------------ 0.09034991264343262 backwards -------------------------------
10 recon_decay
------------ 0.0850374698638916 backwards -------------------------------
10 recon_decay
------------ 0.08480668067932129 backwards -------------------------------
10 recon_decay
------------ 0.08486175537109375 backwards -------------------------------
10 recon_decay
------------ 0.08568477630615234 backwards -------------------------------
10 recon_decay
------------ 0.08922457695007324 backwards -------------------------------
10 recon_decay
------------ 0.08423280715942383 backwards -------------------------------
10 recon_decay
------------ 0.08485960960388184 backwards -------------------------------
10 recon_decay
------------ 0.0852811336517334 backwards -------------------------------
10 recon_decay
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   2%|██████                                                                                                                                                                                                                                                      | 9/378 [00:01<00:52,  6.98it/s, loss=2.22]
  2%|██████▌                                                                                                                                                                                                                                                                          | 9/378 [00:01<00:52,  6.98it/s]
------------ 0.08456230163574219 backwards -------------------------------
10 recon_decay
------------ 0.08516097068786621 backwards -------------------------------
10 recon_decay
------------ 0.08410048484802246 backwards -------------------------------
10 recon_decay
------------ 0.08416962623596191 backwards -------------------------------
10 recon_decay
------------ 0.08450722694396973 backwards -------------------------------
10 recon_decay
------------ 0.08520150184631348 backwards -------------------------------
10 recon_decay
------------ 0.08533811569213867 backwards -------------------------------
10 recon_decay
------------ 0.08448100090026855 backwards -------------------------------
10 recon_decay
------------ 0.08419537544250488 backwards -------------------------------
10 recon_decay
------------ 0.0850214958190918 backwards -------------------------------
10 recon_decay
------------ 0.08508682250976562 backwards -------------------------------
10 recon_decay
------------ 0.08449292182922363 backwards -------------------------------
10 recon_decay
------------ 0.08441329002380371 backwards -------------------------------
10 recon_decay
------------ 0.0845787525177002 backwards -------------------------------
10 recon_decay
------------ 0.08515000343322754 backwards -------------------------------

  6%|█████████████████▎                                                                                                                                                                                                                                                              | 24/378 [00:03<00:47,  7.42it/s]
------------ 0.08461284637451172 backwards -------------------------------
10 recon_decay
------------ 0.08541607856750488 backwards -------------------------------
10 recon_decay
------------ 0.08430266380310059 backwards -------------------------------
10 recon_decay
------------ 0.0847320556640625 backwards -------------------------------
10 recon_decay
------------ 0.08495640754699707 backwards -------------------------------
10 recon_decay
------------ 0.08533215522766113 backwards -------------------------------
10 recon_decay
------------ 0.08594870567321777 backwards -------------------------------
10 recon_decay
------------ 0.08570289611816406 backwards -------------------------------
10 recon_decay
------------ 0.08473038673400879 backwards -------------------------------
10 recon_decay
------------ 0.08491110801696777 backwards -------------------------------
10 recon_decay
------------ 0.0852196216583252 backwards -------------------------------
10 recon_decay
------------ 0.08453631401062012 backwards -------------------------------
10 recon_decay
------------ 0.08571147918701172 backwards -------------------------------
10 recon_decay
------------ 0.08474302291870117 backwards -------------------------------
10 recon_decay
------------ 0.08504676818847656 backwards -------------------------------

 10%|████████████████████████████                                                                                                                                                                                                                                                    | 39/378 [00:05<00:45,  7.41it/s]
------------ 0.08472442626953125 backwards -------------------------------
10 recon_decay
------------ 0.08495640754699707 backwards -------------------------------
10 recon_decay
------------ 0.08525538444519043 backwards -------------------------------
10 recon_decay
------------ 0.08489704132080078 backwards -------------------------------
10 recon_decay
------------ 0.1055452823638916 backwards -------------------------------
10 recon_decay
------------ 0.08472895622253418 backwards -------------------------------
10 recon_decay
------------ 0.08515357971191406 backwards -------------------------------
10 recon_decay
------------ 0.08484148979187012 backwards -------------------------------
10 recon_decay
------------ 0.08523321151733398 backwards -------------------------------
10 recon_decay
------------ 0.08518123626708984 backwards -------------------------------
10 recon_decay
------------ 0.0845789909362793 backwards -------------------------------
10 recon_decay
------------ 0.08474874496459961 backwards -------------------------------
10 recon_decay
------------ 0.08515548706054688 backwards -------------------------------
10 recon_decay
------------ 0.08575844764709473 backwards -------------------------------
10 recon_decay
------------ 0.08471107482910156 backwards -------------------------------
10 recon_decay
------------ 0.08467221260070801 backwards -------------------------------
10 recon_decay
------------ 0.08577990531921387 backwards -------------------------------
10 recon_decay
------------ 0.08577919006347656 backwards -------------------------------
10 recon_decay
------------ 0.08482718467712402 backwards -------------------------------
10 recon_decay
------------ 0.08514690399169922 backwards -------------------------------
10 recon_decay
------------ 0.08448910713195801 backwards -------------------------------
10 recon_decay
------------ 0.08539247512817383 backwards -------------------------------
10 recon_decay
------------ 0.08471202850341797 backwards -------------------------------
10 recon_decay
------------ 0.08508706092834473 backwards -------------------------------
10 recon_decay
------------ 0.08537077903747559 backwards -------------------------------
10 recon_decay
------------ 0.08499431610107422 backwards -------------------------------
10 recon_decay
------------ 0.08513855934143066 backwards -------------------------------
10 recon_decay
------------ 0.08466577529907227 backwards -------------------------------
10 recon_decay
------------ 0.0853121280670166 backwards -------------------------------
10 recon_decay
------------ 0.08510518074035645 backwards -------------------------------
10 recon_decay
------------ 0.08546638488769531 backwards -------------------------------
10 recon_decay
------------ 0.08467245101928711 backwards -------------------------------
10 recon_decay
------------ 0.08511090278625488 backwards -------------------------------
10 recon_decay
------------ 0.08546638488769531 backwards -------------------------------
10 recon_decay
------------ 0.08459258079528809 backwards -------------------------------
10 recon_decay
------------ 0.0855255126953125 backwards -------------------------------
10 recon_decay
------------ 0.08501911163330078 backwards -------------------------------
10 recon_decay
------------ 0.08486533164978027 backwards -------------------------------
10 recon_decay
------------ 0.08480119705200195 backwards -------------------------------
10 recon_decay
------------ 0.08592081069946289 backwards -------------------------------
10 recon_decay
------------ 0.08603358268737793 backwards -------------------------------
10 recon_decay
------------ 0.08460402488708496 backwards -------------------------------
10 recon_decay
------------ 0.08490729331970215 backwards -------------------------------
10 recon_decay
------------ 0.08542156219482422 backwards -------------------------------
10 recon_decay
------------ 0.08515024185180664 backwards -------------------------------
10 recon_decay
------------ 0.08558535575866699 backwards -------------------------------
10 recon_decay
------------ 0.08470940589904785 backwards -------------------------------
10 recon_decay
------------ 0.08498597145080566 backwards -------------------------------
10 recon_decay
------------ 0.08466887474060059 backwards -------------------------------
10 recon_decay
------------ 0.0851137638092041 backwards -------------------------------
10 recon_decay
------------ 0.08599138259887695 backwards -------------------------------
10 recon_decay
------------ 0.08498692512512207 backwards -------------------------------
10 recon_decay
------------ 0.0851287841796875 backwards -------------------------------
10 recon_decay
------------ 0.08501958847045898 backwards -------------------------------
10 recon_decay
------------ 0.08561372756958008 backwards -------------------------------
10 recon_decay
------------ 0.0860590934753418 backwards -------------------------------
10 recon_decay
------------ 0.08547639846801758 backwards -------------------------------
10 recon_decay
------------ 0.08655142784118652 backwards -------------------------------
10 recon_decay
------------ 0.08473849296569824 backwards -------------------------------
10 recon_decay
------------ 0.08479142189025879 backwards -------------------------------
10 recon_decay
------------ 0.08524012565612793 backwards -------------------------------
10 recon_decay
------------ 0.08532285690307617 backwards -------------------------------
10 recon_decay
------------ 0.08475923538208008 backwards -------------------------------
10 recon_decay
------------ 0.08525753021240234 backwards -------------------------------
10 recon_decay
------------ 0.0849449634552002 backwards -------------------------------
10 recon_decay
------------ 0.08555221557617188 backwards -------------------------------
10 recon_decay
------------ 0.08535623550415039 backwards -------------------------------
10 recon_decay
------------ 0.08533024787902832 backwards -------------------------------
10 recon_decay
------------ 0.08487272262573242 backwards -------------------------------
10 recon_decay
------------ 0.08556580543518066 backwards -------------------------------
10 recon_decay
------------ 0.08524346351623535 backwards -------------------------------
10 recon_decay
------------ 0.08488845825195312 backwards -------------------------------
10 recon_decay
------------ 0.08577656745910645 backwards -------------------------------
10 recon_decay
------------ 0.08528923988342285 backwards -------------------------------
10 recon_decay
------------ 0.08487987518310547 backwards -------------------------------
10 recon_decay
------------ 0.08587360382080078 backwards -------------------------------
10 recon_decay
------------ 0.08613133430480957 backwards -------------------------------
10 recon_decay
------------ 0.08475637435913086 backwards -------------------------------
10 recon_decay
------------ 0.08481788635253906 backwards -------------------------------
10 recon_decay
------------ 0.0852975845336914 backwards -------------------------------
10 recon_decay
------------ 0.08464789390563965 backwards -------------------------------
10 recon_decay
------------ 0.08518743515014648 backwards -------------------------------
10 recon_decay
------------ 0.0852510929107666 backwards -------------------------------
10 recon_decay
------------ 0.08604073524475098 backwards -------------------------------
10 recon_decay
------------ 0.08488631248474121 backwards -------------------------------
10 recon_decay
------------ 0.08502721786499023 backwards -------------------------------
10 recon_decay
------------ 0.08512353897094727 backwards -------------------------------
10 recon_decay
------------ 0.08598875999450684 backwards -------------------------------
10 recon_decay
------------ 0.08493828773498535 backwards -------------------------------
10 recon_decay
------------ 0.08509039878845215 backwards -------------------------------
10 recon_decay
------------ 0.08535981178283691 backwards -------------------------------
10 recon_decay
------------ 0.08515262603759766 backwards -------------------------------
10 recon_decay
------------ 0.085205078125 backwards -------------------------------
10 recon_decay
------------ 0.08477592468261719 backwards -------------------------------
10 recon_decay
------------ 0.08635544776916504 backwards -------------------------------
10 recon_decay
------------ 0.08513665199279785 backwards -------------------------------
10 recon_decay
------------ 0.08492159843444824 backwards -------------------------------
10 recon_decay
------------ 0.08517169952392578 backwards -------------------------------
10 recon_decay
------------ 0.08613872528076172 backwards -------------------------------
10 recon_decay
------------ 0.08469724655151367 backwards -------------------------------
10 recon_decay
------------ 0.08496880531311035 backwards -------------------------------
10 recon_decay
------------ 0.08615374565124512 backwards -------------------------------
10 recon_decay
------------ 0.0847177505493164 backwards -------------------------------
10 recon_decay
------------ 0.0849003791809082 backwards -------------------------------
10 recon_decay
------------ 0.08522677421569824 backwards -------------------------------
10 recon_decay
------------ 0.0859375 backwards -------------------------------
10 recon_decay
------------ 0.08489108085632324 backwards -------------------------------
10 recon_decay
------------ 0.0846242904663086 backwards -------------------------------
10 recon_decay
------------ 0.08489155769348145 backwards -------------------------------
10 recon_decay
------------ 0.08539056777954102 backwards -------------------------------
10 recon_decay
------------ 0.08505392074584961 backwards -------------------------------
10 recon_decay
------------ 0.0850226879119873 backwards -------------------------------
10 recon_decay
------------ 0.08644604682922363 backwards -------------------------------
10 recon_decay
------------ 0.08565807342529297 backwards -------------------------------
10 recon_decay
------------ 0.08526301383972168 backwards -------------------------------
10 recon_decay
------------ 0.08527851104736328 backwards -------------------------------
10 recon_decay
------------ 0.08555459976196289 backwards -------------------------------
10 recon_decay
------------ 0.08518028259277344 backwards -------------------------------
10 recon_decay
------------ 0.08515334129333496 backwards -------------------------------
10 recon_decay
------------ 0.086761474609375 backwards -------------------------------
10 recon_decay
------------ 0.08567452430725098 backwards -------------------------------
10 recon_decay
------------ 0.08567476272583008 backwards -------------------------------
10 recon_decay
------------ 0.08520197868347168 backwards -------------------------------
10 recon_decay
------------ 0.08540058135986328 backwards -------------------------------
10 recon_decay
------------ 0.08590126037597656 backwards -------------------------------
10 recon_decay
------------ 0.08515191078186035 backwards -------------------------------
10 recon_decay
------------ 0.08542966842651367 backwards -------------------------------
10 recon_decay
------------ 0.08558464050292969 backwards -------------------------------
10 recon_decay
------------ 0.08528685569763184 backwards -------------------------------
10 recon_decay
------------ 0.08532381057739258 backwards -------------------------------
10 recon_decay
------------ 0.08575010299682617 backwards -------------------------------
10 recon_decay
------------ 0.08670401573181152 backwards -------------------------------
10 recon_decay
------------ 0.08559966087341309 backwards -------------------------------
10 recon_decay
------------ 0.08508872985839844 backwards -------------------------------
10 recon_decay
------------ 0.08573317527770996 backwards -------------------------------
10 recon_decay
------------ 0.08495807647705078 backwards -------------------------------
10 recon_decay
------------ 0.08564519882202148 backwards -------------------------------
10 recon_decay
------------ 0.0860147476196289 backwards -------------------------------
10 recon_decay
------------ 0.08609271049499512 backwards -------------------------------
10 recon_decay
------------ 0.0852196216583252 backwards -------------------------------
10 recon_decay
------------ 0.08510327339172363 backwards -------------------------------
10 recon_decay
------------ 0.08607840538024902 backwards -------------------------------
10 recon_decay
------------ 0.08557987213134766 backwards -------------------------------
10 recon_decay
------------ 0.08544015884399414 backwards -------------------------------
10 recon_decay
------------ 0.0850362777709961 backwards -------------------------------
10 recon_decay
------------ 0.08597016334533691 backwards -------------------------------
10 recon_decay
------------ 0.0853724479675293 backwards -------------------------------
10 recon_decay
------------ 0.0854799747467041 backwards -------------------------------
10 recon_decay
------------ 0.08559560775756836 backwards -------------------------------
10 recon_decay
------------ 0.08556699752807617 backwards -------------------------------
10 recon_decay
------------ 0.08511829376220703 backwards -------------------------------
10 recon_decay
------------ 0.08530879020690918 backwards -------------------------------
10 recon_decay
------------ 0.08573222160339355 backwards -------------------------------
10 recon_decay
------------ 0.08559536933898926 backwards -------------------------------
10 recon_decay
------------ 0.08508920669555664 backwards -------------------------------
10 recon_decay
------------ 0.08616018295288086 backwards -------------------------------
10 recon_decay
------------ 0.08544206619262695 backwards -------------------------------
10 recon_decay
------------ 0.08611536026000977 backwards -------------------------------
10 recon_decay
------------ 0.08499717712402344 backwards -------------------------------
10 recon_decay
------------ 0.08589601516723633 backwards -------------------------------
10 recon_decay
------------ 0.08500337600708008 backwards -------------------------------
10 recon_decay
------------ 0.0857245922088623 backwards -------------------------------
10 recon_decay
------------ 0.0855259895324707 backwards -------------------------------
10 recon_decay
------------ 0.0854499340057373 backwards -------------------------------
10 recon_decay
------------ 0.08585548400878906 backwards -------------------------------
10 recon_decay
------------ 0.08532118797302246 backwards -------------------------------
10 recon_decay
------------ 0.08594417572021484 backwards -------------------------------
10 recon_decay
------------ 0.08544135093688965 backwards -------------------------------
10 recon_decay
------------ 0.08558201789855957 backwards -------------------------------
10 recon_decay
------------ 0.08553624153137207 backwards -------------------------------
10 recon_decay
------------ 0.08577442169189453 backwards -------------------------------
10 recon_decay
------------ 0.08603501319885254 backwards -------------------------------
10 recon_decay
------------ 0.08561944961547852 backwards -------------------------------
10 recon_decay
------------ 0.08594775199890137 backwards -------------------------------
10 recon_decay
------------ 0.08547544479370117 backwards -------------------------------
10 recon_decay
------------ 0.08571696281433105 backwards -------------------------------
10 recon_decay
------------ 0.08543157577514648 backwards -------------------------------
10 recon_decay
------------ 0.08486080169677734 backwards -------------------------------
10 recon_decay
------------ 0.08535242080688477 backwards -------------------------------
10 recon_decay
------------ 0.08568453788757324 backwards -------------------------------
10 recon_decay
------------ 0.08602762222290039 backwards -------------------------------
10 recon_decay
------------ 0.08504080772399902 backwards -------------------------------
10 recon_decay
------------ 0.08568191528320312 backwards -------------------------------
10 recon_decay
------------ 0.08506536483764648 backwards -------------------------------
10 recon_decay
------------ 0.08568644523620605 backwards -------------------------------
10 recon_decay
------------ 0.08520054817199707 backwards -------------------------------
10 recon_decay
------------ 0.08600711822509766 backwards -------------------------------
10 recon_decay
------------ 0.0860588550567627 backwards -------------------------------
10 recon_decay
------------ 0.08547616004943848 backwards -------------------------------
10 recon_decay
------------ 0.08558845520019531 backwards -------------------------------
10 recon_decay
------------ 0.08571076393127441 backwards -------------------------------
10 recon_decay
------------ 0.08536744117736816 backwards -------------------------------
10 recon_decay
------------ 0.08564376831054688 backwards -------------------------------
10 recon_decay
------------ 0.08546996116638184 backwards -------------------------------
10 recon_decay
------------ 0.08699989318847656 backwards -------------------------------
10 recon_decay
------------ 0.08541297912597656 backwards -------------------------------
10 recon_decay
------------ 0.086181640625 backwards -------------------------------
10 recon_decay
------------ 0.08550620079040527 backwards -------------------------------
10 recon_decay
------------ 0.08624839782714844 backwards -------------------------------
10 recon_decay
------------ 0.08531427383422852 backwards -------------------------------
10 recon_decay
------------ 0.08591485023498535 backwards -------------------------------
10 recon_decay
------------ 0.08626127243041992 backwards -------------------------------
10 recon_decay
------------ 0.08564138412475586 backwards -------------------------------
10 recon_decay
------------ 0.08545804023742676 backwards -------------------------------
10 recon_decay
------------ 0.08553051948547363 backwards -------------------------------
10 recon_decay
------------ 0.08581805229187012 backwards -------------------------------
10 recon_decay
------------ 0.08577728271484375 backwards -------------------------------
10 recon_decay
------------ 0.0855858325958252 backwards -------------------------------
10 recon_decay
------------ 0.08650016784667969 backwards -------------------------------
10 recon_decay
------------ 0.08541464805603027 backwards -------------------------------
10 recon_decay
------------ 0.08568763732910156 backwards -------------------------------
10 recon_decay
------------ 0.08571887016296387 backwards -------------------------------
10 recon_decay
------------ 0.08628273010253906 backwards -------------------------------
10 recon_decay
------------ 0.08577752113342285 backwards -------------------------------
10 recon_decay
------------ 0.08553576469421387 backwards -------------------------------
10 recon_decay
------------ 0.08614015579223633 backwards -------------------------------
10 recon_decay
------------ 0.08552980422973633 backwards -------------------------------
10 recon_decay
------------ 0.08545947074890137 backwards -------------------------------
10 recon_decay
------------ 0.08589029312133789 backwards -------------------------------
10 recon_decay
------------ 0.08716082572937012 backwards -------------------------------
10 recon_decay
------------ 0.08601951599121094 backwards -------------------------------
10 recon_decay
------------ 0.08545279502868652 backwards -------------------------------
10 recon_decay
------------ 0.08631515502929688 backwards -------------------------------
10 recon_decay
------------ 0.08530020713806152 backwards -------------------------------
10 recon_decay
------------ 0.0859687328338623 backwards -------------------------------
10 recon_decay
------------ 0.08553194999694824 backwards -------------------------------
10 recon_decay
------------ 0.08709239959716797 backwards -------------------------------
10 recon_decay
------------ 0.08558440208435059 backwards -------------------------------
10 recon_decay
------------ 0.08585715293884277 backwards -------------------------------
10 recon_decay
------------ 0.08577632904052734 backwards -------------------------------
10 recon_decay
------------ 0.08595848083496094 backwards -------------------------------
10 recon_decay
------------ 0.08564496040344238 backwards -------------------------------
10 recon_decay
------------ 0.08534717559814453 backwards -------------------------------
10 recon_decay
------------ 0.08752274513244629 backwards -------------------------------
10 recon_decay
------------ 0.08561277389526367 backwards -------------------------------
10 recon_decay
------------ 0.0858469009399414 backwards -------------------------------
10 recon_decay
------------ 0.08551692962646484 backwards -------------------------------
10 recon_decay
------------ 0.08776092529296875 backwards -------------------------------
10 recon_decay
------------ 0.08597469329833984 backwards -------------------------------
10 recon_decay
------------ 0.08580255508422852 backwards -------------------------------
10 recon_decay
------------ 0.0869140625 backwards -------------------------------
10 recon_decay
------------ 0.08589053153991699 backwards -------------------------------
10 recon_decay
------------ 0.08542728424072266 backwards -------------------------------
10 recon_decay
------------ 0.08578658103942871 backwards -------------------------------
10 recon_decay
------------ 0.08566021919250488 backwards -------------------------------
10 recon_decay
------------ 0.08605742454528809 backwards -------------------------------
10 recon_decay
------------ 0.08580231666564941 backwards -------------------------------
10 recon_decay
------------ 0.08695387840270996 backwards -------------------------------
10 recon_decay
------------ 0.08576202392578125 backwards -------------------------------
10 recon_decay
------------ 0.08553767204284668 backwards -------------------------------
10 recon_decay
------------ 0.08649230003356934 backwards -------------------------------
10 recon_decay
------------ 0.0856931209564209 backwards -------------------------------
10 recon_decay
------------ 0.08570122718811035 backwards -------------------------------
10 recon_decay
------------ 0.0859687328338623 backwards -------------------------------
10 recon_decay
------------ 0.08721184730529785 backwards -------------------------------
10 recon_decay
------------ 0.08570432662963867 backwards -------------------------------
10 recon_decay
------------ 0.08590960502624512 backwards -------------------------------
10 recon_decay
------------ 0.08614349365234375 backwards -------------------------------
10 recon_decay
------------ 0.08588910102844238 backwards -------------------------------
10 recon_decay
------------ 0.08580875396728516 backwards -------------------------------
10 recon_decay
------------ 0.08588027954101562 backwards -------------------------------
10 recon_decay
------------ 0.08714818954467773 backwards -------------------------------
10 recon_decay
------------ 0.08606123924255371 backwards -------------------------------
10 recon_decay
------------ 0.08556938171386719 backwards -------------------------------
10 recon_decay
------------ 0.08583259582519531 backwards -------------------------------
10 recon_decay
------------ 0.08593249320983887 backwards -------------------------------
10 recon_decay
------------ 0.08576226234436035 backwards -------------------------------
10 recon_decay
------------ 0.08632779121398926 backwards -------------------------------
10 recon_decay
------------ 0.08654284477233887 backwards -------------------------------
10 recon_decay
------------ 0.08532905578613281 backwards -------------------------------
10 recon_decay
------------ 0.08668708801269531 backwards -------------------------------
10 recon_decay
------------ 0.08597540855407715 backwards -------------------------------
10 recon_decay
------------ 0.08554267883300781 backwards -------------------------------
10 recon_decay
------------ 0.0857536792755127 backwards -------------------------------
10 recon_decay
------------ 0.0863034725189209 backwards -------------------------------
10 recon_decay
------------ 0.08629250526428223 backwards -------------------------------
10 recon_decay
------------ 0.08539485931396484 backwards -------------------------------
10 recon_decay
------------ 0.0867776870727539 backwards -------------------------------
10 recon_decay
------------ 0.08547282218933105 backwards -------------------------------
10 recon_decay
------------ 0.08568263053894043 backwards -------------------------------
10 recon_decay
------------ 0.08636093139648438 backwards -------------------------------
10 recon_decay
------------ 0.08585286140441895 backwards -------------------------------
10 recon_decay
------------ 0.08626699447631836 backwards -------------------------------
10 recon_decay
------------ 0.08557248115539551 backwards -------------------------------
10 recon_decay
------------ 0.0858452320098877 backwards -------------------------------
10 recon_decay
------------ 0.08645272254943848 backwards -------------------------------
10 recon_decay
------------ 0.08626413345336914 backwards -------------------------------
10 recon_decay
------------ 0.08576846122741699 backwards -------------------------------
10 recon_decay
------------ 0.08581304550170898 backwards -------------------------------
10 recon_decay
------------ 0.08623600006103516 backwards -------------------------------
10 recon_decay
------------ 0.08563804626464844 backwards -------------------------------
10 recon_decay
------------ 0.08592605590820312 backwards -------------------------------
10 recon_decay
------------ 0.08599638938903809 backwards -------------------------------
10 recon_decay
------------ 0.08695197105407715 backwards -------------------------------
10 recon_decay
------------ 0.08588075637817383 backwards -------------------------------
10 recon_decay
------------ 0.08585715293884277 backwards -------------------------------
10 recon_decay
------------ 0.08611559867858887 backwards -------------------------------
10 recon_decay
------------ 0.08579277992248535 backwards -------------------------------
10 recon_decay
------------ 0.08572506904602051 backwards -------------------------------
10 recon_decay
------------ 0.08685827255249023 backwards -------------------------------
10 recon_decay
------------ 0.08596587181091309 backwards -------------------------------
10 recon_decay
------------ 0.08632636070251465 backwards -------------------------------
10 recon_decay
------------ 0.08611488342285156 backwards -------------------------------
10 recon_decay
------------ 0.08634614944458008 backwards -------------------------------
10 recon_decay
------------ 0.08600163459777832 backwards -------------------------------
10 recon_decay
------------ 0.08609247207641602 backwards -------------------------------
10 recon_decay
------------ 0.08610796928405762 backwards -------------------------------
10 recon_decay
------------ 0.08592557907104492 backwards -------------------------------
10 recon_decay
------------ 0.08698034286499023 backwards -------------------------------
10 recon_decay
------------ 0.08544802665710449 backwards -------------------------------
10 recon_decay
------------ 0.08569145202636719 backwards -------------------------------
10 recon_decay
------------ 0.08602309226989746 backwards -------------------------------
10 recon_decay
------------ 0.08568811416625977 backwards -------------------------------
10 recon_decay
------------ 0.08635592460632324 backwards -------------------------------
10 recon_decay
------------ 0.08586001396179199 backwards -------------------------------
10 recon_decay
------------ 0.08605504035949707 backwards -------------------------------
10 recon_decay
------------ 0.08544778823852539 backwards -------------------------------
10 recon_decay
------------ 0.0858614444732666 backwards -------------------------------
10 recon_decay
------------ 0.08630084991455078 backwards -------------------------------
10 recon_decay
------------ 0.08549690246582031 backwards -------------------------------
10 recon_decay
------------ 0.08641481399536133 backwards -------------------------------
10 recon_decay
------------ 0.08548474311828613 backwards -------------------------------
10 recon_decay
------------ 0.08573508262634277 backwards -------------------------------
10 recon_decay
------------ 0.08595895767211914 backwards -------------------------------
10 recon_decay
------------ 0.08524012565612793 backwards -------------------------------
10 recon_decay
------------ 0.0868220329284668 backwards -------------------------------
10 recon_decay
------------ 0.08548784255981445 backwards -------------------------------
10 recon_decay
------------ 0.08597970008850098 backwards -------------------------------
10 recon_decay
------------ 0.08562088012695312 backwards -------------------------------
10 recon_decay
------------ 0.08544445037841797 backwards -------------------------------
10 recon_decay
------------ 0.08799600601196289 backwards -------------------------------
10 recon_decay
------------ 0.08560514450073242 backwards -------------------------------
10 recon_decay
------------ 0.08584403991699219 backwards -------------------------------
10 recon_decay
------------ 0.08509683609008789 backwards -------------------------------
10 recon_decay
------------ 0.08554792404174805 backwards -------------------------------
10 recon_decay
------------ 0.08520340919494629 backwards -------------------------------
10 recon_decay
------------ 0.0860595703125 backwards -------------------------------
10 recon_decay
------------ 0.0863642692565918 backwards -------------------------------
10 recon_decay
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 377/378 [00:53<00:00,  7.06it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:55<00:00,  1.23s/it]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 43/45 [00:55<00:00, 12.81it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)


 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 124/126 [01:01<00:00, 24.56it/s]
saving model......................................................
done
------------ 61.748021364212036 seg time alll epoch -------------------------------
saving model......................................................
here1
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.17539284 0.17852199 0.17862189 ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.1748133  0.17662917 0.1766292  ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.1784955  0.17852199 0.18041165 ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.17852199 0.17862189 0.1796051  ... 0.9971584  0.9972343  0.99730194] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.17852199 0.17862189 0.17908311 ... 0.9971584  0.9972343  0.99730194] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.17539284 0.17852199 0.17862189 ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.1748133  0.17662917 0.1766292  ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.1784955  0.17852199 0.18041165 ... 0.99711525 0.9972343  0.99730194] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.17852199 0.17862189 0.1796051  ... 0.9971584  0.9972343  0.99730194] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.17852199 0.17862189 0.17908311 ... 0.9971584  0.9972343  0.99730194] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNet UNet_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/167/exp2d_DF_3_2d True unet2d_167 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   9%|██████████████████████▊                                                                                                                                                                                                                                       | 34/378 [00:01<00:13, 26.20it/s, loss=0]
False False load_model
arrive train
arrive location
here opt
here seg decay
epoch #0:  23%|███████████████████████████████████████████████████████████▏                                                                                                                                                                                                  | 88/378 [00:03<00:10, 26.43it/s, loss=0]
epoch #0:  38%|███████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                              | 142/378 [00:05<00:08, 26.23it/s, loss=0]
epoch #0:  48%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                   | 181/378 [00:07<00:07, 26.44it/s, loss=0]
epoch #0:  62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                               | 235/378 [00:09<00:05, 26.28it/s, loss=0]
epoch #0:  76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                           | 289/378 [00:11<00:03, 25.62it/s, loss=0]
epoch #0:  90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 340/378 [00:13<00:01, 25.62it/s, loss=0]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:14<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:14<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:15<00:00,  2.91it/s]
 69%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 31/45 [00:15<00:04,  3.43it/s]
saving model......................................................
done
------------ 16.842684745788574 seg time alll epoch -------------------------------
saving model......................................................
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 31/45 [00:15<00:04,  3.43it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 31/45 [00:15<00:04,  3.43it/s]
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:16<00:00,  7.55it/s]
 96%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 121/126 [00:16<00:00, 38.80it/s]
saving model......................................................
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19333707 0.1946078  0.19681644 ... 0.9989826  0.9989844  0.9989881 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.20592143 0.20620246 0.2068237  ... 0.99898356 0.9989839  0.99898463] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.20645942 0.20657553 0.20754096 ... 0.99900824 0.99901223 0.999025  ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.20529217 0.20683874 0.20719256 ... 0.9990319  0.999035   0.99905235] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.21370646 0.21546014 0.21603236 ... 0.9989827  0.99898463 0.9989887 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19333707 0.1946078  0.19681644 ... 0.9989826  0.9989844  0.9989881 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.20592143 0.20620246 0.2068237  ... 0.99898356 0.9989839  0.99898463] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.20645942 0.20657553 0.20754096 ... 0.99900824 0.99901223 0.999025  ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.20529217 0.20683874 0.20719256 ... 0.9990319  0.999035   0.99905235] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.21370646 0.21546014 0.21603236 ... 0.9989827  0.99898463 0.9989887 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNetC UNetC_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/167/exp2d_DF_3_2d True unet2dC_167 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
here opt
here seg decay
    self.training_model(model_dir, tensorboard_dir)██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 121/126 [00:16<00:00, 38.80it/s]
  File "lord_exp.py", line 133, in main
    args.func(args)
  File "lord_exp.py", line 114, in run_exp
    train_model(model_dict, path_new_exp, model_name, exp_dict['data_l_name'], exp_dict['data_u_name'], exp_dict['data_v_name'], exp_dict['data_t_name'], exp_dict['base_dir'])
  File "lord_exp.py", line 31, in train_model
    take_from_arg = False)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/lord_unet.py", line 367, in train_ulord
    loaded_model=load_model
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 432, in train
    classes_t, model_dir, tensorboard_dir, loaded_model, dim)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 484, in train_UNetSCModel
    self.training_model(model_dir, tensorboard_dir)██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏          | 121/126 [00:16<00:00, 38.80it/s]
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 671, in training_model
    shape=(1, self.imgs.shape[1], self.imgs.shape[2]), randomized=False)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 829, in generate_samples_ulord2d
    path_results_output, randomized, recon_loss=self.g_rec)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 1017, in generate_rec_seg_samples2d
    samples['class_id'][[i]])
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 290, in do_predict_ulord
    return self.ulord_model(images, classes, images, classes)
  File "/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
TypeError: forward() takes 3 positional arguments but 5 were given