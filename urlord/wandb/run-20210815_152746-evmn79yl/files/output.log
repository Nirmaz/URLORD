True cuda
args: None URLord URLord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/166/exp2d_DF_3_2d True urlord2d_166 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
/cs/casmip/nirm/embryo_project_version1/venu-pytorch/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)
  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
here seg decay
here recon_loss
10 recon_decay
epoch #0:   4%|█████████▉                                                                                                                                                                                                                                                 | 15/378 [00:02<00:48,  7.43it/s, loss=2.44]
  4%|██████████▊                                                                                                                                                                                                                                                                     | 15/378 [00:02<00:48,  7.43it/s]
------------ 0.10079407691955566 backwards -------------------------------
10 recon_decay
------------ 0.08333873748779297 backwards -------------------------------
10 recon_decay
------------ 0.0836176872253418 backwards -------------------------------
10 recon_decay
------------ 0.08419418334960938 backwards -------------------------------
10 recon_decay
------------ 0.08375716209411621 backwards -------------------------------
10 recon_decay
------------ 0.08334231376647949 backwards -------------------------------
10 recon_decay
------------ 0.08442378044128418 backwards -------------------------------
10 recon_decay
------------ 0.08399558067321777 backwards -------------------------------
10 recon_decay
------------ 0.08411407470703125 backwards -------------------------------
10 recon_decay
------------ 0.08402776718139648 backwards -------------------------------
10 recon_decay
------------ 0.08445930480957031 backwards -------------------------------
10 recon_decay
------------ 0.08327317237854004 backwards -------------------------------
10 recon_decay
------------ 0.08393192291259766 backwards -------------------------------
10 recon_decay
------------ 0.08434176445007324 backwards -------------------------------
10 recon_decay
------------ 0.08371472358703613 backwards -------------------------------

  8%|█████████████████████▌                                                                                                                                                                                                                                                          | 30/378 [00:04<00:46,  7.51it/s]
------------ 0.08399748802185059 backwards -------------------------------
10 recon_decay
------------ 0.08369827270507812 backwards -------------------------------
10 recon_decay
------------ 0.08357834815979004 backwards -------------------------------
10 recon_decay
------------ 0.08407711982727051 backwards -------------------------------
10 recon_decay
------------ 0.08469915390014648 backwards -------------------------------
10 recon_decay
------------ 0.08418512344360352 backwards -------------------------------
10 recon_decay
------------ 0.08382701873779297 backwards -------------------------------
10 recon_decay
------------ 0.08353185653686523 backwards -------------------------------
10 recon_decay
------------ 0.08431386947631836 backwards -------------------------------
10 recon_decay
------------ 0.08404421806335449 backwards -------------------------------
10 recon_decay
------------ 0.08405017852783203 backwards -------------------------------
10 recon_decay
------------ 0.0844566822052002 backwards -------------------------------
10 recon_decay
------------ 0.08385086059570312 backwards -------------------------------
10 recon_decay
------------ 0.08371233940124512 backwards -------------------------------
10 recon_decay
------------ 0.0832972526550293 backwards -------------------------------

 12%|████████████████████████████████▍                                                                                                                                                                                                                                               | 45/378 [00:06<00:44,  7.47it/s]
------------ 0.08504676818847656 backwards -------------------------------
10 recon_decay
------------ 0.08378338813781738 backwards -------------------------------
10 recon_decay
------------ 0.08391380310058594 backwards -------------------------------
10 recon_decay
------------ 0.08375334739685059 backwards -------------------------------
10 recon_decay
------------ 0.08478379249572754 backwards -------------------------------
10 recon_decay
------------ 0.08419156074523926 backwards -------------------------------
10 recon_decay
------------ 0.0838620662689209 backwards -------------------------------
10 recon_decay
------------ 0.08385562896728516 backwards -------------------------------
10 recon_decay
------------ 0.08379483222961426 backwards -------------------------------
10 recon_decay
------------ 0.08349132537841797 backwards -------------------------------
10 recon_decay
------------ 0.08371877670288086 backwards -------------------------------
10 recon_decay
------------ 0.0843808650970459 backwards -------------------------------
10 recon_decay
------------ 0.08400321006774902 backwards -------------------------------
10 recon_decay
------------ 0.08345293998718262 backwards -------------------------------

 16%|███████████████████████████████████████████▏                                                                                                                                                                                                                                    | 60/378 [00:08<00:42,  7.42it/s]
------------ 0.08413410186767578 backwards -------------------------------
10 recon_decay
------------ 0.08480048179626465 backwards -------------------------------
10 recon_decay
------------ 0.08376908302307129 backwards -------------------------------
10 recon_decay
------------ 0.08377313613891602 backwards -------------------------------
10 recon_decay
------------ 0.08386111259460449 backwards -------------------------------
10 recon_decay
------------ 0.08426666259765625 backwards -------------------------------
10 recon_decay
------------ 0.08471322059631348 backwards -------------------------------
10 recon_decay
------------ 0.08383870124816895 backwards -------------------------------
10 recon_decay
------------ 0.08405160903930664 backwards -------------------------------
10 recon_decay
------------ 0.0837564468383789 backwards -------------------------------
10 recon_decay
------------ 0.08379292488098145 backwards -------------------------------
10 recon_decay
------------ 0.08431458473205566 backwards -------------------------------
10 recon_decay
------------ 0.08497357368469238 backwards -------------------------------
10 recon_decay
------------ 0.08524584770202637 backwards -------------------------------
10 recon_decay
------------ 0.0835878849029541 backwards -------------------------------

 20%|█████████████████████████████████████████████████████▉                                                                                                                                                                                                                          | 75/378 [00:10<00:40,  7.42it/s]
------------ 0.08426976203918457 backwards -------------------------------
10 recon_decay
------------ 0.08413195610046387 backwards -------------------------------
10 recon_decay
------------ 0.08393049240112305 backwards -------------------------------
10 recon_decay
------------ 0.08410334587097168 backwards -------------------------------
10 recon_decay
------------ 0.08455705642700195 backwards -------------------------------
10 recon_decay
------------ 0.08453726768493652 backwards -------------------------------
10 recon_decay
------------ 0.08366847038269043 backwards -------------------------------
10 recon_decay
------------ 0.0847017765045166 backwards -------------------------------
10 recon_decay
------------ 0.08562374114990234 backwards -------------------------------
10 recon_decay
------------ 0.08441948890686035 backwards -------------------------------
10 recon_decay
------------ 0.08358883857727051 backwards -------------------------------
10 recon_decay
------------ 0.08387422561645508 backwards -------------------------------
10 recon_decay
------------ 0.08471226692199707 backwards -------------------------------
10 recon_decay
------------ 0.08401155471801758 backwards -------------------------------
10 recon_decay
------------ 0.08403301239013672 backwards -------------------------------

 24%|████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                               | 90/378 [00:12<00:38,  7.39it/s]
------------ 0.08410167694091797 backwards -------------------------------
10 recon_decay
------------ 0.08457303047180176 backwards -------------------------------
10 recon_decay
------------ 0.08433985710144043 backwards -------------------------------
10 recon_decay
------------ 0.08377981185913086 backwards -------------------------------
10 recon_decay
------------ 0.08952450752258301 backwards -------------------------------
10 recon_decay
------------ 0.0843808650970459 backwards -------------------------------
10 recon_decay
------------ 0.08390426635742188 backwards -------------------------------
10 recon_decay
------------ 0.0838935375213623 backwards -------------------------------
10 recon_decay
------------ 0.08524465560913086 backwards -------------------------------
10 recon_decay
------------ 0.08417081832885742 backwards -------------------------------
10 recon_decay
------------ 0.08392882347106934 backwards -------------------------------
10 recon_decay
------------ 0.0836644172668457 backwards -------------------------------
10 recon_decay
------------ 0.0847787857055664 backwards -------------------------------
10 recon_decay
------------ 0.08459329605102539 backwards -------------------------------
10 recon_decay
------------ 0.08452820777893066 backwards -------------------------------

 28%|███████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                   | 105/378 [00:14<00:37,  7.34it/s]
------------ 0.08463001251220703 backwards -------------------------------
10 recon_decay
------------ 0.08420586585998535 backwards -------------------------------
10 recon_decay
------------ 0.08337998390197754 backwards -------------------------------
10 recon_decay
------------ 0.0849761962890625 backwards -------------------------------
10 recon_decay
------------ 0.08474397659301758 backwards -------------------------------
10 recon_decay
------------ 0.0846867561340332 backwards -------------------------------
10 recon_decay
------------ 0.08357048034667969 backwards -------------------------------
10 recon_decay
------------ 0.08402276039123535 backwards -------------------------------
10 recon_decay
------------ 0.08453702926635742 backwards -------------------------------
10 recon_decay
------------ 0.0845482349395752 backwards -------------------------------
10 recon_decay
------------ 0.0837557315826416 backwards -------------------------------
10 recon_decay
------------ 0.08455228805541992 backwards -------------------------------
10 recon_decay
------------ 0.08375310897827148 backwards -------------------------------
10 recon_decay
------------ 0.08410525321960449 backwards -------------------------------

 31%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                         | 119/378 [00:16<00:35,  7.31it/s]
------------ 0.08416748046875 backwards -------------------------------
10 recon_decay
------------ 0.08451962471008301 backwards -------------------------------
10 recon_decay
------------ 0.08467817306518555 backwards -------------------------------
10 recon_decay
------------ 0.08424949645996094 backwards -------------------------------
10 recon_decay
------------ 0.08408188819885254 backwards -------------------------------
10 recon_decay
------------ 0.08417344093322754 backwards -------------------------------
10 recon_decay
------------ 0.08416938781738281 backwards -------------------------------
10 recon_decay
------------ 0.08416104316711426 backwards -------------------------------
10 recon_decay
------------ 0.08381819725036621 backwards -------------------------------
10 recon_decay
------------ 0.08512187004089355 backwards -------------------------------
10 recon_decay
------------ 0.08397388458251953 backwards -------------------------------
10 recon_decay
------------ 0.08393192291259766 backwards -------------------------------
10 recon_decay
------------ 0.08438873291015625 backwards -------------------------------
10 recon_decay
------------ 0.08376860618591309 backwards -------------------------------
10 recon_decay
------------ 0.08381104469299316 backwards -------------------------------

 35%|████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                               | 134/378 [00:18<00:33,  7.26it/s]
------------ 0.08451581001281738 backwards -------------------------------
10 recon_decay
------------ 0.08650350570678711 backwards -------------------------------
10 recon_decay
------------ 0.08393669128417969 backwards -------------------------------
10 recon_decay
------------ 0.08392882347106934 backwards -------------------------------
10 recon_decay
------------ 0.08469533920288086 backwards -------------------------------
10 recon_decay
------------ 0.08427667617797852 backwards -------------------------------
10 recon_decay
------------ 0.08375668525695801 backwards -------------------------------
10 recon_decay
------------ 0.08419108390808105 backwards -------------------------------
10 recon_decay
------------ 0.08472633361816406 backwards -------------------------------
10 recon_decay
------------ 0.08422183990478516 backwards -------------------------------
10 recon_decay
------------ 0.08430814743041992 backwards -------------------------------
10 recon_decay
------------ 0.08520889282226562 backwards -------------------------------
10 recon_decay
------------ 0.0847480297088623 backwards -------------------------------
10 recon_decay
------------ 0.08413505554199219 backwards -------------------------------
10 recon_decay
------------ 0.08429288864135742 backwards -------------------------------

 39%|██████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                    | 149/378 [00:20<00:31,  7.25it/s]
------------ 0.08459591865539551 backwards -------------------------------
10 recon_decay
------------ 0.08435964584350586 backwards -------------------------------
10 recon_decay
------------ 0.0840761661529541 backwards -------------------------------
10 recon_decay
------------ 0.08492541313171387 backwards -------------------------------
10 recon_decay
------------ 0.08541369438171387 backwards -------------------------------
10 recon_decay
------------ 0.08436250686645508 backwards -------------------------------
10 recon_decay
------------ 0.08434820175170898 backwards -------------------------------
10 recon_decay
------------ 0.08452391624450684 backwards -------------------------------
10 recon_decay
------------ 0.08452844619750977 backwards -------------------------------
10 recon_decay
------------ 0.08488655090332031 backwards -------------------------------
10 recon_decay
------------ 0.08400130271911621 backwards -------------------------------
10 recon_decay
------------ 0.08490777015686035 backwards -------------------------------
10 recon_decay
------------ 0.08455419540405273 backwards -------------------------------
10 recon_decay
------------ 0.08449530601501465 backwards -------------------------------

 43%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                          | 163/378 [00:22<00:29,  7.21it/s]
------------ 0.08417606353759766 backwards -------------------------------
10 recon_decay
------------ 0.0844869613647461 backwards -------------------------------
10 recon_decay
------------ 0.0843348503112793 backwards -------------------------------
10 recon_decay
------------ 0.08435225486755371 backwards -------------------------------
10 recon_decay
------------ 0.08515167236328125 backwards -------------------------------
10 recon_decay
------------ 0.08692502975463867 backwards -------------------------------
10 recon_decay
------------ 0.08394622802734375 backwards -------------------------------
10 recon_decay
------------ 0.0845804214477539 backwards -------------------------------
10 recon_decay
------------ 0.08556246757507324 backwards -------------------------------
10 recon_decay
------------ 0.08407402038574219 backwards -------------------------------
10 recon_decay
------------ 0.08540701866149902 backwards -------------------------------
10 recon_decay
------------ 0.08411478996276855 backwards -------------------------------
10 recon_decay
------------ 0.08464217185974121 backwards -------------------------------
10 recon_decay
------------ 0.08354592323303223 backwards -------------------------------
10 recon_decay
------------ 0.08391499519348145 backwards -------------------------------

 47%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                               | 178/378 [00:24<00:27,  7.22it/s]
------------ 0.08476519584655762 backwards -------------------------------
10 recon_decay
------------ 0.08453726768493652 backwards -------------------------------
10 recon_decay
------------ 0.08392906188964844 backwards -------------------------------
10 recon_decay
------------ 0.0848550796508789 backwards -------------------------------
10 recon_decay
------------ 0.08493757247924805 backwards -------------------------------
10 recon_decay
------------ 0.08410930633544922 backwards -------------------------------
10 recon_decay
------------ 0.08418393135070801 backwards -------------------------------
10 recon_decay
------------ 0.08504819869995117 backwards -------------------------------
10 recon_decay
------------ 0.08443832397460938 backwards -------------------------------
10 recon_decay
------------ 0.08482885360717773 backwards -------------------------------
10 recon_decay
------------ 0.08563852310180664 backwards -------------------------------
10 recon_decay
------------ 0.08458065986633301 backwards -------------------------------
10 recon_decay
------------ 0.0842132568359375 backwards -------------------------------
10 recon_decay
------------ 0.0845341682434082 backwards -------------------------------

 51%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                     | 192/378 [00:26<00:26,  7.11it/s]
------------ 0.08478879928588867 backwards -------------------------------
10 recon_decay
------------ 0.08443927764892578 backwards -------------------------------
10 recon_decay
------------ 0.08445978164672852 backwards -------------------------------
10 recon_decay
------------ 0.0845181941986084 backwards -------------------------------
10 recon_decay
------------ 0.0858149528503418 backwards -------------------------------
10 recon_decay
------------ 0.08435964584350586 backwards -------------------------------
10 recon_decay
------------ 0.08470392227172852 backwards -------------------------------
10 recon_decay
------------ 0.08400297164916992 backwards -------------------------------
10 recon_decay
------------ 0.0848691463470459 backwards -------------------------------
10 recon_decay
------------ 0.08412981033325195 backwards -------------------------------
10 recon_decay
------------ 0.08439326286315918 backwards -------------------------------
10 recon_decay
------------ 0.08460783958435059 backwards -------------------------------
10 recon_decay
------------ 0.08526897430419922 backwards -------------------------------
10 recon_decay
------------ 0.0839993953704834 backwards -------------------------------

 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                          | 207/378 [00:28<00:23,  7.15it/s]
------------ 0.08414626121520996 backwards -------------------------------
10 recon_decay
------------ 0.08482813835144043 backwards -------------------------------
10 recon_decay
------------ 0.08464431762695312 backwards -------------------------------
10 recon_decay
------------ 0.08447837829589844 backwards -------------------------------
10 recon_decay
------------ 0.0843667984008789 backwards -------------------------------
10 recon_decay
------------ 0.0853877067565918 backwards -------------------------------
10 recon_decay
------------ 0.08427000045776367 backwards -------------------------------
10 recon_decay
------------ 0.08478546142578125 backwards -------------------------------
10 recon_decay
------------ 0.0848846435546875 backwards -------------------------------
10 recon_decay
------------ 0.08469581604003906 backwards -------------------------------
10 recon_decay
------------ 0.08417296409606934 backwards -------------------------------
10 recon_decay
------------ 0.08440423011779785 backwards -------------------------------
10 recon_decay
------------ 0.08527469635009766 backwards -------------------------------
10 recon_decay
------------ 0.0842740535736084 backwards -------------------------------
10 recon_decay
------------ 0.08446359634399414 backwards -------------------------------

 58%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                | 221/378 [00:30<00:22,  7.10it/s]
------------ 0.08487725257873535 backwards -------------------------------
10 recon_decay
------------ 0.08439064025878906 backwards -------------------------------
10 recon_decay
------------ 0.08430171012878418 backwards -------------------------------
10 recon_decay
------------ 0.08450627326965332 backwards -------------------------------
10 recon_decay
------------ 0.08521032333374023 backwards -------------------------------
10 recon_decay
------------ 0.08504486083984375 backwards -------------------------------
10 recon_decay
------------ 0.08413219451904297 backwards -------------------------------
10 recon_decay
------------ 0.08487296104431152 backwards -------------------------------
10 recon_decay
------------ 0.0844564437866211 backwards -------------------------------
10 recon_decay
------------ 0.0844717025756836 backwards -------------------------------
10 recon_decay
------------ 0.08536410331726074 backwards -------------------------------
10 recon_decay
------------ 0.0849161148071289 backwards -------------------------------
10 recon_decay
------------ 0.0851907730102539 backwards -------------------------------
10 recon_decay
------------ 0.08482956886291504 backwards -------------------------------

 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                     | 236/378 [00:32<00:19,  7.12it/s]
------------ 0.08476901054382324 backwards -------------------------------
10 recon_decay
------------ 0.08432340621948242 backwards -------------------------------
10 recon_decay
------------ 0.08447670936584473 backwards -------------------------------
10 recon_decay
------------ 0.08438634872436523 backwards -------------------------------
10 recon_decay
------------ 0.08528470993041992 backwards -------------------------------
10 recon_decay
------------ 0.0849158763885498 backwards -------------------------------
10 recon_decay
------------ 0.08480453491210938 backwards -------------------------------
10 recon_decay
------------ 0.08411407470703125 backwards -------------------------------
10 recon_decay
------------ 0.0848228931427002 backwards -------------------------------
10 recon_decay
------------ 0.08432865142822266 backwards -------------------------------
10 recon_decay
------------ 0.08476686477661133 backwards -------------------------------
10 recon_decay
------------ 0.08492493629455566 backwards -------------------------------
10 recon_decay
------------ 0.08531904220581055 backwards -------------------------------
10 recon_decay
------------ 0.08417940139770508 backwards -------------------------------

 66%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                           | 250/378 [00:34<00:18,  7.08it/s]
------------ 0.0845944881439209 backwards -------------------------------
10 recon_decay
------------ 0.08417463302612305 backwards -------------------------------
10 recon_decay
------------ 0.08488345146179199 backwards -------------------------------
10 recon_decay
------------ 0.0843658447265625 backwards -------------------------------
10 recon_decay
------------ 0.0852518081665039 backwards -------------------------------
10 recon_decay
------------ 0.08500957489013672 backwards -------------------------------
10 recon_decay
------------ 0.08482861518859863 backwards -------------------------------
10 recon_decay
------------ 0.0848085880279541 backwards -------------------------------
10 recon_decay
------------ 0.08532834053039551 backwards -------------------------------
10 recon_decay
------------ 0.0855095386505127 backwards -------------------------------
10 recon_decay
------------ 0.0846867561340332 backwards -------------------------------
10 recon_decay
------------ 0.08562302589416504 backwards -------------------------------
10 recon_decay
------------ 0.08481311798095703 backwards -------------------------------
10 recon_decay
------------ 0.08515477180480957 backwards -------------------------------

 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                 | 265/378 [00:36<00:15,  7.16it/s]
------------ 0.0857076644897461 backwards -------------------------------
10 recon_decay
------------ 0.08526754379272461 backwards -------------------------------
10 recon_decay
------------ 0.08527851104736328 backwards -------------------------------
10 recon_decay
------------ 0.08476090431213379 backwards -------------------------------
10 recon_decay
------------ 0.08506608009338379 backwards -------------------------------
10 recon_decay
------------ 0.08472919464111328 backwards -------------------------------
10 recon_decay
------------ 0.08472418785095215 backwards -------------------------------
10 recon_decay
------------ 0.08445906639099121 backwards -------------------------------
10 recon_decay
------------ 0.08571124076843262 backwards -------------------------------
10 recon_decay
------------ 0.08569169044494629 backwards -------------------------------
10 recon_decay
------------ 0.08469009399414062 backwards -------------------------------
10 recon_decay
------------ 0.08556795120239258 backwards -------------------------------
10 recon_decay
------------ 0.08469986915588379 backwards -------------------------------
10 recon_decay
------------ 0.08496379852294922 backwards -------------------------------
10 recon_decay
------------ 0.08483648300170898 backwards -------------------------------

 74%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                       | 279/378 [00:38<00:14,  7.03it/s]
------------ 0.08558344841003418 backwards -------------------------------
10 recon_decay
------------ 0.08540034294128418 backwards -------------------------------
10 recon_decay
------------ 0.08483386039733887 backwards -------------------------------
10 recon_decay
------------ 0.08492541313171387 backwards -------------------------------
10 recon_decay
------------ 0.08515238761901855 backwards -------------------------------
10 recon_decay
------------ 0.08514785766601562 backwards -------------------------------
10 recon_decay
------------ 0.08482170104980469 backwards -------------------------------
10 recon_decay
------------ 0.08528494834899902 backwards -------------------------------
10 recon_decay
------------ 0.08523273468017578 backwards -------------------------------
10 recon_decay
------------ 0.08482241630554199 backwards -------------------------------
10 recon_decay
------------ 0.0847785472869873 backwards -------------------------------
10 recon_decay
------------ 0.08507466316223145 backwards -------------------------------
10 recon_decay
------------ 0.08537149429321289 backwards -------------------------------
10 recon_decay
------------ 0.08485722541809082 backwards -------------------------------

 78%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                             | 293/378 [00:40<00:12,  7.07it/s]
------------ 0.08533263206481934 backwards -------------------------------
10 recon_decay
------------ 0.08538007736206055 backwards -------------------------------
10 recon_decay
------------ 0.08478903770446777 backwards -------------------------------
10 recon_decay
------------ 0.08465266227722168 backwards -------------------------------
10 recon_decay
------------ 0.08529186248779297 backwards -------------------------------
10 recon_decay
------------ 0.08496284484863281 backwards -------------------------------
10 recon_decay
------------ 0.08544397354125977 backwards -------------------------------
10 recon_decay
------------ 0.0856168270111084 backwards -------------------------------
10 recon_decay
------------ 0.08480978012084961 backwards -------------------------------
10 recon_decay
------------ 0.0859684944152832 backwards -------------------------------
10 recon_decay
------------ 0.08472156524658203 backwards -------------------------------
10 recon_decay
------------ 0.0855565071105957 backwards -------------------------------
10 recon_decay
------------ 0.08465170860290527 backwards -------------------------------
10 recon_decay
------------ 0.08727216720581055 backwards -------------------------------

 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 307/378 [00:42<00:10,  7.02it/s]
------------ 0.0849456787109375 backwards -------------------------------
10 recon_decay
------------ 0.08511018753051758 backwards -------------------------------
10 recon_decay
------------ 0.08520269393920898 backwards -------------------------------
10 recon_decay
------------ 0.08586716651916504 backwards -------------------------------
10 recon_decay
------------ 0.0855557918548584 backwards -------------------------------
10 recon_decay
------------ 0.0846402645111084 backwards -------------------------------
10 recon_decay
------------ 0.09095931053161621 backwards -------------------------------
10 recon_decay
------------ 0.08574581146240234 backwards -------------------------------
10 recon_decay
------------ 0.08449387550354004 backwards -------------------------------
10 recon_decay
------------ 0.08552861213684082 backwards -------------------------------
10 recon_decay
------------ 0.08527827262878418 backwards -------------------------------
10 recon_decay
------------ 0.08522653579711914 backwards -------------------------------
10 recon_decay
------------ 0.08504557609558105 backwards -------------------------------
10 recon_decay
------------ 0.08522319793701172 backwards -------------------------------

 84%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 318/378 [00:44<00:08,  7.00it/s]
------------ 0.08649945259094238 backwards -------------------------------
10 recon_decay
------------ 0.08472847938537598 backwards -------------------------------
10 recon_decay
------------ 0.08516669273376465 backwards -------------------------------
10 recon_decay
------------ 0.08510828018188477 backwards -------------------------------
10 recon_decay
------------ 0.08503389358520508 backwards -------------------------------
10 recon_decay
------------ 0.0848534107208252 backwards -------------------------------
10 recon_decay
------------ 0.08468842506408691 backwards -------------------------------
10 recon_decay
------------ 0.08600378036499023 backwards -------------------------------
10 recon_decay
------------ 0.08477163314819336 backwards -------------------------------
10 recon_decay
------------ 0.08481502532958984 backwards -------------------------------
10 recon_decay
------------ 0.08495092391967773 backwards -------------------------------
10 recon_decay
------------ 0.08520269393920898 backwards -------------------------------
10 recon_decay
------------ 0.08520627021789551 backwards -------------------------------
10 recon_decay
------------ 0.08508133888244629 backwards -------------------------------

 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                 | 332/378 [00:46<00:06,  6.98it/s]
------------ 0.08573293685913086 backwards -------------------------------
10 recon_decay
------------ 0.08523702621459961 backwards -------------------------------
10 recon_decay
------------ 0.08484148979187012 backwards -------------------------------
10 recon_decay
------------ 0.08550739288330078 backwards -------------------------------
10 recon_decay
------------ 0.08530569076538086 backwards -------------------------------
10 recon_decay
------------ 0.08457016944885254 backwards -------------------------------
10 recon_decay
------------ 0.08534860610961914 backwards -------------------------------
10 recon_decay
------------ 0.08651328086853027 backwards -------------------------------
10 recon_decay
------------ 0.08546710014343262 backwards -------------------------------
10 recon_decay
------------ 0.08484673500061035 backwards -------------------------------
10 recon_decay
------------ 0.0860452651977539 backwards -------------------------------
10 recon_decay
------------ 0.08490896224975586 backwards -------------------------------
10 recon_decay
------------ 0.08469772338867188 backwards -------------------------------
10 recon_decay
------------ 0.08521199226379395 backwards -------------------------------

 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                       | 346/378 [00:48<00:04,  6.88it/s]
------------ 0.08602595329284668 backwards -------------------------------
10 recon_decay
------------ 0.08498525619506836 backwards -------------------------------
10 recon_decay
------------ 0.08508658409118652 backwards -------------------------------
10 recon_decay
------------ 0.08640599250793457 backwards -------------------------------
10 recon_decay
------------ 0.08452129364013672 backwards -------------------------------
10 recon_decay
------------ 0.08493304252624512 backwards -------------------------------
10 recon_decay
------------ 0.08454394340515137 backwards -------------------------------
10 recon_decay
------------ 0.0847785472869873 backwards -------------------------------
10 recon_decay
------------ 0.0847012996673584 backwards -------------------------------
10 recon_decay
------------ 0.08606266975402832 backwards -------------------------------
10 recon_decay
------------ 0.0856025218963623 backwards -------------------------------
10 recon_decay
------------ 0.08603119850158691 backwards -------------------------------
10 recon_decay
------------ 0.08569884300231934 backwards -------------------------------

 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 360/378 [00:50<00:02,  6.85it/s]
------------ 0.08563613891601562 backwards -------------------------------
10 recon_decay
------------ 0.08473634719848633 backwards -------------------------------
10 recon_decay
------------ 0.08516454696655273 backwards -------------------------------
10 recon_decay
------------ 0.08530902862548828 backwards -------------------------------
10 recon_decay
------------ 0.08528685569763184 backwards -------------------------------
10 recon_decay
------------ 0.08500933647155762 backwards -------------------------------
10 recon_decay
------------ 0.0860283374786377 backwards -------------------------------
10 recon_decay
------------ 0.0846099853515625 backwards -------------------------------
10 recon_decay
------------ 0.08473896980285645 backwards -------------------------------
10 recon_decay
------------ 0.08498811721801758 backwards -------------------------------
10 recon_decay
------------ 0.08478260040283203 backwards -------------------------------
10 recon_decay
------------ 0.0852973461151123 backwards -------------------------------
10 recon_decay
------------ 0.08483171463012695 backwards -------------------------------
10 recon_decay
------------ 0.0855565071105957 backwards -------------------------------

 99%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 374/378 [00:52<00:00,  7.07it/s]
------------ 0.08497834205627441 backwards -------------------------------
10 recon_decay
------------ 0.0850687026977539 backwards -------------------------------
10 recon_decay
------------ 0.08459877967834473 backwards -------------------------------
10 recon_decay
------------ 0.08490777015686035 backwards -------------------------------
10 recon_decay
------------ 0.08558440208435059 backwards -------------------------------
10 recon_decay
------------ 0.08469605445861816 backwards -------------------------------
10 recon_decay
------------ 0.08599233627319336 backwards -------------------------------
10 recon_decay
------------ 0.09054780006408691 backwards -------------------------------
10 recon_decay
------------ 0.0848844051361084 backwards -------------------------------
10 recon_decay
------------ 0.08570146560668945 backwards -------------------------------
10 recon_decay
------------ 0.0844871997833252 backwards -------------------------------
10 recon_decay
------------ 0.08498883247375488 backwards -------------------------------
10 recon_decay
------------ 0.08523702621459961 backwards -------------------------------
10 recon_decay
------------ 0.08472537994384766 backwards -------------------------------
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 377/378 [00:53<00:00,  7.07it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                       | 28/45 [00:54<00:04,  3.78it/s]
------------ 0.0849771499633789 backwards -------------------------------
10 recon_decay
------------ 0.08499908447265625 backwards -------------------------------
10 recon_decay
------------ 0.08440947532653809 backwards -------------------------------
10 recon_decay
------------ 0.08575320243835449 backwards -------------------------------
10 recon_decay
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 43/45 [00:55<00:00, 12.93it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)


 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 124/126 [01:00<00:00, 24.67it/s]
saving model......................................................
done
------------ 60.85560131072998 seg time alll epoch -------------------------------
saving model......................................................
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py:908: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).
  plt.figure()
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19470502 0.19861099 0.19861104 ... 0.99788386 0.9979304  0.9979584 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.19470502 0.19861099 0.19861104 ... 0.99789387 0.9979584  0.99797887] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.19470502 0.19861099 0.19861104 ... 0.9979584  0.99796534 0.99797887] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19402294 0.19402297 0.19470502 ... 0.9980983  0.99816626 0.99820757] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.19470502 0.19861099 0.19861104 ... 0.9979417  0.9979584  0.9981477 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19470502 0.19861099 0.19861104 ... 0.99788386 0.9979304  0.9979584 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.19470502 0.19861099 0.19861104 ... 0.99789387 0.9979584  0.99797887] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.19470502 0.19861099 0.19861104 ... 0.9979584  0.99796534 0.99797887] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19402294 0.19402297 0.19470502 ... 0.9980983  0.99816626 0.99820757] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.19470502 0.19861099 0.19861104 ... 0.9979417  0.9979584  0.9981477 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None ULord ULord_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/166/exp2d_DF_3_2d True ulord2d_166 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
2 30 config['n_classes'], config['class_dim']
here seg decay
here recon_loss
/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/modules_unet.py:356: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  tensor = torch.tensor(x, requires_grad = False)
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:   3%|██████▋                                                                                                                                                                                                                                                    | 10/378 [00:01<00:51,  7.10it/s, loss=1.98]
  3%|███████▏                                                                                                                                                                                                                                                                        | 10/378 [00:01<00:51,  7.11it/s]
10 recon_decay
------------ 0.08915877342224121 backwards -------------------------------
10 recon_decay
------------ 0.08399438858032227 backwards -------------------------------
10 recon_decay
------------ 0.08440661430358887 backwards -------------------------------
10 recon_decay
------------ 0.08450651168823242 backwards -------------------------------
10 recon_decay
------------ 0.0835409164428711 backwards -------------------------------
10 recon_decay
------------ 0.08352923393249512 backwards -------------------------------
10 recon_decay
------------ 0.08454060554504395 backwards -------------------------------
10 recon_decay
------------ 0.0835869312286377 backwards -------------------------------
10 recon_decay
------------ 0.08368945121765137 backwards -------------------------------
10 recon_decay
------------ 0.08385324478149414 backwards -------------------------------

  7%|██████████████████▋                                                                                                                                                                                                                                                             | 26/378 [00:03<00:47,  7.43it/s]
------------ 0.0880589485168457 backwards -------------------------------
10 recon_decay
------------ 0.08393740653991699 backwards -------------------------------
10 recon_decay
------------ 0.08454084396362305 backwards -------------------------------
10 recon_decay
------------ 0.08380651473999023 backwards -------------------------------
10 recon_decay
------------ 0.08329629898071289 backwards -------------------------------
10 recon_decay
------------ 0.08343029022216797 backwards -------------------------------
10 recon_decay
------------ 0.0832674503326416 backwards -------------------------------
10 recon_decay
------------ 0.08499979972839355 backwards -------------------------------
10 recon_decay
------------ 0.08393001556396484 backwards -------------------------------
10 recon_decay
------------ 0.08375072479248047 backwards -------------------------------
10 recon_decay
------------ 0.08421015739440918 backwards -------------------------------
10 recon_decay
------------ 0.08397054672241211 backwards -------------------------------
10 recon_decay
------------ 0.0836329460144043 backwards -------------------------------
10 recon_decay
------------ 0.08425736427307129 backwards -------------------------------
10 recon_decay
------------ 0.08413100242614746 backwards -------------------------------

 11%|████████████████████████████▊                                                                                                                                                                                                                                                   | 40/378 [00:05<00:46,  7.34it/s]
------------ 0.08352828025817871 backwards -------------------------------
10 recon_decay
------------ 0.08416032791137695 backwards -------------------------------
10 recon_decay
------------ 0.08393335342407227 backwards -------------------------------
10 recon_decay
------------ 0.08458328247070312 backwards -------------------------------
10 recon_decay
------------ 0.0836946964263916 backwards -------------------------------
10 recon_decay
------------ 0.08372044563293457 backwards -------------------------------
10 recon_decay
------------ 0.0838015079498291 backwards -------------------------------
10 recon_decay
------------ 0.08490490913391113 backwards -------------------------------
10 recon_decay
------------ 0.08314776420593262 backwards -------------------------------
10 recon_decay
------------ 0.08315157890319824 backwards -------------------------------
10 recon_decay
------------ 0.0840749740600586 backwards -------------------------------
10 recon_decay
------------ 0.08396053314208984 backwards -------------------------------
10 recon_decay
------------ 0.08379340171813965 backwards -------------------------------
10 recon_decay
------------ 0.08369565010070801 backwards -------------------------------

 15%|███████████████████████████████████████▌                                                                                                                                                                                                                                        | 55/378 [00:07<00:44,  7.26it/s]
------------ 0.08576321601867676 backwards -------------------------------
10 recon_decay
------------ 0.08415722846984863 backwards -------------------------------
10 recon_decay
------------ 0.08408236503601074 backwards -------------------------------
10 recon_decay
------------ 0.08385300636291504 backwards -------------------------------
10 recon_decay
------------ 0.08459806442260742 backwards -------------------------------
10 recon_decay
------------ 0.08417057991027832 backwards -------------------------------
10 recon_decay
------------ 0.08406782150268555 backwards -------------------------------
10 recon_decay
------------ 0.08355259895324707 backwards -------------------------------
10 recon_decay
------------ 0.0839543342590332 backwards -------------------------------
10 recon_decay
------------ 0.08381009101867676 backwards -------------------------------
10 recon_decay
------------ 0.08590555191040039 backwards -------------------------------
10 recon_decay
------------ 0.08635449409484863 backwards -------------------------------
10 recon_decay
------------ 0.08369159698486328 backwards -------------------------------
10 recon_decay
------------ 0.08374714851379395 backwards -------------------------------
10 recon_decay
------------ 0.08442306518554688 backwards -------------------------------

 17%|███████████████████████████████████████████████▍                                                                                                                                                                                                                                | 66/378 [00:09<00:42,  7.33it/s]
------------ 0.08431053161621094 backwards -------------------------------
10 recon_decay
------------ 0.08352255821228027 backwards -------------------------------
10 recon_decay
------------ 0.08338427543640137 backwards -------------------------------
10 recon_decay
------------ 0.08422517776489258 backwards -------------------------------
10 recon_decay
------------ 0.08397531509399414 backwards -------------------------------
10 recon_decay
------------ 0.08389544486999512 backwards -------------------------------
10 recon_decay
------------ 0.08463573455810547 backwards -------------------------------
10 recon_decay
------------ 0.08489441871643066 backwards -------------------------------
10 recon_decay
------------ 0.08392596244812012 backwards -------------------------------
10 recon_decay
------------ 0.08382225036621094 backwards -------------------------------
10 recon_decay
------------ 0.08377575874328613 backwards -------------------------------
10 recon_decay
------------ 0.08508110046386719 backwards -------------------------------
10 recon_decay
------------ 0.08393239974975586 backwards -------------------------------
10 recon_decay
------------ 0.0844430923461914 backwards -------------------------------
10 recon_decay
------------ 0.08411097526550293 backwards -------------------------------
10 recon_decay
------------ 0.08411002159118652 backwards -------------------------------
10 recon_decay
------------ 0.11054158210754395 backwards -------------------------------
10 recon_decay
------------ 0.08358526229858398 backwards -------------------------------
10 recon_decay
------------ 0.08506250381469727 backwards -------------------------------
10 recon_decay
------------ 0.08399224281311035 backwards -------------------------------
10 recon_decay
------------ 0.08397841453552246 backwards -------------------------------
10 recon_decay
------------ 0.08432984352111816 backwards -------------------------------
10 recon_decay
------------ 0.08502030372619629 backwards -------------------------------
10 recon_decay
------------ 0.08398294448852539 backwards -------------------------------
10 recon_decay
------------ 0.08426165580749512 backwards -------------------------------
10 recon_decay
------------ 0.08438324928283691 backwards -------------------------------
10 recon_decay
------------ 0.0836479663848877 backwards -------------------------------
10 recon_decay
------------ 0.08455204963684082 backwards -------------------------------
10 recon_decay
------------ 0.08406233787536621 backwards -------------------------------
10 recon_decay
------------ 0.08454012870788574 backwards -------------------------------
10 recon_decay
------------ 0.08440089225769043 backwards -------------------------------
10 recon_decay
------------ 0.08397364616394043 backwards -------------------------------
10 recon_decay
------------ 0.08428072929382324 backwards -------------------------------
10 recon_decay
------------ 0.08413052558898926 backwards -------------------------------
10 recon_decay
------------ 0.08429646492004395 backwards -------------------------------
10 recon_decay
------------ 0.08410525321960449 backwards -------------------------------
10 recon_decay
------------ 0.08373880386352539 backwards -------------------------------
10 recon_decay
------------ 0.08443689346313477 backwards -------------------------------
10 recon_decay
------------ 0.08469080924987793 backwards -------------------------------
10 recon_decay
------------ 0.08414316177368164 backwards -------------------------------
10 recon_decay
------------ 0.08529806137084961 backwards -------------------------------
10 recon_decay
------------ 0.0842897891998291 backwards -------------------------------
10 recon_decay
------------ 0.08419346809387207 backwards -------------------------------
10 recon_decay
------------ 0.0845022201538086 backwards -------------------------------
10 recon_decay
------------ 0.08410310745239258 backwards -------------------------------
10 recon_decay
------------ 0.08441829681396484 backwards -------------------------------
10 recon_decay
------------ 0.08438491821289062 backwards -------------------------------
10 recon_decay
------------ 0.08410072326660156 backwards -------------------------------
10 recon_decay
------------ 0.08430051803588867 backwards -------------------------------
10 recon_decay
------------ 0.08472919464111328 backwards -------------------------------
10 recon_decay
------------ 0.08378338813781738 backwards -------------------------------
10 recon_decay
------------ 0.08497452735900879 backwards -------------------------------
10 recon_decay
------------ 0.08465743064880371 backwards -------------------------------
10 recon_decay
------------ 0.08374905586242676 backwards -------------------------------
10 recon_decay
------------ 0.08453083038330078 backwards -------------------------------
10 recon_decay
------------ 0.08441543579101562 backwards -------------------------------
10 recon_decay
------------ 0.08463597297668457 backwards -------------------------------
10 recon_decay
------------ 0.0845346450805664 backwards -------------------------------
10 recon_decay
------------ 0.08529424667358398 backwards -------------------------------
10 recon_decay
------------ 0.08412647247314453 backwards -------------------------------
10 recon_decay
------------ 0.08491706848144531 backwards -------------------------------
10 recon_decay
------------ 0.08518004417419434 backwards -------------------------------
10 recon_decay
------------ 0.08434104919433594 backwards -------------------------------
10 recon_decay
------------ 0.08517646789550781 backwards -------------------------------
10 recon_decay
------------ 0.0847008228302002 backwards -------------------------------
10 recon_decay
------------ 0.08476614952087402 backwards -------------------------------
10 recon_decay
------------ 0.0844125747680664 backwards -------------------------------
10 recon_decay
------------ 0.0844261646270752 backwards -------------------------------
10 recon_decay
------------ 0.08476948738098145 backwards -------------------------------
10 recon_decay
------------ 0.08447909355163574 backwards -------------------------------
10 recon_decay
------------ 0.08554387092590332 backwards -------------------------------
10 recon_decay
------------ 0.08459830284118652 backwards -------------------------------
10 recon_decay
------------ 0.08454751968383789 backwards -------------------------------
10 recon_decay
------------ 0.08436226844787598 backwards -------------------------------
10 recon_decay
------------ 0.08392715454101562 backwards -------------------------------
10 recon_decay
------------ 0.08459615707397461 backwards -------------------------------
10 recon_decay
------------ 0.08438706398010254 backwards -------------------------------
10 recon_decay
------------ 0.0850214958190918 backwards -------------------------------
10 recon_decay
------------ 0.0842893123626709 backwards -------------------------------
10 recon_decay
------------ 0.08441376686096191 backwards -------------------------------
10 recon_decay
------------ 0.08471012115478516 backwards -------------------------------
10 recon_decay
------------ 0.08455610275268555 backwards -------------------------------
10 recon_decay
------------ 0.08401322364807129 backwards -------------------------------
10 recon_decay
------------ 0.08507966995239258 backwards -------------------------------
10 recon_decay
------------ 0.08423018455505371 backwards -------------------------------
10 recon_decay
------------ 0.0842585563659668 backwards -------------------------------
10 recon_decay
------------ 0.08428406715393066 backwards -------------------------------
10 recon_decay
------------ 0.08461666107177734 backwards -------------------------------
10 recon_decay
------------ 0.0846254825592041 backwards -------------------------------
10 recon_decay
------------ 0.0843355655670166 backwards -------------------------------
10 recon_decay
------------ 0.08435678482055664 backwards -------------------------------
10 recon_decay
------------ 0.0846564769744873 backwards -------------------------------
10 recon_decay
------------ 0.08446812629699707 backwards -------------------------------
10 recon_decay
------------ 0.08449292182922363 backwards -------------------------------
10 recon_decay
------------ 0.0849905014038086 backwards -------------------------------
10 recon_decay
------------ 0.08524942398071289 backwards -------------------------------
10 recon_decay
------------ 0.08434081077575684 backwards -------------------------------
10 recon_decay
------------ 0.08444690704345703 backwards -------------------------------
10 recon_decay
------------ 0.0837702751159668 backwards -------------------------------
10 recon_decay
------------ 0.08461737632751465 backwards -------------------------------
10 recon_decay
------------ 0.08463525772094727 backwards -------------------------------
10 recon_decay
------------ 0.08536171913146973 backwards -------------------------------
10 recon_decay
------------ 0.08397436141967773 backwards -------------------------------
10 recon_decay
------------ 0.08442139625549316 backwards -------------------------------
10 recon_decay
------------ 0.08472704887390137 backwards -------------------------------
10 recon_decay
------------ 0.08504939079284668 backwards -------------------------------
10 recon_decay
------------ 0.0842435359954834 backwards -------------------------------
10 recon_decay
------------ 0.08434200286865234 backwards -------------------------------
10 recon_decay
------------ 0.08623933792114258 backwards -------------------------------
10 recon_decay
------------ 0.08481025695800781 backwards -------------------------------
10 recon_decay
------------ 0.08431816101074219 backwards -------------------------------
10 recon_decay
------------ 0.08503198623657227 backwards -------------------------------
10 recon_decay
------------ 0.0846104621887207 backwards -------------------------------
10 recon_decay
------------ 0.08454728126525879 backwards -------------------------------
10 recon_decay
------------ 0.08435535430908203 backwards -------------------------------
10 recon_decay
------------ 0.0850369930267334 backwards -------------------------------
10 recon_decay
------------ 0.08488965034484863 backwards -------------------------------
10 recon_decay
------------ 0.08468127250671387 backwards -------------------------------
10 recon_decay
------------ 0.08400368690490723 backwards -------------------------------
10 recon_decay
------------ 0.08595848083496094 backwards -------------------------------
10 recon_decay
------------ 0.08418536186218262 backwards -------------------------------
10 recon_decay
------------ 0.08478426933288574 backwards -------------------------------
10 recon_decay
------------ 0.0839848518371582 backwards -------------------------------
10 recon_decay
------------ 0.08468365669250488 backwards -------------------------------
10 recon_decay
------------ 0.0841667652130127 backwards -------------------------------
10 recon_decay
------------ 0.08466601371765137 backwards -------------------------------
10 recon_decay
------------ 0.08525919914245605 backwards -------------------------------
10 recon_decay
------------ 0.08487772941589355 backwards -------------------------------
10 recon_decay
------------ 0.08416032791137695 backwards -------------------------------
10 recon_decay
------------ 0.08498835563659668 backwards -------------------------------
10 recon_decay
------------ 0.08417844772338867 backwards -------------------------------
10 recon_decay
------------ 0.08479094505310059 backwards -------------------------------
10 recon_decay
------------ 0.08490395545959473 backwards -------------------------------
10 recon_decay
------------ 0.08460736274719238 backwards -------------------------------
10 recon_decay
------------ 0.0842278003692627 backwards -------------------------------
10 recon_decay
------------ 0.08486413955688477 backwards -------------------------------
10 recon_decay
------------ 0.08457398414611816 backwards -------------------------------
10 recon_decay
------------ 0.08425045013427734 backwards -------------------------------
10 recon_decay
------------ 0.08501124382019043 backwards -------------------------------
10 recon_decay
------------ 0.08412528038024902 backwards -------------------------------
10 recon_decay
------------ 0.08502364158630371 backwards -------------------------------
10 recon_decay
------------ 0.08402633666992188 backwards -------------------------------
10 recon_decay
------------ 0.08429741859436035 backwards -------------------------------
10 recon_decay
------------ 0.08425474166870117 backwards -------------------------------
10 recon_decay
------------ 0.08451676368713379 backwards -------------------------------
10 recon_decay
------------ 0.08401870727539062 backwards -------------------------------
10 recon_decay
------------ 0.08512020111083984 backwards -------------------------------
10 recon_decay
------------ 0.08461451530456543 backwards -------------------------------
10 recon_decay
------------ 0.08399724960327148 backwards -------------------------------
10 recon_decay
------------ 0.08467912673950195 backwards -------------------------------
10 recon_decay
------------ 0.08459687232971191 backwards -------------------------------
10 recon_decay
------------ 0.08510208129882812 backwards -------------------------------
10 recon_decay
------------ 0.08448243141174316 backwards -------------------------------
10 recon_decay
------------ 0.08447885513305664 backwards -------------------------------
10 recon_decay
------------ 0.08521795272827148 backwards -------------------------------
10 recon_decay
------------ 0.08462691307067871 backwards -------------------------------
10 recon_decay
------------ 0.08476567268371582 backwards -------------------------------
10 recon_decay
------------ 0.08477139472961426 backwards -------------------------------
10 recon_decay
------------ 0.08461189270019531 backwards -------------------------------
10 recon_decay
------------ 0.08473753929138184 backwards -------------------------------
10 recon_decay
------------ 0.0844120979309082 backwards -------------------------------
10 recon_decay
------------ 0.08594942092895508 backwards -------------------------------
10 recon_decay
------------ 0.08415389060974121 backwards -------------------------------
10 recon_decay
------------ 0.0844876766204834 backwards -------------------------------
10 recon_decay
------------ 0.0842742919921875 backwards -------------------------------
10 recon_decay
------------ 0.0846700668334961 backwards -------------------------------
10 recon_decay
------------ 0.0848073959350586 backwards -------------------------------
10 recon_decay
------------ 0.08417296409606934 backwards -------------------------------
10 recon_decay
------------ 0.08598136901855469 backwards -------------------------------
10 recon_decay
------------ 0.08427143096923828 backwards -------------------------------
10 recon_decay
------------ 0.08459877967834473 backwards -------------------------------
10 recon_decay
------------ 0.08536291122436523 backwards -------------------------------
10 recon_decay
------------ 0.08441734313964844 backwards -------------------------------
10 recon_decay
------------ 0.08449339866638184 backwards -------------------------------
10 recon_decay
------------ 0.08472943305969238 backwards -------------------------------
10 recon_decay
------------ 0.08519625663757324 backwards -------------------------------
10 recon_decay
------------ 0.08465194702148438 backwards -------------------------------
10 recon_decay
------------ 0.08734583854675293 backwards -------------------------------
10 recon_decay
------------ 0.08410310745239258 backwards -------------------------------
10 recon_decay
------------ 0.08465409278869629 backwards -------------------------------
10 recon_decay
------------ 0.08485841751098633 backwards -------------------------------
10 recon_decay
------------ 0.08424258232116699 backwards -------------------------------
10 recon_decay
------------ 0.08570575714111328 backwards -------------------------------
10 recon_decay
------------ 0.08476924896240234 backwards -------------------------------
10 recon_decay
------------ 0.08451724052429199 backwards -------------------------------
10 recon_decay
------------ 0.08504080772399902 backwards -------------------------------
10 recon_decay
------------ 0.0839993953704834 backwards -------------------------------
10 recon_decay
------------ 0.08517265319824219 backwards -------------------------------
10 recon_decay
------------ 0.08479809761047363 backwards -------------------------------
10 recon_decay
------------ 0.0851140022277832 backwards -------------------------------
10 recon_decay
------------ 0.08463358879089355 backwards -------------------------------
10 recon_decay
------------ 0.08498287200927734 backwards -------------------------------
10 recon_decay
------------ 0.08410930633544922 backwards -------------------------------
10 recon_decay
------------ 0.08487725257873535 backwards -------------------------------
10 recon_decay
------------ 0.08513283729553223 backwards -------------------------------
10 recon_decay
------------ 0.08419656753540039 backwards -------------------------------
10 recon_decay
------------ 0.0852353572845459 backwards -------------------------------
10 recon_decay
------------ 0.08539462089538574 backwards -------------------------------
10 recon_decay
------------ 0.0845193862915039 backwards -------------------------------
10 recon_decay
------------ 0.08547759056091309 backwards -------------------------------
10 recon_decay
------------ 0.08510041236877441 backwards -------------------------------
10 recon_decay
------------ 0.08414673805236816 backwards -------------------------------
10 recon_decay
------------ 0.08526134490966797 backwards -------------------------------
10 recon_decay
------------ 0.08550786972045898 backwards -------------------------------
10 recon_decay
------------ 0.08464503288269043 backwards -------------------------------
10 recon_decay
------------ 0.08499479293823242 backwards -------------------------------
10 recon_decay
------------ 0.08552169799804688 backwards -------------------------------
10 recon_decay
------------ 0.08457446098327637 backwards -------------------------------
10 recon_decay
------------ 0.08424949645996094 backwards -------------------------------
10 recon_decay
------------ 0.08501100540161133 backwards -------------------------------
10 recon_decay
------------ 0.08447933197021484 backwards -------------------------------
10 recon_decay
------------ 0.08454155921936035 backwards -------------------------------
10 recon_decay
------------ 0.08489847183227539 backwards -------------------------------
10 recon_decay
------------ 0.0862874984741211 backwards -------------------------------
10 recon_decay
------------ 0.08466410636901855 backwards -------------------------------
10 recon_decay
------------ 0.08443069458007812 backwards -------------------------------
10 recon_decay
------------ 0.08433914184570312 backwards -------------------------------
10 recon_decay
------------ 0.0838935375213623 backwards -------------------------------
10 recon_decay
------------ 0.0852358341217041 backwards -------------------------------
10 recon_decay
------------ 0.0845174789428711 backwards -------------------------------
10 recon_decay
------------ 0.0847630500793457 backwards -------------------------------
10 recon_decay
------------ 0.08443593978881836 backwards -------------------------------
10 recon_decay
------------ 0.0841214656829834 backwards -------------------------------
10 recon_decay
------------ 0.08452796936035156 backwards -------------------------------
10 recon_decay
------------ 0.08456563949584961 backwards -------------------------------
10 recon_decay
------------ 0.08392047882080078 backwards -------------------------------
10 recon_decay
------------ 0.08457589149475098 backwards -------------------------------
10 recon_decay
------------ 0.08556747436523438 backwards -------------------------------
10 recon_decay
------------ 0.08437967300415039 backwards -------------------------------
10 recon_decay
------------ 0.08495497703552246 backwards -------------------------------
10 recon_decay
------------ 0.08526182174682617 backwards -------------------------------
10 recon_decay
------------ 0.08450102806091309 backwards -------------------------------
10 recon_decay
------------ 0.08460474014282227 backwards -------------------------------
10 recon_decay
------------ 0.08489489555358887 backwards -------------------------------
10 recon_decay
------------ 0.08572244644165039 backwards -------------------------------
10 recon_decay
------------ 0.08466768264770508 backwards -------------------------------
10 recon_decay
------------ 0.08497476577758789 backwards -------------------------------
10 recon_decay
------------ 0.08482241630554199 backwards -------------------------------
10 recon_decay
------------ 0.08516693115234375 backwards -------------------------------
10 recon_decay
------------ 0.08458471298217773 backwards -------------------------------
10 recon_decay
------------ 0.08477044105529785 backwards -------------------------------
10 recon_decay
------------ 0.08461856842041016 backwards -------------------------------
10 recon_decay
------------ 0.0845479965209961 backwards -------------------------------
10 recon_decay
------------ 0.08587145805358887 backwards -------------------------------
10 recon_decay
------------ 0.08582091331481934 backwards -------------------------------
10 recon_decay
------------ 0.08502078056335449 backwards -------------------------------
10 recon_decay
------------ 0.08463001251220703 backwards -------------------------------
10 recon_decay
------------ 0.08592510223388672 backwards -------------------------------
10 recon_decay
------------ 0.0849907398223877 backwards -------------------------------
10 recon_decay
------------ 0.08505105972290039 backwards -------------------------------
10 recon_decay
------------ 0.08468508720397949 backwards -------------------------------
10 recon_decay
------------ 0.08490848541259766 backwards -------------------------------
10 recon_decay
------------ 0.08564472198486328 backwards -------------------------------
10 recon_decay
------------ 0.0851750373840332 backwards -------------------------------
10 recon_decay
------------ 0.08583498001098633 backwards -------------------------------
10 recon_decay
------------ 0.08509087562561035 backwards -------------------------------
10 recon_decay
------------ 0.08526062965393066 backwards -------------------------------
10 recon_decay
------------ 0.08474397659301758 backwards -------------------------------
10 recon_decay
------------ 0.08620762825012207 backwards -------------------------------
10 recon_decay
------------ 0.08527731895446777 backwards -------------------------------
10 recon_decay
------------ 0.08522176742553711 backwards -------------------------------
10 recon_decay
------------ 0.08587455749511719 backwards -------------------------------
10 recon_decay
------------ 0.08536839485168457 backwards -------------------------------
10 recon_decay
------------ 0.08533835411071777 backwards -------------------------------
10 recon_decay
------------ 0.08568668365478516 backwards -------------------------------
10 recon_decay
------------ 0.08560943603515625 backwards -------------------------------
10 recon_decay
------------ 0.08461451530456543 backwards -------------------------------
10 recon_decay
------------ 0.08482599258422852 backwards -------------------------------
10 recon_decay
------------ 0.08593535423278809 backwards -------------------------------
10 recon_decay
------------ 0.08519983291625977 backwards -------------------------------
10 recon_decay
------------ 0.08499383926391602 backwards -------------------------------
10 recon_decay
------------ 0.0860896110534668 backwards -------------------------------
10 recon_decay
------------ 0.08466172218322754 backwards -------------------------------
10 recon_decay
------------ 0.08517980575561523 backwards -------------------------------
10 recon_decay
------------ 0.08616328239440918 backwards -------------------------------
10 recon_decay
------------ 0.08505415916442871 backwards -------------------------------
10 recon_decay
------------ 0.08515381813049316 backwards -------------------------------
10 recon_decay
------------ 0.08531022071838379 backwards -------------------------------
10 recon_decay
------------ 0.08577346801757812 backwards -------------------------------
10 recon_decay
------------ 0.08444786071777344 backwards -------------------------------
10 recon_decay
------------ 0.08472442626953125 backwards -------------------------------
10 recon_decay
------------ 0.08557009696960449 backwards -------------------------------
10 recon_decay
------------ 0.08499383926391602 backwards -------------------------------
10 recon_decay
------------ 0.08566594123840332 backwards -------------------------------
10 recon_decay
------------ 0.08535265922546387 backwards -------------------------------
10 recon_decay
------------ 0.08475399017333984 backwards -------------------------------
10 recon_decay
------------ 0.08500504493713379 backwards -------------------------------
10 recon_decay
------------ 0.08573055267333984 backwards -------------------------------
10 recon_decay
------------ 0.08554363250732422 backwards -------------------------------
10 recon_decay
------------ 0.08534955978393555 backwards -------------------------------
10 recon_decay
------------ 0.084686279296875 backwards -------------------------------
10 recon_decay
------------ 0.08518862724304199 backwards -------------------------------
10 recon_decay
------------ 0.0864720344543457 backwards -------------------------------
10 recon_decay
------------ 0.0853719711303711 backwards -------------------------------
10 recon_decay
------------ 0.08574509620666504 backwards -------------------------------
10 recon_decay
------------ 0.08522629737854004 backwards -------------------------------
10 recon_decay
------------ 0.08517956733703613 backwards -------------------------------
10 recon_decay
------------ 0.0851430892944336 backwards -------------------------------
10 recon_decay
------------ 0.08676981925964355 backwards -------------------------------
10 recon_decay
------------ 0.08474302291870117 backwards -------------------------------
10 recon_decay
------------ 0.0845191478729248 backwards -------------------------------
10 recon_decay
------------ 0.08655571937561035 backwards -------------------------------
10 recon_decay
------------ 0.08585572242736816 backwards -------------------------------
10 recon_decay
------------ 0.08546805381774902 backwards -------------------------------
10 recon_decay
------------ 0.08610963821411133 backwards -------------------------------
10 recon_decay
------------ 0.08512639999389648 backwards -------------------------------
10 recon_decay
------------ 0.0843510627746582 backwards -------------------------------
10 recon_decay
------------ 0.08487725257873535 backwards -------------------------------
10 recon_decay
------------ 0.08559036254882812 backwards -------------------------------
10 recon_decay
------------ 0.08527541160583496 backwards -------------------------------
10 recon_decay
------------ 0.08446121215820312 backwards -------------------------------
10 recon_decay
------------ 0.08493542671203613 backwards -------------------------------
10 recon_decay
------------ 0.08480191230773926 backwards -------------------------------
10 recon_decay
------------ 0.08482098579406738 backwards -------------------------------
10 recon_decay
------------ 0.08550739288330078 backwards -------------------------------
10 recon_decay
------------ 0.0852975845336914 backwards -------------------------------
10 recon_decay
------------ 0.08501958847045898 backwards -------------------------------
10 recon_decay
------------ 0.08442926406860352 backwards -------------------------------
10 recon_decay
------------ 0.08642053604125977 backwards -------------------------------
10 recon_decay
------------ 0.08497858047485352 backwards -------------------------------
10 recon_decay
------------ 0.08492541313171387 backwards -------------------------------
10 recon_decay
------------ 0.08519697189331055 backwards -------------------------------
10 recon_decay
------------ 0.08586287498474121 backwards -------------------------------
10 recon_decay

100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎| 377/378 [00:54<00:00,  6.95it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:56<00:00,  1.26s/it]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊            | 43/45 [00:56<00:00, 12.47it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)

 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 124/126 [01:02<00:00, 23.36it/s]
saving model......................................................
done
------------ 62.80448937416077 seg time alll epoch -------------------------------
saving model......................................................
here1
saving model......................................................
2 30 config['n_classes'], config['class_dim']
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19947663 0.2009953  0.20195669 ... 0.9999907  0.99999213 0.99999297] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.20035057 0.20210132 0.20215563 ... 0.99999166 0.99999297 0.9999932 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.20488515 0.2059438  0.20599897 ... 0.9999906  0.9999913  0.9999914 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.2009953  0.20195669 0.20250943 ... 0.9999906  0.99999297 0.9999932 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.20099533 0.20348942 0.20404527 ... 0.9999889  0.9999896  0.9999906 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19947663 0.2009953  0.20195669 ... 0.9999907  0.99999213 0.99999297] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.20035057 0.20210132 0.20215563 ... 0.99999166 0.99999297 0.9999932 ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.20488515 0.2059438  0.20599897 ... 0.9999906  0.9999913  0.9999914 ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.2009953  0.20195669 0.20250943 ... 0.9999906  0.99999297 0.9999932 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([16, 1, 128, 128]) patch size
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.20099533 0.20348942 0.20404527 ... 0.9999889  0.9999896  0.9999906 ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNet UNet_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/166/exp2d_DF_3_2d True unet2d_166 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
arrive train
arrive location
here opt
here seg decay
  0%|                                                                                                                                                                                                                                                                                         | 0/378 [00:00<?, ?it/s][W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                               | 0/45 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/126 [00:00<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
epoch #0:  22%|█████████████████████████████████████████████████████████                                                                                                                                                                                                     | 85/378 [00:03<00:11, 25.58it/s, loss=0]
epoch #0:  37%|█████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                | 139/378 [00:05<00:09, 25.69it/s, loss=0]
epoch #0:  50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                             | 190/378 [00:07<00:07, 25.47it/s, loss=0]
epoch #0:  64%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                           | 241/378 [00:09<00:05, 25.61it/s, loss=0]
epoch #0:  78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                       | 295/378 [00:11<00:03, 25.64it/s, loss=0]
epoch #0:  92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                     | 346/378 [00:13<00:01, 25.55it/s, loss=0]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:15<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)                                                                                                                                                                                              | 0/378 [00:15<?, ?it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 45/45 [00:15<00:00,  2.83it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)███████████████████████████████████████████████████████████████████████████████████████████                                                                                           | 30/45 [00:15<00:04,  3.25it/s]
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
saving model......................................................
done
------------ 17.31709384918213 seg time alll epoch -------------------------------
saving model......................................................
here1
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 126/126 [00:17<00:00,  7.35it/s]
 95%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 120/126 [00:17<00:00, 37.35it/s]
saving model......................................................
dict_keys(['104', '135', '170', '174', '176', '177', '184', '188', '189', '191', '197', '200', '201', '203', '205', '208', '209', '211', '223', '224', '225', '228', '232', '234', '258', '76', '78', '81', '91', '93', '95'])
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19791919 0.19799757 0.19878393 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.19686438 0.19696608 0.19710347 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.19409736 0.19516844 0.19533399 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19784972 0.1982132  0.19839191 ... 0.99944264 0.99944276 0.9994429 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.19383763 0.1950197  0.1950691  ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
184 subject id
139 dif cols diff_row 76
x:48:124 y:71:210 z:26:92
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19791919 0.19799757 0.19878393 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
200 subject id
119 dif cols diff_row 163
x:63:226 y:80:199 z:21:78
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(45, 128, 128, 16) patch shape
torch.Size([720, 1, 128, 128]) patches torch
720 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
torch.Size([720, 1, 128, 128]) prediction size
(720, 128, 128, 1) prediction shape  45 16
[0.19686438 0.19696608 0.19710347 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(45, 128, 128, 16) prediction shape
203 subject id
135 dif cols diff_row 136
x:43:179 y:34:169 z:12:36
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(36, 128, 128, 16) patch shape
torch.Size([576, 1, 128, 128]) patches torch
576 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
torch.Size([576, 1, 128, 128]) prediction size
(576, 128, 128, 1) prediction shape  36 16
[0.19409736 0.19516844 0.19533399 ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(36, 128, 128, 16) prediction shape
209 subject id
145 dif cols diff_row 76
x:103:179 y:106:251 z:22:81
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(54, 128, 128, 16) patch shape
torch.Size([864, 1, 128, 128]) patches torch
864 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
432 iiiiiii
448 iiiiiii
464 iiiiiii
480 iiiiiii
496 iiiiiii
512 iiiiiii
528 iiiiiii
544 iiiiiii
560 iiiiiii
576 iiiiiii
592 iiiiiii
608 iiiiiii
624 iiiiiii
640 iiiiiii
656 iiiiiii
672 iiiiiii
688 iiiiiii
704 iiiiiii
720 iiiiiii
736 iiiiiii
752 iiiiiii
768 iiiiiii
784 iiiiiii
800 iiiiiii
816 iiiiiii
832 iiiiiii
848 iiiiiii
torch.Size([864, 1, 128, 128]) prediction size
(864, 128, 128, 1) prediction shape  54 16
[0.19784972 0.1982132  0.19839191 ... 0.99944264 0.99944276 0.9994429 ] prediction unique check if we are really rounding the score
(54, 128, 128, 16) prediction shape
211 subject id
166 dif cols diff_row 90
x:102:192 y:61:227 z:3:58
hereeeeee
[128, 128, 16] [64, 64, 16]
dim2
(27, 128, 128, 16) patch shape
torch.Size([432, 1, 128, 128]) patches torch
432 patches size
16 batch size
0 iiiiiii
16 iiiiiii
32 iiiiiii
48 iiiiiii
64 iiiiiii
80 iiiiiii
96 iiiiiii
112 iiiiiii
128 iiiiiii
144 iiiiiii
160 iiiiiii
176 iiiiiii
192 iiiiiii
208 iiiiiii
224 iiiiiii
240 iiiiiii
256 iiiiiii
272 iiiiiii
288 iiiiiii
304 iiiiiii
320 iiiiiii
336 iiiiiii
352 iiiiiii
368 iiiiiii
384 iiiiiii
400 iiiiiii
416 iiiiiii
torch.Size([432, 1, 128, 128]) prediction size
(432, 128, 128, 1) prediction shape  27 16
[0.19383763 0.1950197  0.1950691  ... 0.99944276 0.9994429  0.999443  ] prediction unique check if we are really rounding the score
(27, 128, 128, 16) prediction shape
4.0 post_processing.max_over_lap
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
[0. 1. 2. 3. 4.] unique pred
args: None UNetC UNetC_2 /cs/casmip/nirm/embryo_project_version1/EXP_FOLDER/exp_fp2/166/exp2d_DF_3_2d True unet2dC_166 DF_3_2d_TL DF_3_2d_TU DF_3_2d_VA DF_3_2d_TE False True /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d /cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/models_config/config_2d
dividing....
(2016, 128, 128, 1) imaggggggeeeeeeeeeeeeeeee shape
[0.] unique claseesssssssssssssssssssssssssssssssssssssssssssssssss
False False load_model
    main()█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████             | 120/126 [00:17<00:00, 37.35it/s]
  File "lord_exp.py", line 133, in main
    args.func(args)
  File "lord_exp.py", line 114, in run_exp
    train_model(model_dict, path_new_exp, model_name, exp_dict['data_l_name'], exp_dict['data_u_name'], exp_dict['data_v_name'], exp_dict['data_t_name'], exp_dict['base_dir'])
  File "lord_exp.py", line 31, in train_model
    take_from_arg = False)
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/lord_unet.py", line 367, in train_ulord
    loaded_model=load_model
  File "/cs/labs/josko/nirm/embryo_project_version1/embyo_projects_codes/lord-pytorch-unet/model/training_unet.py", line 429, in train
    raise Exception(f"no model id {model_id}")
Exception: no model id UNetC